%% Le lingue utilizzate, che verranno passate come opzioni al pacchetto babel. Come sempre, l'ultima indicata sarà quella primaria.
%% Se si utilizzano una o più lingue diverse da "italian" o "english", leggere le istruzioni in fondo.
\def\thudbabelopt{english,italian}
%% Valori ammessi per target: bach (tesi triennale), mst (tesi magistrale), phd (tesi di dottorato).
\documentclass[target=mst]{thud}[2014/03/11]

%% Aggiunta pacchetto per inserimento di immagini
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{url}

\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codered}{rgb}{0.95,0,0}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%% Custom style per i listing
\lstdefinestyle{customcpp}{
        language=C++,
		backgroundcolor=\color{backcolour},   
		commentstyle=\color{codegreen},
		keywordstyle=\color{magenta},
		numberstyle=\tiny\color{codegray},
		stringstyle=\color{codepurple},
		basicstyle=\footnotesize,
		breakatwhitespace=false,         
		breaklines=true,                 
		captionpos=b,                    
		keepspaces=true,                 
		numbers=left,                    
		numbersep=5pt,                  
		showspaces=false,                
		showstringspaces=false,
		showtabs=false,                  
		tabsize=2
}

\lstdefinelanguage{diff}{
    morecomment=[f][\color{codegray}]{@@},     % group identifier
    morecomment=[f][\color{codered}]-,         % deleted lines 
    morecomment=[f][\color{codegreen}]+,       % added lines
    morecomment=[f][\color{codered}]{---}, % Diff header lines (must appear after +,-)
    morecomment=[f][\color{codegreen}]{+++},
}

\lstdefinestyle{plain}{
		backgroundcolor=\color{backcolour},   
		basicstyle=\footnotesize,
		breakatwhitespace=false,         
		breaklines=true,                 
		captionpos=b,                    
		keepspaces=true,                 
		showspaces=false,                
		showstringspaces=false,
		showtabs=false,                  
		tabsize=2
}

%% Setting globale per i listing: stile di default customcpp
\lstset{style=plain}
% Traduzione didascalia listati: sostituisce Listing > Listato
\renewcommand{\lstlistingname}{Listato}

\usepackage{capt-of}
\graphicspath{ {img/Introduzione/} {img/ContestoTecnologico/1_PCNEurotech/} {img/ContestoTecnologico/2_TecnTesi/} {img/CorpoTesi/1_AmbientiLavoro/} {img/CorpoTesi/2_PCNBackSub/} {img/CorpoTesi/3_RSPCN/} {img/CorpoTesi/4_Confronto/} {img/CorpoTesi/5_Yocto/} {img/Manuale/} {img/Appendici/}}

%% --- Informazioni sulla tesi ---
%% Per tutti i tipi di tesi
\title{Elaborazione di Immagini in ambito Embedded con OpenCV: Passenger Counter}
\author{Mattia Dal Ben}
\course{Ingegneria Elettronica}
\supervisor{Prof.\ Antonio Abramo}
\cosupervisor{Ing.\ Marco Carrer}
%% Altri campi disponibili: \reviewer, \tutor, \chair, \date (anno accademico, calcolato in automatico).
%% Con \supervisor, \cosupervisor, \reviewer e \tutor si possono indicare più nomi separati da \and.
%% Per le sole tesi di dottorato
\phdnumber{313}
\cycle{XXVIII}
\contacts{Via della Sintassi Astratta, 0/1\\65536 Gigatera --- Italia\\+39 0123 456789\\\texttt{http://www.example.com}\\\texttt{inbox@example.com}}
\rights{Tutti i diritti riservati a me stesso e basta.}
%% Campi obbligatori: \title, \author e \course.

%% --- Pacchetti consigliati ---
%% hyperref: Regola le impostazioni della creazione del PDF... più tante altre cose.
%% tocbibind: Inserisce nell'indice anche la lista delle figure, la bibliografia, ecc.

%% --- Stili di pagina disponibili (comando \pagestyle) ---
%% sfbig (predefinito): Apertura delle parti e dei capitoli col numero grande; titoli delle parti e dei capitoli e intestazioni di pagina in sans serif.
%% big: Come "sfbig", solo serif.
%% plain: Apertura delle parti e dei capitoli tradizionali di LaTeX; intestazioni di pagina come "big".

\begin{document}

%% Il frontespizio prima di tutto!
\maketitle

%% Dedica (opzionale)
%% \begin{dedication}A mia madre.\end{dedication}

%% Ringraziamenti (opzionali)
%% \acknowledgements
%% Sed vel lorem a arcu faucibus aliquet eu semper tortor. Aliquam dolor lacus, semper vitae ligula sed, blandit iaculis leo. Nam pharetra lobortis leo nec auctor. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Fusce ac risus pulvinar, congue eros non, interdum metus. Mauris tincidunt neque et aliquam imperdiet. Aenean ac tellus id nibh pellentesque pulvinar ut eu lacus. Proin tempor facilisis tortor, et hendrerit purus commodo laoreet. Quisque sed augue id ligula consectetur adipiscing. Vestibulum libero metus, lacinia ac vestibulum eu, varius non arcu. Nam et gravida velit.

%% Sommario (opzionale)
\abstract
Nello sviluppo di questa tesi si \`e affrontato lo studio e la progettazione di un sistema di conteggio dei passeggeri, chiamato ``Passenger Counter'', su piattaforma embedded. Il software \`e basato su algoritmi di elaborazione di immagine resi disponibili dalla libreria open-source per l'image processing OpenCV. La piattaforma software \`e stata realizzata utilizzando il progetto Yocto, il quale permette la creazione di distribuzioni Linux custom e finalizzate all'utilizzo in ambito embedded.

L'applicazione Passenger Counter ha come scopo quello di contare i passeggeri che attraversano in entrata e in uscita le porte di un mezzo pubblico, in modo tale da permettere un conteggio esatto delle persone presenti sul mezzo.

Lo sviluppo si \`e diviso in tre fasi principali:
\begin{itemize}
\item Una indagine preliminare atta ad individuare la miglior piattaforma hardware/software sulla quale sviluppare l'applicazione.
\item Progettazione ed implementazione del contatore basato interamente su algoritmi di elaborazione delle immagini, individuando i passaggi pi\`u pesanti dal punto di vista computazionale. Nel resto del testo ci riferiremo a questa versione del software come: Passenger Counter a sottrazione dello sfondo, o PCN.
\item Progettazione e implementazione del contatore sfruttando telecamere ad infrarossi Intel RealSense. Nel resto del testo ci riferiremo a questa versione dell'applicazione come: Passenger Counter con telecamere RealSense, o RSPCN.
\end{itemize}

Contemporaneamente allo sviluppo dell'applicazione si \`e realizzata la piattaforma software sulla quale integrare tutte le tecnologie utilizzate dall'applicativo, per mezzo del progetto Yocto, perch\'e potesse essere installato sull'hardware in dotazione.

%% Indice
\tableofcontents

%% Lista delle figure (se presenti)
\listoffigures

%% Lista delle tabelle (se presenti)
\listoftables

%% Corpo principale del documento
\mainmatter

%% Parte
%% La suddivisione in parti è opzionale; talvolta sono sufficienti i capitoli.
%% \part{Parte}

\chapter{Introduzione}\label{CapitoloIntroduzione}
In questo capitolo tratteremo il contesto all'interno del quale si configura l'applicazione sviluppata nel corso della tesi. Quindi verr\`a descritta la finalit\`a dell'applicazione Passenger Counter e gli obiettivi di questa tesi.

%% Sezione Contesto IoT
%% NOTA: L'intera sezione che segue \`e stata presa dalla Wikipedia inglese alla voce: Internet of Things.
%% TODO: Cambiare un po' le parole in modo che non si capisca che ho copiato =D
\section{Il contesto IoT}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{IoTInfrastructure.png}
  \caption{Infrastruttura dell'internet of things.}
  \label{fig:IoTInfrastructure}
\end{figure}

L'Internet of Things (IoT) \`e l'interconnesione di device, veicoli, edifici e oggetti dotati di elettronica, software, sensori, attuatori e connettivit\`a che permettono a questi oggetti di raccogliere e scambiare dati. L'IoT permette agli oggetti di essere rilevati e controllati in remoto attraverso l'infrastruttura di rete esistente, creando opportunit\`a per una integrazione pi\`u diretta del mondo fisico all'interno di sistemi informatizzati, con l'obbiettivo di aumentare l'efficienza, la precisione e il beneficio economico riducendo al contempo la necessit\`a dell'intervento umano. Tipicamente ci si aspetta che l'IoT offra connettivit\`a avanzata tra device, sistemi e servizi che vadano oltre la comunicazione Machine-to-machine (M2M) e coprano una variet\`a di protocolli, domini ed applicazioni. L'obiettivo \`e quello di introdurre processi di automazione in tutti i settori.

%% Storia dell'IoT
\subsection{Storia dell'Internet of Things}
Il neologismo inglese Internet of Things \`e stato introdotto per la prima volta da Kevin Ashton, cofondatore e direttore esecutivo di Auto-ID Center (consorzio di ricerca con sede al MIT), durante una presentazione nel 1999, ma il concetto di una rete di device ``intelligenti'' fu discusso per la prima volta nel 1982, con un distributore di bibite opportunamente modificato per interfacciarsi ad internet dalla Carniegie Mellon University. Esso era capace di riportare il suo inventario e qualora le bibite di cui era appena stato rifornito fossero ancora calde. Tra il 1993 e il 1996 molte aziende cominciarono a proporre soluzioni per l'Internet delle Cose ma \`e solamente dopo il 1999 che il settore cominci\`o ad assumere rilevanza. Le prime applicazioni per questo tipo di concetti erano finalizzate all'inventariamento degli oggetti all'interno di grandi fabbriche. Ci\`o veniva realizzato utilizzando tag RFID (Radio-frequency Identification) che permettessero ai sistemi informatici di identificare e tracciare gli oggetti presenti all'interno di ambienti vasti. Questo tipo di applicazione dell'IoT \`e ormai pratica standard nota come RFID Asset Tracking.
Ad oggi il concetto di IoT si \`e molto evoluto grazie al progresso tecnologico. La capacit\`a di integrare negli oggetti elettronica, sensori e connettivit\`a wireless ne ha ampliato le capacit\`a e le possibili applicazioni. Nel settore IoT ora convergono molteplici tecnologie quali real-time analytics, machine learning, commodity sensors e sistemi embedded.

\subsection{Sviluppi futuri}
%% Parte di questa sezione l'ho rubacchiata da wikipedia cercando: Industria 4.0
%% TODO: Cambiare un po' le parole in modo che non si capisca che ho copiato =D
Secondo le proiezioni di Gartner Inc., una corporation per la ricerca e advisory tecnologica, entro il 2020 ci saranno oltre 20 miliardi di device connesse all'Internet of Things\cite{Gart}. Si sta parlando di una Industria 4.0 dove l'automazione industriale integra l'IoT per migliorare le condizioni di lavoro e la produttivit\`a. La chiave di volta dell'Industria 4.0 sono i sistemi ciberfisici (CPS), ovvero sistemi fisici che sono strettamente connessi con i sistemi informatici e possono interagire e collaborare con altri sistemi CPS i quali costituiscono lo step evolutivo successivo delle device IoT. 
Un altro settore nel quale si proiettano ulteriori sviluppi \`e la Big Data Analysis. L'ubiquit\`a dei dispositivi intelligenti connessi all'Internet delle Cose permette analisi di dati vastissimi ai quali precedentemente era impensabile avere accesso. Le informazioni che si possono ricavare da questi dati sono molteplici e di sicuro interesse per molti ambiti di applicazione.

%% Interruzione di pagina
\newpage

%% Classi di applicazioni IoT
\section{Classi di applicazioni IoT}
Passiamo ora ad analizzare i campi di applicazione pi\`u diffusi per l'Internet delle Cose.

%% Qui ho rubato da Wikipedia inglese il paragrafo sulle Applications nella pagina dedicata all'IoT
%% TODO: Cambiare un po' le parole in modo che non si capisca che ho copiato =D
\subsection{Monitoraggio ambientale}
Le applicazioni di monitoraggio ambientale tipicamente utilizzano sensori collegati all'Internet delle Cose per fornire assistenza nella protezione dell'ambiente. Il monitoraggio pu\`o interessare la qualit\`a dell'aria o dell'acqua, condizioni atmosferiche o del suolo, e pu\`o includere aree come il monitoraggio degli spostamenti della fauna selvatica e del suo habitat. Ci\`o pu\`o avere applicazioni anche nell'ambito della rilevazione di disastri naturali: sistemi di allerta per terremoti e tsunami possono essere implementati nell'ambito IoT. Device IoT in questo campo di applicazione sono tipicamente dislocate su un'ampia area geografica e possono anche essere mobili.

\subsection{Gestione delle infrastrutture}
Il monitoraggio e il controllo di infrastrutture urbane come ponti, rotaie, wind-farms \`e un campo di applicazione chiave dell'IoT. L'infrastruttura IoT pu\`o essere usata per monitorare eventi o cambiamenti nelle condizioni strutturali che possono compromettere la sicurezza o aumentare i rischi di incidenti. Pu\`o altres\`\i\ essere usato per la programmazione di interventi di manutenzione in maniera pi\`u efficiente coordinando le operazioni tra diversi fornitori di servizi. Si pensa che l'utilizzo di device IoT possa migliorare la gestione degli incidenti e la coordinazione in caso emergenze, la qualit\`a del servizio e ridurre i costi in tutte le aree legate alla gestione delle infrastrutture.

\subsection{Settore manifatturiero}
L'ambiente manifatturiero \`e sicuramente uno dei settori di punta dell'IoT fin dalla sua nascita. Le applicazioni dell'IoT spaziano dalla gestione degli equipaggiamenti al tracking degli asset nell'ambiente produttivo. Questa sinergia permette alle aziende di avere una maggior flessibilit\`a e di ottimizzare in tempo reale i sistemi di produzione nonch\'e la rete di approvvigionamento grazie all'interconnessione tra macchinari, sensori e sistemi.

L'integrazione di sistemi IoT all'interno di catene di produzione permette la predictive maintenance (tecniche dedicate al rilevamento dello stato di salute di macchinari ed equipaggiamenti in modo tale da poter prevedere quando effettuare la manutenzione), valutazione statistica dello stato di usura dei macchinari, e prevenzione di guasti.

Come visto in precedenza i concetti della rivoluzione industriale 4.0 sono interamente basati sull'integrazione tra sistemi di produzione e l'Internet of Things.

\subsection{Gestione dell'energia}
L'integrazione di reti di sensori e attuatori, connessi ad internet, si pensa possa ottimizzare il consumo energetico in ambito industriale e casalingo. L'integrazione di device IoT all'interno di contatori per l'energia, cos\`\i\ come dispositivi domestici e industriali, capaci di comunicare con le compagnie per la rete elettrica possono permettere una gestione migliore della generazione ed utilizzo dell'energia. Questo tipo di device inoltre consente agli utenti di regolare in remoto i loro dispositivi, o controllarli centralmente grazie a interfacce web accessibili tramite la rete cloud IoT, in modo tale da implementare funzioni avanzate come la programmazione delle accensioni/spegnimenti o dell'utilizzo dei dispositivi. Uno dei prodotti consumer pi\`u comuni in questo ambito \`e il termostato IoT\cite{Thermostat}, diffuso soprattutto in applicazioni domotiche.

\subsection{Settore medicale ed assistenza sanitaria}
Dispositivi connessi all'Internet delle Cose possono abilitare il monitoraggio remoto dello stato di salute dei pazienti e sistemi di notifica delle emergenze. Le applicazioni di queste device possono variare dal rilevamento della pressione arteriosa fino al conteggio dei battiti del cuore. Alcuni ospedali hanno cominciato ad utilizzare ``letti smart'' in grado di rilevare quanto il letto \`e occupato e quando il paziente sta cercando di alzarsi. 

Ormai sono molto diffusi dispositivi per il tracciamento delle attivit\`a dell'utente che incoraggiano uno stile di vita salutare, in questo ambito rientrano le wearable device come i fitness trackers.

\subsection{Settore dei trasporti}
L'IoT pu\`o dare assistenza nella integrazione delle comunicazioni, controlli e elaborazione delle informazioni attraverso vari sistemi di trasporto. Le applicazioni dell'IoT si estendono a tutti gli aspetti dei sistemi di trasporto: i veicoli, l'infrastruttura, il pilota e i passeggeri. L'interazione dinamica tra questi componenti permette una comunicazione inter e intra veicolare, controllo del traffico intelligente, smart parking, gestione della logistica e delle flotte, controllo dei veicoli e sicurezza stradale.

L'applicazione sviluppata nel corso della tesi \`e appunto legata a questo ambito di applicazione dell'IoT e nel prossimo paragrafo ne analizzeremo gli obiettivi e le funzionalit\`a.

%% Interruzione di pagina
\newpage

%% Sezione PCN
\section{Passenger Counter}
Il Passenger Counter, o contatore di passeggeri, \`e un dispositivo IoT la cui funzione \`e quella di rilevare e contare i viaggiatori presenti all'interno di un sistema di trasporto. Esso deve altres\`\i\ fornire i dati in tempo reale al gestore del servizio, sfruttando la connessione all'Internet delle Cose, in modo tale che sia possibile:
\begin{itemize}
\item Rilevare frodi.
\item Aumentare l'efficienza della flotta di mezzi migliorando la gestione e la programmazione dei percorsi.
\item Imporre una restrizione sul numero di passeggeri presenti sui mezzi per ragioni di sicurezza.
\item Analizzare i flussi di traffico all'interno delle citt\`a.
\end{itemize}
Il dispositivo deve essere in grado di effettuare il conteggio in modo non invasivo e contact-less, tenendo conto delle restrizioni dovute all'ambiente nel quale deve essere installato.

\begin{figure}[h!]
  \centering
  \includegraphics[width=9cm]{PassengerCounters.jpg}
  \caption{Schematizzazione del contatore di passeggeri.}
  \label{fig:SchemPCN}
\end{figure}

Il lavoro presentato in questa tesi \`e stato commissionato da Eurotech: azienda dedicata alla ricerca, sviluppo e produzione di sistemi embedded e computer ad alte prestazioni con sede ad Amaro. L'azienda inoltre ha una forte rilevanza in ambito IoT in quanto, oltre a fornire dispositivi IoT ready, ha realizzato una piattaforma Machine-to-Machine che consente ai dispositivi, ai sensori e a tutti i sistemi distribuiti sul campo, di comunicare tra loro trasferendo le informazioni rilevanti alle business application ed alle infrastrutture IT. Il Passenger Counter \`e uno dei prodotti di punta dell'azienda ed \`e stato selezionato come progetto per questa tesi.

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{SchemaPCNIoT.png}
  \caption{Schema Passenger Counter Eurotech.}
  \label{fig:SchemPCNEurotech}
\end{figure}

Il sistema di conteggio di passeggeri consta di tre parti fondamentali come schematizzato in figura \ref{fig:SchemPCNEurotech}:
\begin{enumerate}
\item Dispositivo di acquisizione video ed elaborazione principale.
\item Dispositivo per la trasmissione dei dati in real-time.
\item Infrastruttura di rete cloud IoT per il raccoglimento dei dati.
\end{enumerate}
\`E quindi possibile accedere ai dati raccolti da pi\`u dispositivi per mezzo della piattaforma M2M proprietaria di Eurotech.

%% Interruzione di pagina
\newpage

\section{Obiettivi della tesi}
L'obiettivo della tesi \`e stato quello di realizzare una nuova versione del Passenger Counter di Eurotech basandosi sull'hardware fornito dall'azienda, cercando di migliorare quanto gi\`a fatto dalla stessa, esplorando soluzioni tecnologiche alternative. Il lavoro \`e stato quindi suddiviso in tre parti:
\begin{enumerate}
\item Identificare gli ambienti di sviluppo pi\`u idonei alla realizzazione del progetto.
\item Realizzare una nuova versione del Passenger Counter puntando a migliorare le prestazione ed abbattere i costi, che fosse compatible con le tecnologie usate dall'azienda per i suoi prodotti. 
\item Realizzare una infrastruttura software che fornisse tutti gli strumenti e le librerie necessarie all'implementazione del Passenger Counter realizzato e che potesse essere installata sulla piattaforma hardware fornita da Eurotech.
\end{enumerate}

% TODO: Attenzione! Aggiornare questa sezione ad ogni cambiamento della struttura del testo. (Se ce ne sono).
\section{Struttura della tesi}
Qui di seguito \`e riportata l'organizzazione del testo:
\begin{itemize}
\item Capitolo \ref{CapitoloIntroduzione}: Introduzione al contesto dell'Internet of Things e all'applicazione di conteggio dei passeggeri.
\item Capitolo \ref{CapitoloContestoTecnologico}: Analisi della versione corrente del Passenger Counter realizzato da Eurotech. Segue una trattazione dettagliata delle tecnologie e software utilizzati nella realizzazione del progetto.
\item Capitolo \ref{CapitoloAnalisiAmbienti}: Analisi degli ambienti di sviluppo disponibili per l'image processing in ambito embedded e motivazioni che hanno portato alla scelta finale.
\item Capitolo \ref{CapitoloPCN}: Prima versione dell'applicativo, Passenger Counter a sottrazione del background.
\item Capitolo \ref{CapitoloRSPCN}: Seconda versione dell'applicativo, Passenger Counter con telecamere RealSense.
\item Capitolo \ref{CapitoloConfronto}: Confronto versioni dell'applicativo realizzate.
\item Capitolo \ref{CapitoloYocto}: Realizzazione e features della distribuzione Linux realizzata a supporto del progetto.
\item Capitolo \ref{CapitoloManuale}: Manuale d'uso del progetto.
\item Capitolo \ref{CapitoloConclusioni}: Riassunto e conclusioni.
\end{itemize}

\chapter{Contesto tecnologico di dettaglio}\label{CapitoloContestoTecnologico}
Passiamo ora a descrivere pi\`u tecnicamente e nel dettaglio le tecnologie utilizzate nello svolgimento della tesi.

\section{Tecnologie Passenger Counter Eurotech}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{StrutturaPCN.png}
  \caption{Struttura del Passenger Counter di Eurotech.}
  \label{fig:PCNEurotechHardware}
\end{figure}

%% Interruzione di pagina
\newpage

\subsection{Hardware}
Il sistema di conteggio dei passeggeri consta di due componenti hardware fondamentali: il gateway e il dispositivo di acquisizione ed elaborazione delle immagini.

Il gateway \`e un prodotto della Eurotech noto come ReliaGate 50-21. Utilizza un processore x86 Intel Atom Z510P al quale sono state aggiunte opportune connessioni di rete per il deployment mobile. Esso \`e dotato infatti di interfacce 2G/3G, WiFi, 802.15.4/Zigbee e GPS. Questo componente rappresenta l'unit\`a di elaborazione centrale del sistema nonch\'e il mezzo attraverso il quale i dati di conteggio vengono raccolti e trasmessi alla piattaforma cloud di Eurotech.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{DispositivoAcquisizioneImmagini.png}
  \caption{Sistema di conteggio dei passeggeri Eurotech, DynaPCN 10-20.}
  \label{fig:DynaPCN1020}
\end{figure}

Il cuore del sistema \`e il DynaPCN 10-20, visibile in figura \ref{fig:DynaPCN1020}, il quale si incarica dell'elaborazione video e conteggio degli attraversamenti. Esso nasconde al suo interno una FPGA programmata con una IP (Intellectual Property) proprietaria Eurotech che permette la ricostruzione in 3D delle immagini acquisite dalle telecamere stereoscopiche di cui \`e dotato il dispositivo.

Il funzionamento \`e il seguente: i proiettori di luce infrarossa illuminano la scena, le telecamere ad infrarossi acquisiscono le immagini nello spettro della luce infrarossa. Le due immagini vengono passate alla FPGA che, per mezzo di tecniche di ricostruzione stereoscopica\footnote{Per approfondire il funzionamento della Computer Stereo Vision si faccia riferimento all'appendice \ref{ComputerStereoVision} a pagina \pageref{ComputerStereoVision}.}, accelerate in hardware, permette di generare una immagine in scala di grigi dove l'informazione di profondit\`a \`e data dal colore del pixel. Queste immagini vengono quindi passate all'unit\`a di elaborazione centrale che vi applica un semplice algoritmo per il tracciamento dei passeggeri di cui discuteremo il funzionamento nel dettaglio nel prossimo paragrafo.

%% Interruzione di pagina
\newpage

\subsection{Algoritmo per il tracciamento dei passeggeri}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.75]{Algoritmov2.png}
  \caption{Funzionamento algoritmo tracciamento passeggeri Passenger Counter Eurotech.}
  \label{fig:SchemAlgrTrackPCNEurotech}
\end{figure}

%% TODO: Chiedere al prof se il conteggio viene effettuato sul ReliaGate o sul DynaPCN
L'unit\`a di elaborazione riceve in ingresso dalla FPGA una immagine in scala di grigi in cui \`e codificata l'informazione di profondit\`a tramite il colore dei pixel. I punti pi\`u vicini al rilevatore tendono al bianco, i punti pi\`u lontani tendono al nero.
Il cuore dell'algoritmo opera come segue:
\begin{enumerate}
\item Vengono rilevati i massimi locali all'interno dell'immagine in scala di grigi, i quali rappresentano le teste delle persone.
\item Traccia la posizione nel tempo di questi massimi locali all'interno del flusso di immagini.
\item Rileva quando questi massimi locali attraversano una linea di demarcazione virtuale, la quale indica l'ingresso o uscita dalla soglia e aggiorna i contatori.
\end{enumerate}
In figura \ref{fig:SchemAlgrTrackPCNEurotech} \`e riportata una schematizzazione di quanto accade durante il conteggio.

I dati raccolti dal DynaPCN 10-20 vengono quindi passati, tramite Ethernet, al gateway ReliGate 50-21 il quale li invia sulla rete cloud Eurotech per mezzo delle connessioni di cui \`e dotato. \`E altres\`\i\ possibile sfruttare il GPIO presente sul dispositivo per interfacciarsi, ad esempio, alle porte automatiche di un'ambiente, per bloccarle qualora si sia raggiunto il limite massimo impostato di occupanti.

%% Interruzione di pagina
\newpage

\subsection{Infrastruttura software}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{SoftwareStack.png}
  \caption{Stack software del sistema di Passenger Counter Eurotech.}
  \label{fig:StackSoftwarePCNEurotech}
\end{figure}

La piattaforma sulla quale si basa il Passenger Counter Eurotech \`e basata su una distribuzione custom di Linux realizzata per mezzo del progetto Yocto (Per approfondimenti circa questo strumento si veda il paragrafo \ref{Yocto}). Ad essa sono stati aggiunti i driver proprietari per le tecnologie Eurotech. Al di sopra di essa \`e presente la Java Virtual Machine la quale permette l'esecuzione del framework OSGi. 

OSGi (Open Services Gateway Initiative) \`e una organizzazione open standard fondata per specificare e mantenere lo standard OSGi, il quale \`e una specifica che descrive un sistema modulare e una service platform per il linguaggio Java che implementa un component model completo e dinamico. Applicazioni e componenti, i quali sono raccolti in ``bundle'' per il deployment, possono essere installati, fermati, aggiornati e disinstallati in remoto senza che sia necessario il reboot del sistema.

Su OSGi \`e basato il framework proprietario di Eurotech: Everyware Software Framework (ESF). Esso permette il deployment di applicazioni del cliente in modo flessibile tramite interfaccia grafica web, la quale \`e resa disponibile dall'infrastruttura M2M di Eurotech.

Nei prossimi capitoli vedremo come sia stato necessario adattare l'applicazione realizzata nel corso di questa tesi alla infrastruttura qui descritta, nonch\'e modificare parte dell'infrastruttura per aggiungere funzionalit\`a mancanti e necessarie alla nuova versione del Passenger Counter.

%% Interruzione di pagina
\newpage

\subsection{Problematiche di questa soluzione}\label{ProblemiPCNEurotech}
Questo sistema di conteggio dei passeggeri non \`e esente da problematiche, le quali sono state il punto di partenza di questa tesi. I problemi principali sono i seguenti:

\begin{enumerate}
\item Le ottiche del dispositivo di acquisizione delle immagini sono costose e difficili da reperire.
\item L'utilizzo di una FPGA per la ricostruzione delle informazioni di profondit\`a fa aumentare i costi di produzione.
\item Per come \`e stata implementata la soluzione \`e necessario installare un gateway per ogni DynaPCN 10-20 presente nell'ambiente. Anche questo contribuisce ad aumentare i costi di produzione del sistema e ne diminuisce l'appetibilit\`a sul mercato.
\end{enumerate}

Vedremo che nella versione del Passenger Counter realizzata nel corso di questa tesi sono state risolte in buona parte tutte queste problematiche, con vari gradi di successo.

%% Interruzione di pagina
\newpage

\section{Tecnologie utilizzate durante lo sviluppo della tesi}\label{TecnologieTesi}
In questa sezione andremo ad esaminare le tecnologie che sono state utilizzate per realizzare la nuova versione del Passenger Counter. Si tenga presente che le telecamere RealSense sono state usate solamente nella seconda implementazione del Passenger Counter.

%%TODO: Aggiungere nella bibliografia i riferimenti ai Datasheet delle telecamere
\subsection{Telecamere Intel RealSense}
Come dispositivi di acquisizione delle immagini sono state utilizzate le telecamere Intel RealSense. Esse sono telecamere ad infrarossi di nuova generazione che permettono la ricostruzione delle informazioni di profondit\`a della scena filmata sfruttando diverse tecniche di elaborazione delle immagini. Eurotech ha fornito due modelli di queste telecamere per lo sviluppo del progetto.

\subsubsection{Intel RealSense R200}

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=8cm]{R200.jpg}
  \caption{Telecamera R200.}
  \label{fig:R200}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=8cm]{r200_module.png}
  \caption{Componenti telecamera R200.}
  \label{fig:fig:R200Module}
\end{minipage}
\end{figure}

Il funzionamento di questo tipo di telecamere \`e lo stesso del Passenger Counter Eurotech. Sul modulo sono presenti due illuminatori ad infrarossi che illuminano la scena. Due telecamere stereoscopiche acquisiscono due immagini leggermente diverse dovute al diverso posizionamento. Analizzando le differenze tra le due immagini per mezzo di algoritmi di image processing ricostruiscono l'informazione di profondit\`a\footnote{Per approfondire il funzionamento della Computer Stereo Vision si faccia riferimento all'appendice \ref{ComputerStereoVision} a pagina \pageref{ComputerStereoVision}.}. Per ottenere una ricostruzione delle informazioni in real-time \`e stato usato un circuito ASIC che garantisse le prestazioni desiderate. Il funzionamento \`e riassunto in figura \ref{fig:SchemaFunzR200}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{DepthDataFlowR200.png}
  \caption{Schema funzionamento Intel RealSense R200.}
  \label{fig:SchemaFunzR200}
\end{figure}

%% Interruzione di pagina
\newpage

Specifiche tecniche:

\begin{table}[h!]
    \centering
	\begin{tabular}{|c|c|c|}
	\hline
	& Depth Stream & Color Stream \\ \hline
	Risoluzione massima & 640 x 480 pixel & 1920 x 1080 pixel\\ \hline
    Frame rate massimo & 90 fps & 60 fps \\ \hline
    FOV (WxH) & 56$^\circ$ x 43$^\circ$ & 70$^\circ$ x 43$^\circ$ \\ \hline
    Indoor Range & 0.7 - 3.5 m & - \\ \hline
    Outdoor Range & 10 m & - \\
    \hline
	\end{tabular}
    \caption{Specifiche tecniche Intel RealSense R200.}
    \label{table:R200Spec}
\end{table}

\subsubsection{Intel RealSense SR300}

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=8cm]{SR300.png}
  \caption{Telecamera SR300.}
  \label{fig:SR300}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=8cm]{SR300_module.png}
  \caption{Componenti telecamera SR300.}
  \label{fig:SR300Module}
\end{minipage}
\end{figure}

La telecamera S300 utilizza una tecnica di rilevamento tridimensionale nota come luce strutturata. Il proiettore presente sul modulo proietta un pattern noto sulla scena. La deformazione dell'immagine proiettata permette ai sistemi di visione di calcolare la profondit\`a degli oggetti colpiti ed ottenere altre informazioni sulla superficie\footnote{Per approfondire il funzionamento della Luce Strutturata si faccia riferimento all'appendice \ref{LuceStrutturata} a pagina \pageref{LuceStrutturata}.}. L'acquisizione delle immagini a infrarossi in questo caso viene effettuata da un singolo scanner 3D a luce strutturata. In figura \ref{fig:SchemaFunzSR300} \`e schematizzato il funzionamento della telecamera.
Anche i questo caso l'informazione di profondit\`a viene ricostruita utilizzando un circuito ASIC.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{DepthDataFlowSR300.png}
  \caption{Schema funzionamento Intel RealSense SR300.}
  \label{fig:SchemaFunzSR300}
\end{figure}

%% Interruzione di pagina
\newpage

Specifiche tecniche:

\begin{table}[h!]
    \centering
	\begin{tabular}{|c|c|c|}
	\hline
	& Depth Stream & Color Stream \\ \hline
	Risoluzione massima & 640 x 480 pixel & 1920 x 1080 pixel\\ \hline
    Frame rate massimo & 60 fps & 60 fps \\ \hline
    FOV (WxH) & 71$^\circ$ x 55$^\circ$  & 68$^\circ$ x 41$^\circ$ \\ \hline
    Indoor Range & 0.2 - 1.5 m & - \\ \hline
    Outdoor Range & - & - \\
    \hline
	\end{tabular}
    \caption{Specifiche tecniche Intel RealSense SR300.}
    \label{table:SR300Spec}
\end{table}

\subsubsection{Libreria librealsense e formato immagine}\label{librealsense}
Per l'interfacciamento software la Intel fornisce una libreria in C++ tramite la quale \`e possibile recuperare i frame e i dati di profondit\`a dalle telecamere: la libreria \textbf{librealsense}\footnote{Repository libreria all'indirizzo: https://github.com/IntelRealSense/librealsense}. Essa provvede all'inizializzazione delle telecamere, apertura dei vari stream video disponibili, impostazione di risoluzione e framerate. Gli stream disponibili sono: le immagini a colori, le immagini della telecamera a infrarossi e le immagini in cui \`e codificata l'informazione di profondit\`a. Nonostante usino tecnologie diverse entrambe le telecamere forniscono lo stesso formato in uscita.

Lo stream di profondit\`a \`e uno stream di immagini a 16 bit privi di segno in cui il valore di ogni pixel rappresenta la distanza dalla telecamera. La distanza \`e fornita a meno di un fattore di scala reperibile per mezzo di una semplice chiamata di funzione. \`E possibile quindi risalire alla distanza in metri di un pixel dalla telecamera semplicemente moltiplicando il valore del pixel per il fattore di scala.
Vi \`e una eccezione a questa codifica in quanto, un valore nullo \`e attribuito ai pixel per i quali non \`e stato possibile ricavare informazione sulla distanza, ci\`o pu\`o essere dovuto a problemi di range o esposizione dell'immagine.

Vi sono per\`o delle minime differenze tra le due telecamere per quanto riguarda la risoluzione della distanza calcolata:
\begin{itemize}
\item Per la Intel RealSense SR300: il formato dello stream di profondit\`a \`e di 16 bit privi di segno interpolati su un range di 8 metri nonostante il range della telecamere arrivi ad un massimo di 1,5 m. Ci\`o implica che la risoluzione della profondit\`a \`e 0,125 mm. 
\item Per la Intel RealSense R200: il formato \`e di 16 bit privi di segno interpolati su un range di circa 65 m. Ci\`o implica una risoluzione di profondit\`a di 1 mm.
\end{itemize}
Questo fatto non ha per\`o comportato problemi dal punto di vista della funzionalit\`a dell'applicazione in quanto la risoluzione e precisione delle telecamere non \`e critica per il corretto funzionamento del contatore ed il fattore di scala viene ricavato run-time.

%% Interruzione di pagina
\newpage

%% Copiato spudoratamente dalla mia presentazione su OpenCV.
\subsection{Libreria per l'image processing: OpenCV}\label{OpenCV}
Siccome una parte centrale della tesi verteva sull'elaborazione delle immagini \`e stato necessario integrare nel progetto l'uso della libreria OpenCV, ormai standard de facto nell'ambito dell'elaborazione delle immagini.

OpenCV, acronimo di Open Source Computer Vision, \`e una libreria software multi piattaforma finalizzata all'image processing real-time e alla computer vision. La libreria \`e rilasciata tramite licenza Berkeley Software Distribution (BSD) quindi \`e ad uso gratuito sia per fini accademici che commerciali. Contiene pi\`u di 2500 algoritmi pre-ottimizzati per le operazioni di image processing, computer vision e machine learning pi\`u comuni. Sviluppata in C/C++, \`e dotata di interfacce verso C, Python, Java e MATLAB. Poich\`e finalizzata all'utilizzo real-time, la libreria sfrutta molteplici interfacce per l'accelerazione hardware (CUDA, OpenCL, Intel Integrated Perfomance Primitives). OpenCV ha una struttura modulare, il che significa che il pacchetto include diverse librerie statiche o condivise.

\subsubsection{Moduli principali e finalit\`a}
\begin{itemize}
\item \textbf{Modulo core}: funzionalit\`a di base. Lo scopo del modulo \`e  definire interfacce e funzionalit\`a che permettano di semplificare la manipolazione di immagini e flussi video. Il modulo contiene le funzioni alla base delle libreria e ne definisce le strutture fondamentali nonch\'e gestisce la memoria. 
\item \textbf{Modulo highui}: High-level GUI e Media I/O. Il modulo HighGUI \`e stato progettato per fornire funzioni che permettano di provare le funzionalit\`a della libreria ed osservare i risultati velocemente. Fornisce semplici interfacce per creare e manipolare finestre che possano visualizzare immagini e aggiungere slider alle finestre, gestire semplici eventi come click del mouse e comandi da tastiera.
\item \textbf{Modulo imgproc}: image processing. Il modulo di Image Processing contiene funzioni e classi per la manipolazione di immagini. Ci\`o comprende:
    \begin{itemize}
    \item Image Filtering: funzioni per la convoluzione di immagini con un kernel, dilatazione di immagini, filtro Sobel, GaussianBlur ecc...
    \item Trasformazioni geometriche: ridimensionamenti, warping ecc...
    \item Funzioni per il disegno: permettono di disegnare semplici forme sulle immagini che vengono manipolate.
    \item Funzioni per l'analisi delle immagini: istogrammi, analisi strutturale e descrittori di forme, motion analysis e tracking di oggetti.
    \end{itemize}
\item \textbf{Modulo videoio}: lettura e scrittura di file, nonch\'e analisi di flussi video. Le funzioni principali implementate sono: analisi del movimento, sottrazione del background, rilevamento e tracciamento di oggetti.
\item \textbf{Modulo objdetect}: rilevazione di oggetti. Implementa funzioni e classi per il rilevamento e tracciamento di oggetti all'interno di immagini e flussi video. OpenCV ottiene tutto ci\`o facendo leva sui Haar Feature-based Cascade Classifier e Histogram of Oriented Gradients object detector.
\item \textbf{Modulo ml}: machine learning. La Machine Learning Library (MLL) \`e un insieme di classi e funzioni per la classificazione statistica, regressione e clustering dei dati. La maggior parte degli algoritmi di classificazione e regressione sono implementati come classi C++. Algoritmi implementati dal modulo:
    \begin{itemize}
    \item Artificial Neural Networks / Multi-Layer Perceptrons.
    \item Tree Classifier.
    \item Expectation Maximization algorithm.
    \item K-nearest Neighbors model.
    \item Logistic Regression.
    \item Normal Bayes Classifier.
    \item Support Vector Machines (SVM).
    \item Random forest predictor.
    \item Sochastic Gradient Descent SVM classifier.
    \end{itemize}
\end{itemize}

\noindent La struttura fondamentale della libreria OpenCV \`e l'oggetto Mat, il quale \`e usato come contenitore delle immagini. Esso \`e una classe C++ formata da due componenti principali:
\begin{itemize}
\item Un header: contenente informazioni come la dimensione della matrice, il metodo usato per salvarla, l'indirizzo dove \`e salvata ed un puntatore alla matrice.
\item Una matrice: contenente i valori dei pixel dell'immagine la cui dimensionalit\`a dipende dal metodo utilizzato per salvare l'immagine (B/N, RGB ecc...).
\end{itemize}
Questa struttura \`e la stessa per tutti i tipi di immagine, indipendentemente dal formato nella quale \`e salvata. Questa astrazione facilita notevolmente la manipolazione delle immagini in quanto ci si riconduce velocemente ad una matrice di pixel manipolabile. Questa conversione \`e garantita dal modulo imgcodecs, la quale funzione non \`e altro che recuperare i dati sui pixel dell'immagine sulla quale si sta lavorando e generare i metadati per il tipo Mat.

\subsubsection{Storia}
Lanciato ufficialmente nel 1999, il progetto OpenCV era parte di una iniziativa Intel finalizzata all'avanzamento delle applicazioni CPU-intensive che includeva ray tracing real-time e display 3D. Inizialmente le finalit\`a del progetto erano descritte come:
\begin{itemize}
\item Permettere l'avanzamento della computer vision fornendo non solo una libreria di codice open-source ma anche pre-ottimizzato per costruire l'infrastruttura base di applicazioni di computer vision.
\item Condividere conoscenza sulla computer vision fornendo una infrastruttura comune sulla quale tutti gli sviluppatori potessero costruire.
\item Far avanzare le applicazioni commerciali basate sulla computer vision rendendo il codice portabile, ottimizzato per le performance.
\end{itemize}
Ad oggi il supporto al progetto \`e dato dall'organizzazione no-profit OpenCV.org, la quale mantiene uno sviluppatore ed un sito per la documentazione. La libreria \`e ormai lo standard de facto per le applicazioni di computer vision e image processing, vista la sua diffusione e la community molto attiva.

Per lo sviluppo dell'applicazione di Passenger Counter la scelta di utilizzare questa libreria \`e stata quasi obbligata visto l'ottimo supporto e diffusione della libreria. Inoltre la vasta portabilit\`a e l'attenzione alle performance la rendono particolarmente adatto al caso embedded real-time.

%% Interruzione di pagina
\newpage

\subsection{Ambiente di sviluppo per l'OS: Yocto Project}\label{Yocto}
Per integrare nella piattaforma hardware gli strumenti software necessari al funzionamento dell'applicativo \`e stato indispensabile realizzare una distribuzione Linux customizzata adatta alle nostre esigenze. Per farlo \`e stato utilizzato il progetto Yocto, standard industriale de facto per realizzare distribuzioni Linux embedded.

Yocto Project \`e un insieme di strumenti open source finalizzati alla creazione di distribuzioni Linux per sistemi embedded, corredate da toolchain di cross-compilazione ed emulatori, che siano indipendenti dall'architettura hardware. Yocto Project \`e quindi un progetto ombrello sotto il quale si raccolgono vari sotto-progetti volti allo sviluppo di sistemi Linux embedded.

Le architettura supportate dal progetto sono le pi\`u diffuse nell'ambito embedded: ARM, MIPS, PowerPC e x86/x86-64. Una parte fondamentale di questo progetto \`e il build system open source basato sull'architettura OpenEmbedded. Questa implementazione di OpenEmbedded \`e chiamata Poky.

OpenEmbedded \`e framework software usato per creare distribuzioni Linux embedded. Il build system \`e basato sulle ricette BitBake le quali sono degli script bash specializzati che automatizzano la compilazione e la installazione di pacchetti software. BitBake stesso \`e uno dei componenti fondamentali di Yocto Project: \`e un build automation software, ovvero un tool che legge dei metadati sotto forma di ``ricette'' ed esegue task in base a dette ricette. Le ricette BitBake consistono di un URL che punta alla sorgente del pacchetto software, dipendenze e opzioni di compilazione ed installazione. Durante il processo di build sono usate per tenere traccia delle dipendenze ed effettuare cross-compilazione dei pacchetti affinch\'e sia possibile installarli sulla target distribution.

\subsubsection{Componenti del progetto Yocto}
\begin{itemize}
\item OpenEmbedded: \`e un build framework per embedded Linux. Permette la creazione di distribuzioni Linux complete per sistemi immersi e offre un ambiente di cross-compilazione completo. Il build system \`e basato su ricette BitBake.
\item BitBake: \`e un build automation software, ovvero uno strumento software che legge dei metadati ed esegue task in base a tali metadati. \`E uno dei componenti fondamentali di Yocto Project.
\item Poky: \`e una reference distribution di Yocto Project. Contiene l'OpenEmbedded Build System(BitBake e OpenEmbedded) assieme a un insieme di metadati che permettono di cominciare a costruire la propria distribuzione Linux embedded customizzata.
\item Layers: il build system del progetto Yocto \`e composto da layer. I layer sono una collezione logica di ricette che rappresentano stack di applicazioni o Board Support Packaged (BSP). I BSP contengono i pacchetti e i driver fondamentali necessari per costruire una distribuzione Linux per una specifica board o architettura. Solitamente questi BSP sono mantenuti dai produttori dell'hardware e sono l'interfaccia tra l'OS e l'hardware che lo esegue.
\end{itemize}

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{yocto-environment.png}
  \caption{Workflow progetto Yocto.}
  \label{fig:WorkflowYocto}
\end{figure}

\subsubsection{Processo di build di una distribuzione}
Dopo aver configurato e lanciato il processo di build il build system provvede ad eseguire le seguenti operazioni per ogni pacchetto e applicazione che si desidera installare:
\begin{enumerate}
\item Recupera autonomamente i sorgenti remoti e locali.
\item Procede alla cross-compilazione dei sorgenti.
\item Genera i file .deb/.rpm/.ipk dipendentemente dalla configurazione.
\end{enumerate}
Una volta che sono stati generati tutti i pacchetti richiesti dalla configurazione, procede alla creazione dell'immagine della distribuzione: genera quindi il filesystem, installa i pacchetti e d\`a origine all'immagine che pu\`o essere eseguita sulla macchina target. In seguito \`e possibile generare la toolchain per la cross-compilazione di sorgenti per l'immagine creata. Questo processo dar\`a origine ad un piccolo installer il quale installer\`a la toolchain nella macchina host, dedicata allo sviluppo delle applicazioni per la piattaforma target.

%% Interruzione di pagina
\newpage

Per lo sviluppo dell'applicazione del Passenger Counter \`e stato necessario realizzare l'immagine della distribuzione Linux opportunamente modificata e la toolchain di cross-sviluppo adattandole alle necessit\`a dell'applicazione. Siccome si \`e passati attraverso diverse tecnologie il processo \`e stato ripetuto pi\`u volte per poter integrare le nuove tecnologie all'interno della toolchain.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=1]{cross-development-toolchains.png}
  \caption{Toolchain di cross-sviluppo del progetto Yocto.}
  \label{fig:ToolchainYocto}
\end{figure}

Il processo di build e le configurazioni necessarie a generare la distribuzione usata nel corso della tesi saranno discussi in dettaglio nel capitolo \ref{CapitoloYocto} a pagina \pageref{CapitoloYocto}.

% OSGi tmp location
% %% Interruzione di pagina
% \newpage
% 
% \subsection{Framework OSGi}\label{OSGi}
% In pausa finch\`e non sei sicuro di usarlo.

\chapter{Analisi degli ambienti di sviluppo disponibili per l'image processing embedded}\label{CapitoloAnalisiAmbienti}
Nella fase iniziale del progetto si \`e studiato quale potesse essere l'ambiente di sviluppo adatto all'applicazione. Sono state considerate principalmente due possibilit\`a analizzate nelle sezioni seguenti.

\section{Ambiente di sviluppo Xilinx}

\begin{table}[h!]
    \centering
	\begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{| c |}{ \bf Ambiente Xilinx} \\
    \hline
    \hline
	Piattaforma hardware & Avnet Zedboard \\ \hline
	Acquisizione video & HDMI Webcam \\ \hline
    Ambiente di sviluppo & Xilinx SDSoC \\ \hline
    Piattaforma software & Bare-metal\\
    \hline
	\end{tabular}
    \caption{Riassunto componenti ambiente di sviluppo Xilinx.}
\end{table}

Inizialmente si era pensato di usare una scheda di sviluppo Xilinx che integrasse una CPU ARM e una FPGA, in modo che fosse possibile sfruttare i tool di sintesi di alto livello messi a disposizione da Xilinx per lo sviluppo per accelerare in hardware le parti dell'applicativo pi\`u pesanti dal punto di vista computazionale, in modo semplice e con un approccio di alto livello. Utilizzando questo ambiente di sviluppo integrato sarebbe stato possibile realizzare applicazioni ad altissime prestazioni con la flessibilit\`a di un linguaggio ad alto livello come C/C++. Uno dei target di punta di questi sistemi di sviluppo \`e appunto l'image processing e come supporto vengono fornite anche librerie proprietarie pre-ottimizzate per l'hardware Xilinx.

%% Interruzione di pagina
\newpage

\subsection{Hardware}
La piattaforma target sarebbe stata una Avnet Zedboard. Essa rappresenta un development kit completo basato sull'SoC Xilinx Zynq-7000, il quale contiene al suo interno una CPU ARM ed una FPGA. Si \`e voluto usare questa board poich\'e compatibile con il sistema di sviluppo SDSoC per la sintesi di alto livello, di cui parleremo nel seguito.

Specifiche tecniche:

\begin{table}[h!]
    \centering
	\begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{| c |}{ \bf Specifiche tecniche ZYNQ-7000 XC7Z020} \\
    \hline
    \hline
    SOC & Dual-core ARM Cortex-A9 + FPGA Kintex-7 \\ \hline
    Memoria & 512 MB DDR3 + 256 Mb Quad-SPI Flash + 4GB SD card \\ \hline
    Espansione & FMC (Low Pin Count) + Pmod headers (2x6) \\ \hline
    Video & HDMI output (1080p60 + audio) + VGA \\
    \hline
	\end{tabular}
    \caption{Specifiche tecniche Avnet Zedboard.}
\end{table}

Purtroppo, avendo la scheda una sola porta HDMI disponibile solo in output \`e stato necessario procurarsi un modulo aggiuntivo da collegarsi all'header FMC per avere a disposizione anche una porta HDMI in ingresso.

In figura \ref{fig:Zynq7000} \`e riportato il diagramma del SoC Zynq-7000.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{Zynq7000.jpg}
  \caption{Diagramma del SoC Zynq-7000.}
  \label{fig:Zynq7000}
\end{figure}

%% Interruzione di pagina
\newpage

\subsection{Piattaforma per lo sviluppo SDSoC}
La piattaforma per lo sviluppo SDSoC (Software-defined system-on-chip) permette uno sviluppo integrato dell'applicativo e dell'IP hardware utilizzando un linguaggio di alto livello come il C o il C++. Vista la maggiore familiarit\`a con i linguaggi di alto livello l'utilizzo di questo ambiente di sviluppo avrebbe comportato una maggior flessibilit\`a e velocit\`a nello sviluppo con il vantaggio di potersi avvalere dell'accelerazione hardware sfruttando la FPGA presente sul SoC.

\begin{wrapfigure}{r}{0.5\textwidth}
\begin{center}
  \includegraphics[width=0.48\textwidth]{XilinxSDSoC.png}
\end{center}
\caption{Workflow SDSoC.}
\end{wrapfigure}

SDSoC permette di scrivere l'intera applicazione in codice di alto livello. In fase di compilazione si va a specificare quale funzione si vuole eseguire sulla FPGA, per sfruttarne l'accelerazione hardware. A seguito dell'intervento del programmatore per specificare mappatura della memoria e ottimizzazioni per l'esecuzione in hardware, viene generato il bitstream necessario alla programmazione della FPGA assieme all'applicativo destinato alla CPU. 

I compilatori del sistema SDSoC analizzano il programma per determinare il data flow tra le funzioni hardware e software e generano un application specific system-on-chip per realizzare il programma. Per raggiungere alte performance ogni funzione in hardware viene eseguita su un thread indipendente; il compilatore genera i componenti hardware e software che assicurano la sincronizzazione tra i thread hardware e software, mentre abilita la computazione e comunicazione in pipeline. Il codice applicativo pu\`o includere pi\`u funzioni hardware, molteplici istanze di una specifica funzione hardware, e chiamare la funzione hardware da parti diverse del programma.

L'ambiente di sviluppo integrato SDSoC supporta il workflow di sviluppo software tramite analisi delle performance di sistema, profiling, compilazione, linking e debugging. Inoltre, l'ambiente fornisce un tool per la stima delle performance dell'applicativo per poter effettuare analisi di tipo ``what if'' permettendo di esplorare diverse soluzioni prima di impegnarsi in una compilazione interamente in hardware. 

I compilatori di SDSoC mirano ad una piattaforma base ed invocano il tool Vivado High-Level Synthesis (HLS) per compilare funzioni C/C++ sinstetizzabili nella logica programmabile. Quindi generano un systema hardware completo che includa DMA, interconnesioni, buffer hardware e altre IP, e un bitstream per la FPGA invocando i tool della Vivado Design Suite. Per assicurarsi che tutte le chiamate alle funzioni hardware preservino il comportamento originale, i compilatori generano degli stub software e dati di configurazione specifici per il sistema. Il programma include chiamate di funzione ai driver necessari per usare i blocchi IP generati. L'applicazione e il software generato sono compilati e linkati usando toolchain standard GNU.

%% Interruzione di pagina
\newpage

\subsection{Compatibilit\`a con OpenCV e liberia AuvizCV}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{AuvizCV.jpg}
  \caption{Risultati benchmark.}
  \label{fig:BenchmarkAuvizCV}
\end{figure}

Come ulteriore punto a favore di questo ambiente, Xilinx vantava la compatibilit\`a del sistema con OpenCV per l'esecuzione su CPU mentre rendeva disponibile una libreria compatibile con OpenCV per l'esecuzione su FPGA. Quest'ultima prende il nome di \textbf{AuvizCV}, la quale non \`e altro che un sottoinsieme delle funzioni disponibili in OpenCV pre-ottimizzate per l'esecuzione sulle FPGA Xilinx. Questa libreria sfrutta le potenzialit\`a di SDSoC in quanto \`e stata scritta interamente in codice di alto livello e, in fase di compilazione, le funzioni utilizzate vengono sintetizzate sulla FPGA. Questa API si mantiene coerente con OpenCV per facilitare l'implementazione e l'accelerazione in harware. In figura \ref{fig:BenchmarkAuvizCV} sono riportati i risultati dei benchmark confrontando l'accelerazione di alcuni algoritmi di OpenCV usando varie tecnologie tra cui AuvizCV.

%% Interruzione di pagina
\newpage

%TODO: Approfondire?
\section{Ambiente di sviluppo Intel - Yocto}
Come seconda opzione si \`e considerata la possibilit\`a di usare l'hardware reso disponibile da Eurotech e realizzare da zero l'infrastruttura software sfruttando il progetto Yocto.

\begin{table}[h!]
    \centering
	\begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{| c |}{ \bf Ambiente Intel - Yocto} \\
    \hline
    \hline
	Piattaforma hardware & Eurotech ReliaGate 20-25 \\ \hline
	Acquisizione video & Webcam - Telecamere Intel RealSense \\ \hline
    Ambiente di sviluppo & Toolchain generata con Yocto \\ \hline
    Piattaforma software & Distribuzione Poky Linux custom \\
    \hline
	\end{tabular}
    \caption{Riassunto componenti ambiente di sviluppo Intel - Yocto.}
\end{table}

\subsubsection{Hardware}
L'azienda ha fornito come piattaforma hardware un ReliaGate 20-25 nonch\'e le telecamere Intel RealSense.

\begin{table}[ht]
\begin{minipage}[b]{0.56\linewidth}
\centering
    \centering
	\begin{tabular}{|l|c|}
    \hline
    \multicolumn{2}{| c |}{ \bf ReliaGate 20-25-33} \\
    \hline
	\hline
    Processore & \makecell{Intel E3827  \\ Dual Core 1.75GHz x86-64} \\ \hline
    Memoria & \makecell{2GB 1333MHz DDR3L ECC \\ 8GB eMMC} \\ \hline
    Connettivit\`a & \makecell{1x USB 3.0 \\ 2x USB 2.0 \\ 2x RS-232/RS-422/RS-485} \\ \hline
    Video & mini DisplayPort \\
    \hline
	\end{tabular}
    \caption{Specifiche tecniche ReliaGate 20-25.}
    \label{table:SpecificheWebcam}
\end{minipage}\hfill
\begin{minipage}[b]{0.4\linewidth}
  \centering
  \includegraphics[width=60mm]{ReliaGATE2025.jpg}
  \captionof{figure}{ReliaGate 20-25.}
  \label{fig:ReliaGate2025}
\end{minipage}
\end{table}

\subsubsection{Software e piattaforma per lo sviluppo}
Gli strumenti che si sarebbe andati ad utilizzare sono quelli di cui si \`e parlato nel paragrafo \ref{TecnologieTesi} a pagina \pageref{TecnologieTesi}: Yocto per la realizzazione della distribuzione Linux, la toolchain di Yocto per la cross-compilazione dei sorgenti, OpenCV per l'image processing.

Solo in un secondo momento sono state integrate nel sistema le telecamere RealSense con annessa libreria a causa delle prestazioni non soddisfacenti della prima versione del software.

I vantaggi di questa piattaforma sono dovuti in gran parte all'architettura del processore Intel: avendo a disposizione un x86 il software disponibile \`e molto ampio e si \`e in grado di avere grande flessibilit\`a nello sviluppo dell'applicazione.

%% Interruzione di pagina
\newpage

\section{Motivazioni per la scelta finale}
Si \`e infine scelto di utilizzare l'ambiente di sviluppo Intel - Yocto per i seguenti motivi:
\begin{itemize}
\item \textbf{Flessibilit\`a}: la possibilit\`a di aggiungere librerie e strumenti software grazie al progetto Yocto permetteva di avere un ambiente di lavoro molto pi\`u flessibile della controparte Xilinx.
\item \textbf{Completezza}: la libreria AuvizCV, seppur molto performante, mancava di moltissime funzioni rispetto ad una installazione completa di OpenCV, la quale poteva essere comodamente installata sul ReliaGate. AuvizCV infatti possiede solamente 50 dei 2500 e pi\`u algoritmi presenti sulla versione completa di OpenCV.
\item \textbf{Performance}: la CPU Intel \`e molto pi\`u performante dell'ARM presente sulla Zedboard. Siccome sarebbe stato comunque necessario eseguire parte del codice OpenCV sulla CPU, si sarebbero perse molte performance usando l'ambiente Xilinx.
\item \textbf{Compatibilit\`a}: anche se inizialmente non era previsto l'impiego delle telecamere RealSense, la board Xilinx non sarebbe potuta essere compatibile con queste ultime vista la mancanza di una porta USB 3.0 e il supporto software.
\end{itemize}

Nei prossimi capitoli procederemo con la descrizione del codice che \`e stato realizzato per la realizzazione del Passenger Counter.

\chapter{Passenger Counter con sottrazione del background}\label{CapitoloPCN}
In questo capitolo tratteremo la prima versione realizzata del contatore di passeggeri. Questa prima versione non faceva uso delle telecamere RealSense e presentava alcune criticit\`a che vedremo nel seguito. 

Inizialmente descriveremo l'hardware utilizzato, tratteremo le tecniche implementate, quindi passeremo a descrivere la struttura del codice ed il suo funzionamento. Infine parleremo dei risultati ottenuti da questa implementazione e le problematiche principali.

\section{Componenti hardware}\label{HardwarePCN}
Per questa prima implementazione del contatore di passeggeri \`e stata utilizzata una semplice webcam.

\begin{table}[ht]
\begin{minipage}[b]{0.56\linewidth}
\centering
\begin{tabular}{ | l | c | }
    \hline
    \multicolumn{2}{| c |}{ \bf Ligitech QuickCam Pro 5000} \\
    \hline
    \hline
    Risoluzione & 640 x 480 pixel \\ \hline
    Framerate & 30 fps \\ \hline
	Connessione & USB 2.0 \\ \hline
    Tecnologia & driverless \\ \hline
    \end{tabular}
    \caption{Specifiche tecniche webcam.}
    \label{table:SpecificheWebcam}
\end{minipage}\hfill
\begin{minipage}[b]{0.4\linewidth}
\centering
\includegraphics[width=40mm]{WebCam.jpg}
\captionof{figure}{Logitech QuickCam Pro 5000.}
\label{fig:Webcam}
\end{minipage}
\end{table}

Essendo driverless veniva riconosciuta senza problemi dal ReliaGate ed \`e stato possibile leggere i frame in ingresso tramite OpenCV senza l'installazione di componenti software aggiuntivi.

%% Interruzione di pagina
\newpage

\section{Algoritmo per la sottrazione del background}
La sottrazione del background \`e una tecnica molto utilizzata dell'image processing e della computer vision, nella quale viene estratto il primo piano dell'immagine per essere ulteriormente manipolato. La background subtraction \`e una tecnica largamente utilizzata per rilevare soggetti in movimento all'interno di flussi video catturati da una telecamera statica.

\subsection{Principio di funzionamento}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.6\textwidth]{SchemaBackSub.png}
  \caption{Sottrazione del background.}
  \label{fig:SchemaBackSub}
\end{figure}

Come riassunto in figura \ref{fig:SchemaBackSub} l'obiettivo della sottrazione del background \`e l'estrazione delle regioni in movimento (o Foreground, primo piano) comparando il frame corrente con un modello dello sfondo (Background) costruito a partire dai frame precedenti del flusso video. Il prodotto di questa computazione \`e la Foreground Mask, una immagine binaria contenente i pixel che appartengono agli oggetti in movimento nella scena.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.65\textwidth]{Background_Subtraction_Tutorial_Scheme.png}
  \caption{Estrazione della Foreground Mask.}
  \label{fig:EstrazioneFGMask}
\end{figure}

\noindent L'algoritmo si compone di due fasi:

\begin{enumerate}
\item L'inizializzazione del background: dove viene creato il primo modello del background (BG Model).
\item L'aggiornamento del modello: dove le informazioni sul modello del background vengono aggiornate per tenere conto degli eventuali cambiamenti nella scena osservata.
\end{enumerate}

%% Interruzione di pagina
\newpage

\subsection{Algoritmo OpenCV: MOG2}
La funzione utilizzata nello sviluppo di questa versione del Passenger Counter, resa disponibile da OpenCV, implementa l'algoritmo Gaussian Mixture-based Background/Foreground Segmentation Algorithm nella sua versione migliorata nota come MOG2.

L'algoritmo usato all'interno del codice di questa versione del contatore di passeggeri \`e basata su due paper pubblicati da Z.Zivkovic, \textit{Improved adaptive Gausian mixture model for background subtraction} del 2004 e \textit{Efficient Adaptive Density Estimation per Image Pixel for the Task of Background Subtraction} del 2006.

Esso modella ogni pixel del background con una mixture di distribuzioni gaussiane selezionando automaticamente il numero appropriato da utilizzare. I pesi della mixture di gaussiane rappresentano il tempo per il quale quei colori permangono all'interno della scena. I colori che pi\`u probabilmente appartengono al background sono quelli che rimangono per un tempo maggiore all'interno della scena e sono i pi\`u statici.

Tipicamente, per facilitare l'elaborazione delle immagini a valle dell'applicazione dell'algoritmo di background subtraction, si ricava una immagine in scala di grigi nella quale i pixel neri rappresentano il background i pixel bianchi il primo piano detta foreground mask. Nel caso dell'algoritmo usato per il Passenger Counter, le ombre vengono evidenziate con un colore grigio. 

Questo algoritmo \`e reso disponibile per mezzo della struttura \verb|BackgroundSubtractorMOG2| dalla libreria OpenCV. Nei prossimi paragrafi vedremo come \`e stato utilizzato all'interno del codice del Passenger Counter.

Qui di seguito \`e riportato un esempio di funzionamento.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.49\textwidth]{resframe.jpg}
  \includegraphics[width=.49\textwidth]{resmog2.jpg}
  \caption{Esempio funzionamento BackgroundSubtractorMOG2.}
  \label{fig:EsempioBackSub}
\end{figure}

In figura \ref{fig:EsempioBackSub} si pu\`u notare come le persone vengano correttamente associate alla foreground mask poich\`e evidenziate dal colore bianco e come le ombre associate agli oggetti in movimento siano evidenziate dal colore grigio.

%% Interruzione di pagina
\newpage

\section{Algoritmo per il rilevamento dei contorni}\label{AlgoritmoFindContours}
Nella computer vision i metodi di blob detection sono finalizzati a rilevare regioni in un'immagine digitale che differiscano per propriet\`a, come luminosit\`a o colore, rispetto alle regioni circostanti. I blob, quindi, sono regioni di un'immagine nelle quali alcune propriet\`a sono costanti.

Nel caso dell'applicazione del Passenger Counter le regioni di interesse, o blob, saranno i passeggeri che vogliamo tracciare all'interno del raggio visivo della telecamera. Per andare a rilevare questi blob si \`e usata la funzione \verb|findContours|.

\subsection{Principio di funzionamento}
Alla base dell'algoritmo implementato dalla funzione \verb|findContours| c'\`e il Chain Code noto come \textit{Freeman Chain Code of Eight Directions} (FCCE)\cite{freeman61}. Un Chain Code \`e un algoritmo di compressione lossless per immagini monocrome. Il Freeman Chain Code, nello specifico, prevede la codifica dei poligoni presenti in una immagine come una sequenza di step in una delle 8 direzioni rappresentate in figura \ref{fig:Freeman}, in modo tale che ogni componente connesso o blob sia codificato separatamente. Ogni step, successivo al primo, viene quindi rappresentato da un numero intero compreso tra 0 e 7.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{freeman.png}
  \caption{Funzionalmento Freeman Chain Code.}
  \label{fig:Freeman}
\end{figure}

Si consideri in ingresso una immagine monocroma, la quale, ad esempio, rappresenta una scena a cui \`e stato sottratto lo sfondo. Il foreground sar\`a rappresentato per mezzo di pixel bianchi mentre lo sfondo con pixel neri. L'algoritmo si compone delle seguenti operazioni.

Anzitutto si scannerizza l'immagine per individuare il punto dal quale far partire la sequenza che descrive il poligono. Il punto di partenza \`e rappresentato dalle coordinate del pixel all'interno dell'immagine.

%% Interruzione di pagina
\newpage

\begin{wrapfigure}{r}{0.4\textwidth}
\begin{center}
  \includegraphics[width=0.38\textwidth]{boundary.png}
\end{center}
\caption{Analisi pixel appartenenti al bordo del poligono.}
\end{wrapfigure}

Quindi si passa ad analizzare gli 8 pixel adiacenti al punto di partenza, la direzione del primo pixel appartenente al bordo del poligono \`e salvata nella sequenza. Si considera un pixel appartenente al bordo dell'immagine se condivide almeno un lato o un vertice con almeno un pixel appartenente allo sfondo (cio\`e un pixel nero). Si ripete il procedimento fino a trovare la fine della sequenza, rappresentata dalla coordinata del punto di partenza. A questo punto la sequenza descrive completamente il blob.

Dunque si ricomincia la scansione dell'immagine per individuare altri poligoni fino a che anche l'ultimo pixel dell'immagine \`e stato analizzato.

Alla fine di questa operazione otterremo tante sequenze di interi, le quali rappresentano i blob, quanti sono i poligoni presenti nel foreground.

\subsection{Funzione OpenCV: findContours}
L'algoritmo utilizzato per il Passenger Counter \`e descritto in Suzuki, S. and Abe, K., \textit{Topological Structural Analysis of Digitized Binary Images by Border Following}, CVGIP 30 1, pp 32-46 del 1985.

Questo algoritmo per la rilevazione dei contorni \`e reso disponibile dalla funzione \verb|findContours()| di OpenCV ed \`e stato utilizzato anche nella seconda versione del Passenger Counter.

Qui di seguito \`e riportato un esempio di funzionamento.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.75]{findContours.png}
  \caption{Esempio funzionamento findContours.}
  \label{fig:EsempioFindContours}
\end{figure}

In figura \ref{fig:EsempioFindContours} l'immagine a sinistra rappresenta l'immagine di input in bianco e nero ricavata dall'immagine a colori. L'immagine a destra \`e stata ricavata disegnando i contorni in giallo, ricavati per mezzo della funzione \verb|findContours()| sull'immagine a colori.

%% Interruzione di pagina
\newpage

\section{Struttura del codice}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{PCNUML.png}
  \caption{Diagramma UML del Passenger Counter a sottrazione del background.}
  \label{fig:PCNUML}
\end{figure}

In figura \ref{fig:PCNUML} \`e riportata la struttura del programma. Le funzioni dei vari componenti del progetto sono le seguenti:
\begin{enumerate}
\item \textbf{Passenger}: implementa la struttura adatta a rappresentare e memorizzare i passeggeri all'interno del programma.
\item \textbf{PCN}: implementa il contatore di passeggeri vero e proprio. Questa classe si incarica dell'interfacciamento con le telecamere, l'elaborazione delle immagini e dei dati, espone metodi per controllare il funzionamento real-time ed effettua il conteggio degli attraversamenti. Permette inoltre la parallelizzazione dei contatori: \`e infatti possibile istanziare un contatore per telecamera.
\item \textbf{main}: implementa l'interfaccia utente per gestire i settaggi run-time dei PCN sfruttando i metodi pubblici resi disponibili dalla classe PCN.
\end{enumerate}

%% Interruzione di pagina
\newpage

\section{Classe Passenger}\label{ClassePassenger}
Per il tracciamento dei passeggeri \`e stata creata una classe che contenesse le informazioni necessarie per il corretto tracking delle entit\`a.

\begin{lstlisting}[language=C++, caption=passenger.h, style=customcpp]
class Passenger {

  public:

    // Constructor
    Passenger(int id, Point center, int newAge);

    // Selectors
    int getPid() {return pid;};
    int getAge() {return age;};

    vector<Point> getTracks(){return tracks;};
    Scalar getTrackColor(){return trackColor;};

    Point getCurrentPoint(){return tracks[tracks.size()-1];};
    Point getLastPoint(){return tracks[tracks.size()-2];};

    // Methods
    void updateCoords(Point newCenter);
    void updateAge(){age++;return;};
    void resetAge(){age = 0; return;};

  private:
    int pid;    // Passenger ID
    int age;    // Passenger age

    vector<Point> tracks;
    Scalar trackColor;

};
\end{lstlisting}

\begin{lstlisting}[language=C++, caption=passenger.cpp, style=customcpp]
Passenger::Passenger(int id, Point center, int newAge = 0) {
    pid = id;
    age = newAge;

    tracks.push_back(center);
    trackColor = Scalar(rand() % 255, rand() % 255, rand() % 255);
}

void Passenger::updateCoords(Point newCenter) {
    tracks.push_back(newCenter);

    // If too many tracking points remove older points
    if(tracks.size() > MAX_TRACK_LENGTH)
        tracks.erase(tracks.begin());

    return;
}
\end{lstlisting}

\noindent Questa classe ha tre obiettivi principali:
\begin{enumerate}
\item Identificare univocamente le entit\`a che entrano nel raggio d'azione della telecamera.
\item Memorizzare lo storico delle posizioni occupate da tali entit\`a.
\item Tenere traccia del tempo per il quale permangono all'interno del raggio visivo della telecamera.
\end{enumerate}

\noindent Queste necessit\`a sono state assolte per mezzo delle seguenti variabili:
\begin{enumerate}
\item \textbf{pid}: \`e un intero assegnato in fase di registrazione all'entit\`a-passeggero. Esso andr\`a ad identificarlo univocamente all'interno del programma.
\item \textbf{tracks}: \`e un vettore di coordinate che conservano la posizione dei passeggeri. Per l'implementazione si \`e scelto di usare la classe template \verb|vector|, la quale rappresenta una struttura LIFO (Last In First Out), per mantenere la compatibilit\`a con la funzione OpenCV \verb|polylines|, dedicata al plot delle tracce dei passeggeri. La struttura viene riempita come una pila: le coordinate del centro del passeggero pi\`u vecchie si trovano in fondo, quelle pi\`u nuove in cima. Quando si \`e raggiunto il limite massimo di posizioni memorizzate, all'aggiunta di una nuova coordinata segue la cancellazione della coordinata pi\`u vecchia, contenuta nella locazione in fondo alla pila.

Il massimo numero di posizioni mantenute in memoria \`e stato deciso in base alla durata massima della cronologia di posizioni che si vuole mantere. Infatti, ad ogni frame ricevuto dalla telecamera, corrisponde una nuova posizione del passeggero. Nel caso si abbia un frame rate di 30 fps, una lunghezza massima di 60 posizioni memorizzate implicano 2 secondi di cronologia delle posizioni.
\item \textbf{age}: \`e un intero che va a rappresentare da quanto tempo il passeggero non viene rilevato nel raggio d'azione della telecamera. Oltre un certo valore di questo parametro il passeggero verr\`a rimosso dalla memoria del contatore. 

Ad ogni frame ricevuto dalla telecamera si aggiornano le coordinate e si resetta la variabile \verb|age| di tutti i passeggeri rilevati. I passeggeri non pi\`u nel raggio d'azione della telecamera, invece, vengono aggiornati con l'ultima posizone conosciuta e ``invecchiano'' ad ogni frame, viene aggiornata cio\`e la variabile \verb|age|. Raggiunta una soglia massima i passeggeri vengono rimossi dalla memoria. Anche in questo caso la permanenza in memoria dei passeggeri non pi\`u rilevati \`e dettata dal frame rate della telecamera. Se viene impostata una \verb|age| massima di 60, e si ha una telecamera da 30 fps, dopo due secondi di assenza dal raggio visivo della telecamera i passeggeri vengono rimossi dal vettore.

\end{enumerate}

Oltre a queste variabili, le cui finalit\`a sono puramente funzionali, \`e stata implementata la variabile \verb|trackColor|, la quale \`e un vettore di interi che vanno a rappresentare un colore RGB randomizzato. Viene usato per disegnare le tracce lasciate dal transito dei passeggeri nel raggio visivo della telecamera con colori diversi, in modo tale che sia facile verificare che il tracciamento \`e avvenuto correttamente.

%% Interruzione di pagina
\newpage

\noindent Infine gli altri metodi implementati sono i seguenti:

\begin{itemize}
\item \verb|getTracks()|: ritorna l'intero storico delle posizioni rilevate del passeggero.
\item \verb|getTrackColor()|: ritorna il colore col quale tracciare la cornologia delle posizioni del passeggero.
\item \verb|getCurrentPoint()|: ritorna la posizione corrente assunta dal passeggero all'interno del raggio visivo della telecamera. Questa posizione \`e salvata come \verb|Point|, tipo definito da OpenCV, il quale rappresenta una coordinata in pixel.
\item \verb|getLastPoint()|: ritorna l'ultima posizione conosciuta del passeggero.
\item \verb|updateCoords()|: aggiorna l'ultima coordinata conosciuta del passeggero.
\item \verb|updateAge()|: aggiorna la variabile \verb|age| del passeggero.
\item \verb|resetAge()|: resetta a zero la variabile \verb|age| del passeggero.
\end{itemize}

\subsubsection{Utilizzo della classe Passenger}
Si noti che questa rappresentazione dei passeggeri all'interno del contatore \`e stata usata anche dalla versione successiva del Passenger Counter con telecamere RealSense. Per dettagli si veda il paragrafo \ref{StrutturaRSPCN} a pagina \pageref{StrutturaRSPCN}.

%% Interruzione di pagina
\newpage

\section{Classe PCN}

Questa classe ha come obiettivi principali:
\begin{itemize}
\item Implementare un oggetto contatore che fornisca interfacce verso l'esterno per poterne controllare i comportamenti.
\item Effettuare l'interfacciamento al dispositivo di acquisizione delle immagini.
\item Implementare il kernel del programma, ovvero la parte di codice incaricata di effettuare il conteggio degli attraversamenti della soglia da parte dei passeggeri sfruttando gli algoritmi di cui abbiamo parlato in precedenza.
\item Permettere l'esecuzione parallela di pi\`u contatori contemporaneamente, uno per telecamera collegata al unit\`a di elaborazione.
\item Permettere la visualizzazione e la registrazione dei risultati dell'elaborazione.
\end{itemize}

Qui di seguito \`e riportato l'header della classe PCN:
\begin{lstlisting}[language=C++, caption=PCN.h, style=customcpp]
class PCN {

  public:

    PCN(int device);
    ~PCN() { thread_.join(); }

    // Selectors
    string getThreadID(){return threadID;};

    int getCountIn(){return cnt_in;};
    int getCountOut(){return cnt_out;};

    // Setters
    void setCalibration(bool value) {calibrationOn = value; return;};
    void setDisplayColor(bool value) {displayColor = value; return;};
    void setDisplayBacksub(bool value) {displayBacksub = value; return;};
    void setDisplayDenoise(bool value) {displayDenoise = value; return;};
    void setSaveVideo(bool value) {saveVideo = value; return;};

    // Methods
    void start();
    void count();
    void stop(){halt = true;};

    void resetCounters(){cnt_in = 0; cnt_out = 0; return;};

    void toggleCalibration();
    void toggleDisplayColor();
    void toggleDisplayBacksub();
    void toggleDisplayDenoise();

private:
    VideoCapture cap;
    Ptr<BackgroundSubtractor> pMOG2; //MOG2 Background subtractor

    std::thread thread_;
    string threadID;

    bool halt = false;

    // Passenger counters
    int cnt_in  = 0;
    int cnt_out = 0;

    // Passengers tracker
    int pid = 0;
    vector<Passenger> passengers;

    // Options
    bool calibrationOn = false;
    bool displayColor = false;
    bool displayBacksub = false;
    bool displayDenoise = false;
    bool saveVideo = false;
};
\end{lstlisting}

\noindent Variabili:
\begin{itemize}
\item \verb|cap|: questa variabile appartiene alla classe \verb|VideoCapture|, data da OpenCV, e rappresenta la telecamera utilizzata come ingresso dal contatore. Come vedremo viene inizializzata dal costruttore della classe.
\item \verb|pMOG2|: questa struttura \`e definita da OpenCV e implementa l'oggetto che va ad applicare l'algoritmo di sottrazione del background MOG2 di cui abbiamo parlato in precedenza.
\item \verb|halt|: \`e la variabile booleana che ci permette di chiudere il loop principale di cattura dei frame. Questa variabile \`e manipolata solamente dal metodo \verb|stop()|.
\item \verb|cnt_in| e \verb|cnt_out| sono i contatori veri e propri, resi accessibili dall'esterno per mezzo dei selettori \verb|getCountIn()|, \verb|getCountOut()| e \verb|resetCounters()|.
\item \verb|pid|: \`e l'intero che viene aumentato al rilevamento di ogni nuovo passeggero. Ci permette di assegnare un identificativo univoco a tutti gli oggetti rilevati dal contatore.
\item \verb|passengers|: \`e il vettore che implementa la memoria del programma. Qui vengono salvati i passeggeri tracciati dal contatore come spiegato nel paragrafo \ref{ClassePassenger} a pagina \pageref{ClassePassenger}.

%% Interruzione di pagina
\newpage

\item Opzioni: questi booleani permettono di modificare il comportamento del Passenger Counter in real-time.
    \begin{itemize}
    \item \verb|displayColor|, \verb|displayBacksub|, \verb|displayDenoise|: permettono di aprire e chiudere le finestre al cui interno vengono visualizzati gli stream della telecamere nei vari step di elaborazione. Nel seguito vedremo nel dettaglio quali sono questi stream.
    \item \verb|saveVideo|: viene usata dal metodo \verb|count()| per dichiarare le strutture necessarie al salvataggio degli stream video.
    \item \verb|setCalibration|: abilita la calibrazione real-time di alcuni parametri del kernel del programma. Questa calibrazione \`e resa possibile dal modulo HighGui di OpenCV il quale permette di aggiungere delle trackbar alle finestre video.
    \end{itemize}
\end{itemize}

\noindent Metodi:
\begin{itemize}
\item Costruttore: qui di seguito \`e riportato il codice del costruttore della classe PCN.

\begin{lstlisting}[language=C++, caption=Costruttore della classe PCN, style=customcpp]
PCN::PCN(int device)
{
    cap.open(device);

    int history = 1000;
    double varThreshold = BACKGROUN_SUB_THRESHOLD;
    bool detectShadows = true;

#ifdef ReliaGate
    pMOG2 = createBackgroundSubtractorMOG2(history, varThreshold, detectShadows);
#else
    pMOG2 = new BackgroundSubtractorMOG2(history, varThreshold, detectShadows);
#endif
}
\end{lstlisting}

Come \`e possibile notare dal codice, al costrutture viene passato un intero che identifica la telecamera che verr\`a usata come input dal contatore. Ci\`o permette di istanziare pi\`u contatori contemporaneamente.

Nel seguito sono riportati i parametri di impostazione della funzione che implementa l'algoritmo di sottrazione del background:
    \begin{itemize}
    \item \verb|history|: lunghezza della storia sulla quale costruire il modello del background.
    \item \verb|varThreshold|: soglia sulla distanza quadratica di Mahalanobis per decidere se \`e ben descritta dal modello dello sfondo. Si \`e lasciato il valore associato a 4 sigma.
    \item \verb|detectShadows|: questo booleano imposta il sottrattore in modo tale che cerchi di rilevare le ombre. Le ombre saranno contrassegnate dal colore grigio.
    \end{itemize}

Si noti che \`e stato necessario utilizzare delle direttive al compilatore per la chiamata alla funzione di creazione del sottrattore del background poich\`e la versione installata sul ReliaGate di OpenCV era diversa da quella presente sulla macchina usata per lo sviluppo dell'applicazione ed utilizza una sintassi diversa.

\item \verb|start()|: questo metodo crea il thread dove verr\`a eseguito il loop principale del programma. Dopo aver creato il thread lancia il metodo \verb|count()|.
\item \verb|count()|: \`e il kernel del programma. Al suo interno avvengono le operazioni di elaborazioni delle immagini e il conteggio degli attraversamenti. Questo metodo verr\`a analizzato separatamente nel prossimo paragrafo.
\end{itemize}

\subsubsection{Utilizzo della classe PCN}
La sequenza necessaria al funzionamento \`e la seguente:
\begin{itemize}
\item Costruttore: per ogni telecamera va creato un oggetto della classe PCN assegnandogli, per mezzo dell'argomento passato al costruttore, una telecamera diversa dagli altri contatori.
\item \verb|start()|: ogni oggetto della classe PCN va lanciato utilizzando il metodo \verb|start()|, questo far\`a in modo che tutti i contatori vengano eseguiti in parallelo su thread diversi.
\item \verb|stop()|: per chiudere il programma bisogna effettuare la chiamata al metodo \verb|stop()| per ogni istanza di contatore attiva.
\end{itemize}

%% Interruzione di pagina
\newpage

\section{Analisi del kernel del programma: funzione count()}\label{kernelPCN}
La funzione \verb|count()| \`e il cuore del Passenger Counter. Ne suddivideremo il codice in tre sezioni principali:
\begin{enumerate}
\item Estrazione degli oggetti in movimento.
\item Rilevamento e tracking degli oggetti di nostro interesse.
\item Conteggio degli attraversamenti.
\end{enumerate}

\subsection{Estrazione degli oggetti in movimento}

\begin{lstlisting}[language=C++, caption=PNC.cpp metodo count() parte 1, style=customcpp]
void PCN::count()
{
    // Streams
    Mat color;
    Mat fgMaskMOG2;
    Mat morphTrans;
    Mat denoisedMat;

    // Contours detection variables
    vector<vector<Point> > contours;
    vector<Vec4i> hierarchy;

    // Calibration
    int learningRate = LEARNINGRATE;
    int whiteThreshold = THRESHOLD;
    int dilateAmount = DILATE_AMOUNT;
    int erodeAmount = ERODE_AMOUNT;
    int blur_ksize = BLUR_KSIZE;
    int areaMin = AREA_MIN;
    int max1PassArea = MAX_1PASS_AREA;
    int max2PassArea = MAX_2PASS_AREA;
    int xNear = X_NEAR;
    int yNear = Y_NEAR;
    int maxPassengerAge = MAX_PASSENGER_AGE;

    while(!halt)
    {
        bool bSuccess = cap.read(color);

        pMOG2->apply(color, fgMaskMOG2, (float)(learningRate/10000.0));

        // --DENOISE
        // Threshold the image
        threshold(fgMaskMOG2, morphTrans, whiteThreshold, MAXRGBVAL, THRESH_BINARY);

        // Eroding
        erode(morphTrans,morphTrans, Mat(Size(erodeAmount,erodeAmount), CV_8UC1));

        // Dilating
        dilate(morphTrans,morphTrans, Mat(Size(dilateAmount,dilateAmount), CV_8UC1));

        // Blurring the image
        blur(morphTrans,morphTrans, Size(blur_ksize,blur_ksize));
        denoisedMat = morphTrans.clone();

        ...
    }
}
\end{lstlisting}

\noindent Variabili:
\begin{itemize}
\item \verb|color|: questa \`e la matrice che conterr\`a i frame dello stream a colori in ingresso dalla telecamera. Rappresenta l'input che si andr\`a a manipolare per rilevare gli oggetti in movimento.
\item \verb|fgMaskMOG2|: questa \`e la matrice che conterr\`a i frame in uscita dall'algoritmo di sottrazione del background, ovvero la foreground mask.
\item \verb|denoisedMat|: questa \`e la matrice che conterr\`a i frame in uscita dall'algoritmo di sottrazione del background dopo che sono stati passati attraverso le funzioni di riduzione del rumore.
\item \verb|morphTrans|: \`e una copia di \verb|denoisedMat| che verr\`a usata per ricavare i dati di tracciamento. \`E stato necessario creare una copia per poter visualizzare i risultati di riduzione del rumore prima che il flusso video fosse processato ulteriormente.
\end{itemize}

\noindent Variabili di calibrazione del contatore:
\begin{itemize}
\item \verb|learningRate|: coefficiente con il quale viene aggiornato il modello del background all'interno dell'algoritmo di sottrazione dello sfondo.
\item \verb|whiteThreshold|: soglia utilizzata nella funzione per la sottrazione delle ombre.
\item \verb|dilateAmount|: valore che regola l'intensit\`a della funzione di dilatazione.
\item \verb|erodeAmount|: valore che regola l'intensit\`a della funzione di erosione.
\item \verb|blur_ksize|: valore che regola l'intensit\`a della funzione di blur.
\item \verb|areaMin|: area minima per la quale l'oggetto viene tracciato dal programma.
\item \verb|max1PassArea| e \verb|max2PassArea|: rappresentano i limiti di area per i quali un blob rilevato viene considerato formato da 1 o 2 passeggeri.
\item \verb|xNear|: coordinata orizzontale entro la quale due entit\`a vengono identificate come lo stesso passeggero.
\item \verb|yNear|: coordinata verticale entro la quale due entit\`a vengono identificate come lo stesso passeggero.
\item \verb|maxPassengerAge|: valore massimo di \verb|age| permesso. Quando un passeggero supera questo valore viene rimosso dalla memoria.
\end{itemize}

%% Interruzione di pagina
\newpage

\noindent Il codice esegue le seguenti operazioni:
\begin{enumerate}
\item Anzitutto ci appoggiamo a OpenCV per andare a leggere i frame ricevuti dalla telecamera. Essi vengono direttamente convertiti in tipo \verb|Mat| per poter essere manipolati facilmente da OpenCV. Tutto questo \`e implementato dal metodo \verb|read()| della classe \verb|VideoCapture|. La matrice \verb|color| va a raccogliere questi dati di ingresso.
\item I frame vengono passati alla struttura che applica la sottrazione del background \verb|pMOG2|. OpenCV prevede la creazione di un oggetto \verb|BackgroundSubtractor| il quale ha tra i suoi metodi l'applicazione della sottrazione del background. Questo metodo produce come output una immagine in scala di grigi nella quale i pixel bianchi appartengono al primo piano, i pixel neri appartengono al background, i pixel grigi alle ombre. Il risultato di questa operazione \`e salvato nella matrice \verb|fgMaskMOG2|.
\item Seguono delle trasformazioni sui frame atte a minimizzare il rumore nell'immagine e semplificare il lavoro della funzione di rilevamento dei contorni.
    \begin{enumerate}
    \item Threshold: soglia atta ad eliminare le ombre. Come abbiamo visto in precedenza l'algoritmo di sottrazione del background rileva le ombre e le colora di grigio. \`E possibile eliminarle imponendo una soglia che agisca sul colore dell'immagine. \verb|threshold()| fa in modo che ogni pixel il cui valore numerico \`e inferiore al valore dato come soglia (\verb|whiteThreshold|) viene portato al valore \verb|MAXRGBVAL| cio\`e bianco. Ogni pixel che si trova sotto la soglia viene posto a 0, cio\`e nero. Ovvero:
    
    \begin{equation}
    dstPixel(x,y) = \begin{cases} MAXRGBVAL, & \mbox{se } srcPixel(x,y) > threshold \\ 0, & \mbox{altrimenti } \end{cases}
    \end{equation}

    Cos\`\i\ facendo le ombre, che hanno un colore il cui valore in pixel \`e inferiore al valore di soglia, vengono portate a zero e non vengono rilevate nei passaggi seguenti, diminuiendo cos\`\i\ gli errori nel conteggio.
    \item Erosione: Questa operazione ha come risultato quello di espandere le regioni scure dell'immagine. L'obiettivo \`e quello di eliminare il rumore bianco gaussiano dall'immagine.
    \item Dilatazione: Questa operazione ha come risultato quello di far ``crescere'' le regioni luminose dell'immagine. L'obiettivo \`e quello di rendere pi\`u facilmente rilevabili gli oggetti del primo piano.
    \item Blur: Questa operazione effettua una sfocatura dell'immagine. L'obiettivo \`e sempre quello di ridurre il rumore dell'immagine. Questa funzione utilizza una semplice filtro lineare.
    \end{enumerate}
\end{enumerate}

%% Interruzione di pagina
\newpage

\noindent Risultato delle operazioni di riduzione del rumore.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{PCN1.png}
  \caption{Risultato delle operazioni di riduzione del rumore nel Passenger Counter a sottrazione del background.}
  \label{fig:PCN1}
\end{figure}

\noindent In figura \ref{fig:PCN1} sono riportati i vari step di elaborazione dell'immagine:
\begin{itemize}
\item L'immagine a sinistra \`e il contenuto della variabile \verb|color| prima di essere ulteriormente processata.
\item L'immagine al centro rappresenta il contenuto della variabile \verb|fgMaskMOG2|.
\item L'immagine a destra riporta il contenuto della matrice \verb|denoisedMat|.
\end{itemize}

\subsection{Rilevamento e tracking degli oggetti di nostro interesse}

\begin{lstlisting}[language=C++, caption=PNC.cpp metodo count() parte 2, style=customcpp, firstnumber=47]
while(!halt)
{
    ...

    // --FINDING CONTOURS
    findContours(morphTrans, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_NONE);

    // For every detected object
    for(unsigned int idx = 0; idx < contours.size(); idx++)
    {
        // -- AREA
        // Calculating area
        double areaCurrentObject = contourArea(contours[idx]);

        // If calculated area is big enough begin tracking the object
        if(areaCurrentObject > areaMin)
        {
            // Getting bounding rectangle
            Rect br = boundingRect(contours[idx]);
            Point2f objCenter = Point2f((int)(br.x + br.width/2) ,(int)(br.y + br.height/2) );

            // Drawing mass center and bounding rectangle
            rectangle( color, br.tl(), br.br(), GREEN, 2, 8, 0 );
            circle( color, objCenter, 5, RED, 2, 8, 0 );

            ...
        }

        ...
    }
}
\end{lstlisting}

\noindent Il codice esegue le seguenti operazioni:

\begin{enumerate}
\item Ricava i contorni per mezzo della funzione \verb|findContours| di cui abbiamo discusso l'algoritmo nella sezione \ref{AlgoritmoFindContours} a pagina \pageref{AlgoritmoFindContours}.
\item Per ogni contorno completo rilevato dalla funzione eseguiamo le seguenti operazioni:
    \begin{enumerate}
    \item Ne calcoliamo l'area per mezzo della funzione OpenCV \verb|contourArea|. Se supera una certa area minima (\verb|areaMin|), ricavata sperimentalmente, decidiamo che si tratta di una persona.
    \item Ne ricaviamo il rettangolo che inscrive i contorni rilevati tramite la funzione \verb|boundingRectangle|.
    \item Di questo rettangolo rileviamo il centro \verb|objCenter| grazie ad una semplice operazione geometrica.
    \item Per semplificare lo sviluppo e il debugging andiamo a disegnare questo rettangolo e questo centro sull'immagine a colori per mezzo delle funzioni \verb|rectangle| e \verb|circle| rese disponibili da OpenCV.
    \end{enumerate}
\end{enumerate}
L'informazione fondamentale per il resto del codice \`e data dalla posizione del centro di questo rettangolo, contenuto nella variabile \verb|objCenter|, che rappresenter\`a la posizione dei passeggeri all'interno del raggio visivo della telecamera.

Risultato delle operazioni:

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{PCN2.png}
  \caption{Risultato delle operazioni di image processing nel Passenger Counter a sottrazione del background.}
  \label{fig:PCN2}
\end{figure}

\noindent In figura \ref{fig:PCN2} sono riportati i vari step di elaborazione dell'immagine descritti nel paragrafo precedente:
\begin{itemize}
\item L'immagine a sinistra \`e il contenuto della variabile \verb|fgMaskMOG2|.
\item L'immagine al centro rappresenta il contenuto della variabile \verb|denoisedMat| dove sono stati disegnati i contorni rilevati dalla funzione \verb|findContours|.
\item L'immagine a destra riporta l'immagine di ingresso sulla quale sono stati disegnati il bounding rectangle e il centro del rettangolo.
\end{itemize}

%% Interruzione di pagina
\newpage

\subsection{Conteggio degli attraversamenti}
Definiamo all'inizio del codice un vettore di passenger che rappresenter\`a il nostro database di passeggeri. Quindi il tracciamento di questi avverr\`a tramite il seguente codice.

\begin{lstlisting}[language=C++, caption=PNC.cpp metodo count() parte 3, style=customcpp, firstnumber=62]
if(areaCurrentObject > areaMin)
{
    // Getting bounding rectangle
    Rect br = boundingRect(contours[idx]);
    Point2f mc = Point2f((int)(br.x + br.width/2) ,(int)(br.y + br.height/2) );

    // Drawing mass center and bounding rectangle
    rectangle( color, br.tl(), br.br(), GREEN, 2, 8, 0 );
    circle( color, mc, 5, RED, 2, 8, 0 );

    // --PASSENGERS DB UPDATE
    bool newPassenger = true;
    for(unsigned int i = 0; i < passengers.size(); i++)
    {
        // If the passenger is near a known passenger assume they are the same one
        if( abs(objCenter.x - passengers[i].getCurrentPoint().x) <= xNear &&
            abs(objCenter.y - passengers[i].getCurrentPoint().y) <= yNear )
        {
            // Update coordinates
            newPassenger = false;
            passengers[i].updateCoords(objCenter);
            passengers[i].resetAge();

            // --COUNTER
            if(passengers[i].getTracks().size() > 1)
            {
                // Up to down
                if( (passengers[i].getLastPoint().y < color.rows/2 && passengers[i].getCurrentPoint().y >= color.rows/2) ||
                    (passengers[i].getLastPoint().y <= color.rows/2 && passengers[i].getCurrentPoint().y > color.rows/2) )
                {
                    // Counting multiple passenger depending on area size
                    if (areaCurrentObject > max1PassArea && areaCurrentObject < max2PassArea)
                        cnt_out += 2;
                    else if (areaCurrentObject > max2PassArea)
                        cnt_out += 3;
                    else
                        cnt_out++;

                    // Visual feedback
                    circle(color, Point(color.cols - 20, 20), 8, RED, CV_FILLED);
                }

                // Down to up
                if( (passengers[i].getLastPoint().y > color.rows/2 && passengers[i].getCurrentPoint().y <= color.rows/2) ||
                    (passengers[i].getLastPoint().y >= color.rows/2 && passengers[i].getCurrentPoint().y < color.rows/2) )
                {
                    // Counting multiple passenger depending on area size
                    if (areaCurrentObject > max1PassArea && areaCurrentObject < max2PassArea)
                        cnt_in += 2;
                    else if (areaCurrentObject > max2PassArea)
                        cnt_in += 3;
                    else
                        cnt_in++;

                    // Visual feedback
                    circle(color, Point(color.cols - 20, 20), 8, GREEN, CV_FILLED);
                }

            }

            break;
        }
    }

    // If it wasn't near any known object it is a new passenger
    if(newPassenger)
    {
        Passenger p(pid, objCenter);
        passengers.push_back(p);
        pid++;
    }
}
\end{lstlisting}

\noindent Il codice riportato esegue le seguenti operazioni:
\begin{enumerate}
\item Se il centro dell'entit\`a rilevata si trova entro una certa distanza massima da un passeggero che si trova all'interno del database assumiamo che siano lo stesso passeggero. Questa distanza \`e data dalle variabili \verb|xNear| e \verb|yNear| e in figura \ref{fig:NearRect} \`e visualizzato il rettangolo formato da questi due parametri. Alla ricezione di un nuovo frame dalla telecamere, se il nuovo centro dell'oggetto si trova all'interno del rettangolo pi\`u scuro, si assume che sia lo stesso oggetto rilevato precedentemente che si \`e spostato.
\item Qualora si trattasse di un passeggero gi\`a presente in memoria ne aggiorniamo la posizione per mezzo del metodo \verb|updateCoord()| e ne resettiamo il parametro \verb|age|. Nel caso abbia uno storico maggiore di una singola coordinata, verifichiamo che non abbia superato la soglia virtuale rappresentata dalla met\`a del raggio visivo della telecamera. Il superamento della soglia \`e verificato come segue:
    \begin{itemize}
    \item Se la penultima coordinata registrata si trova al di sotto della soglia visiva e l'ultima coordinata sopra, aggiorniamo il contatore degli ingressi.
    \item Se la penultima coordinata registrata si trova al di sopra della soglia visiva e l'ultima coordinata sotto, aggiorniamo il contatore delle uscite.
    \end{itemize}
\item Nel caso l'entit\`a non si trovasse vicino a nessun passeggero noto, ovvero non ricade all'interno del rettangolo formato da \verb|xNear| e \verb|yNear| per nessuno dei passeggeri in memoria, si tratta di un nuovo passeggero appena entrato all'interno del raggio visivo della telecamera. Procediamo quindi ad aggiornare il database creando  nuovo oggetto appartenente alla classe \verb|passenger| e aggiungendolo in memoria per mezzo del metodo \verb|push_back()|.
\end{enumerate}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.65]{NearRect.png}
  \caption{Visualizzazione del rettangolo dato dai parametri xNear e yNear.}
  \label{fig:NearRect}
\end{figure}

Qualora nel raggio d'azione della telecamera siano presenti pi\`u passeggeri, bisogna tenere conto del funzionamento dell'algoritmo di sottrazione dello sfondo. Infatti, se due persone sono troppo vicine tra loro (ad esempio spalla a spalla), nella foreground mask figurer\`a un'unica sagoma bianca e avremo che i due passeggeri vengono inglobati in un unico blob. Quindi, per rilevare correttamente l'attraversamento di pi\`u persone contemporaneamente, si sfrutta l'area rilevata nei passaggi precedenti: se l'area dell'entit\`a rilevata supera una certa soglia, ricavata sperimentalmente, la si considera un blob formato da due passeggeri. Dunque, al rilevamento dell'attraversamento della soglia, si aggiorna il contatore aggiungendo due attraversamenti invece che uno. Allo stesso modo si procede per il rilevamento di tre passeggeri.

%% Interruzione di pagina
\newpage

Per visualizzare quanto avviene all'interno del programma e semplificare il debugging tramite OpenCV si \`e deciso di plottare a schermo tutte le informazioni utilizzate nel codice precedente.
Questa parte di visualizzazione delle informazioni \`e stata implementata per mezzo del codice seguente:

\begin{lstlisting}[language=C++, caption=PNC.cpp metodo count() parte 4, style=customcpp, firstnumber=134, label=countpt4]
// For every passenger in passengers DB
for(unsigned int i = 0; i < passengers.size(); i++)
{
    // -- DRAWING PASSENGER TRAJECTORIES
    if(passengers[i].getTracks().size() > 1)
    {
        polylines(frame, passengers[i].getTracks(), false, passengers[i].getTrackColor(),2);
        putText(frame, "Pid: " + to_string(passengers[i].getPid()), passengers[i].getCurrentPoint(), FONT_HERSHEY_SIMPLEX, 0.5, passengers[i].getTrackColor(), 2);
    }

    // --UPDATE PASSENGER STATS
    // Updating age
    passengers[i].updateAge();

    // If passenger is outside field of view update coords with last known position.
    if(passengers[i].getAge() > 1)
        passengers[i].updateCoords(passengers[i].getCurrentPoint());

    // Removing older passengers
    if(passengers[i].getAge() > maxPassengerAge)
        passengers.erase(passengers.begin() +i);
}

// --PRINTING INFORMATION
putText(frame, "FPS: " + to_string(fps), Point(0,  15) , FONT_HERSHEY_SIMPLEX, 0.5, RED, 2);

line(color, Point(0,color.rows/2), Point(color.cols,color.rows/2), RED, 2, 8);

putText(frame, "Count IN: "  + to_string(cnt_in), Point(0,frame.rows - 30) , FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 2);
putText(frame, "Count OUT: " + to_string(cnt_out), Point(0, frame.rows - 10) , FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 2);
\end{lstlisting}

\noindent Il codice riportato nel listato \ref{countpt4} effettua le seguenti operazioni:
\begin{itemize}
\item Per ogni passeggero in memoria:
    \begin{enumerate}
    \item Disegna la traccia per mezzo della funzione \verb|polylines|.
    \item Visualizza l'identificatore univoco \verb|pid| tramite la funzione \verb|putText|.
    \item Aggiorna il parametro \verb|age|.
    \item Quando il parametro \verb|age| \`e maggiore di 1 significa che non si trova pi\`u all'interno del raggio visivo della telecamera, in tal caso si aggiorna la nuova posizione del passeggero con l'ultima posizione nota. Questo evita il conteggio di falsi attraversamenti.
    \item Nel caso il parametro \verb|age| abbia superato il valore massimo, il passeggero viene rimosso dalla memoria del contatore.
    \end{enumerate}
\item Infine vengono aggiunti i feedback visivi necessari a comprendere meglio il funzionamento del programma: contatori a schermo, la linea che rappresenta la soglia virtuale, e il contatore dei frame al secondo.
\end{itemize}

\noindent Qui di seguito sono riportati i risultati del tracciamento in fase sperimentale. La linea rossa rappresenta la soglia per l'attraversamento.

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=9cm]{PCNMano1.png}
  \caption{Risultato tracciamento sperimentale PCN. Frame precedente all'attraversamento.}
  \label{fig:}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=9cm]{PCNMano2.png}
  \caption{Risultato tracciamento sperimentale PCN. Primo attraversamento.}
  \label{fig:}
\end{minipage}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=9cm]{PCNMano3.png}
  \caption{Risultato tracciamento sperimentale PCN. Secondo attraversamento.}
  \label{fig:PCNMano3}
\end{figure}

Dalla successione delle immagini si pu\`o notare come il tracciamento sia buono in condizioni ideali. Si noti che il punto al centro dello schermo \`e dovuto alla scomparsa di un oggetto rilevato dal raggio visivo della telecamera ma che non ha ancora superato il limite massimo di \verb|age|.

%% Interruzione di pagina
\newpage

\section{main.cpp}
Qui di seguito \`e riportato il main dell'applicazione.

\begin{lstlisting}[language=C++, caption=main.cpp, style=customcpp]
int main(int argc, char * argv[])
{
    bool stop = false;
    bool displayHelpFlag = false;
    char choice;

    vector<PCN *> counters;

    for(int i = 0; i < deviceNumber; i++) {
        counters.push_back(new PCN(i));
        cout << endl; 
        cout << "Device number: " << i << endl;
    }

    if(argc >= 2) {
        if(!strcmp(argv[1], "-s")) {
            for(int i = 0; i < deviceNumber; i++) {
                counters[i]->setSaveVideo(true);
            }
        } else { 
            cout << "Unknown option. Defaulted to normal mode." << endl;
        }
    }

    for(int i = 0; i < deviceNumber; i++) {
        counters[i]->start();
    }

    do {
        cout << "Please enter a valid command:\n>";
        choice = getchar();
        cout << "You entered: " << choice << endl;

        for(int i = 0; i < deviceNumber; i++) {
            cout << "Device : " << i << endl;
            switch(choice)
            {
                case('p'):
                    cout << "Current count:\n";
                    cout << "Count in  = " << counters[i]->getCountIn() << endl;
                    cout << "Count out = " << counters[i]->getCountOut() << endl;
                    cout << "Current balance =" << counters[i]->getCountIn() - counters[i]->getCountOut() << endl;
                    break;

                case('r'):
                    cout << "Resetting counters\n";
                    counters[i]->resetCounters();
                    break;

                case('c'):
                    cout << "Toggle color\n";
                    counters[i]->toggleDisplayColor();
                    break;
                
                ...
                
                case('q'):
                    cout << "Exiting program!\n";
                    counters[i]->stop();
                    cout << "Capture closed.\n";
                    stop = true;
                    break;

                default:
                    displayHelpFlag = true;
                    break;
            }
        }

        if(displayHelpFlag) {
            displayHelp();
            displayHelpFlag = false;
        }

        // Consume input
        while ((choice = getchar()) != '\n' && choice != EOF);

    } while(!stop);

    return 0;
}
\end{lstlisting}

\noindent Il codice effettua le seguenti operazioni:
\begin{enumerate}
\item Crea un vettore di contatori pari al numero di telecamere ad esso collegate. Per ogni telecamera, quindi, viene istanziato un oggetto della classe PCN.
\item Vengono fatti partire in parallelo i contatori per mezzo del metodo \verb|start()|.
\item Viene fatto partire un loop all'interno del quale si aspettano i comandi da eseguire su tutti i contatori immessi dall'utente.
\end{enumerate}

Nel capitolo \ref{CapitoloManuale} a pagina \pageref{CapitoloManuale} sono riportati in dettaglio i comandi real-time implementati da questo codice.

%% Interruzione di pagina
\newpage

\section{Risultati ottenuti}
In questo paragrafo vengono riportati i risultati dei test effettuati con questa versione del contatore di passeggeri.

\subsection{Dimostrazione funzionamento}
Nel seguito sono riportati alcuni fotogrammi tratti da una registrazione di un test del contatore. Per il test si \`e utilizzata la telecamera descritta nel paragrafo \ref{HardwarePCN}, la quale \`e stata montata su uno stipite di una porta ad una altezza di circa 2,10m. Ogni frame riporta a sinistra l'immagine tratta dal video in ingresso dove sono state disegnate le forme per rendere pi\`u comprensibile il tracciamento, al centro l'immagine ottenuta in seguito alla sottrazione del background (cio\`e la foreground mask), a destra l'immagine cui sono state applicate le operazioni di sottrazione del rumore (cio\`e \verb|denoisedMat|).

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{PCNTest1.png}
  \caption{Risultato test PCN a sottrazione del background: frame 1.}
  \label{fig:PCNTest1}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{PCNTest2.png}
  \caption{Risultato test PCN a sottrazione del background: frame 2.}
  \label{fig:PCNTest2}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{PCNTest3.png}
  \caption{Risultato test PCN a sottrazione del background: frame 3.}
  \label{fig:PCNTest3}
\end{figure}

%% Interruzione di pagina
\newpage

In figura \ref{fig:PCNTest3} \`e importante notare l'effetto di riduzione del rumore ed eliminazione dell'ombra ottenuti. Nonostante ci\`o l'ombra proiettata lateralmente sul muro, evidenziata tramite il cerchio rosso, viene rilevata erroneamente come oggetto in movimento. Essa viene quindi rilevata dal programma e tracciata come se si trattasse di un passeggero.

Inoltre, al centro del fotogramma, \`e possibile notare un ulteriore falso tracciamento dovuto ad un picco di rumore registrato dalla telecamera.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{PCNTest4.png}
  \caption{Risultato test PCN a sottrazione del background: frame 4.}
  \label{fig:PCNTest4}
\end{figure}

In figura \ref{fig:PCNTest4} \`e nuovamente possibile notare l'effetto di riduzione del rumore molto elevato in questo frame. Si noti come in questo caso l'ombra proiettata sul muro laterale sia correttamente rilevata come ombra e quindi eliminata dalle operazioni di riduzione del rumore. Nel frame a colori \`e possibile notare come l'ombra sul muro abbia lasciato una scia di tracciamento, ci\`o \`e dovuto al fatto che, pur essendo sparita dal raggio visivo, il suo parametro \verb|age| non ha ancora raggiunto il valore massimo, e quindi non \`e ancora stata eliminata dalla memoria.

%% Interruzione di pagina
\newpage

\subsection{Performance}
Per la misura dei tempi di esecuzione si \`e fatto uso della libreria standard \verb|chrono| di C++. I test sono stati effettuati usando due telecamere di pari risoluzione ma con diverso framerate su due diversi dispositivi: il ReliaGate 20-25 ed una macchina Linux ad alte prestazioni. I tempi sono stati mediati sul numero di cicli effettuati, cio\`e pari al numero di frame catturati dalla telecamera.

Nel seguito sono riportati i risultati relativi alle performance dell'applicazione utilizzando la webcam descritta nel paragrafo \ref{HardwarePCN}: 

\begin{figure}[h!]
    \centering
    \fbox{
    \begin{tikzpicture}
    \begin{axis}[
    xbar,
    y axis line style = { opacity = 0 },
    align = center,
    enlarge y limits  = 0.2,
    enlarge x limits  = 0.02,
    symbolic y coords = {Sottrazione background, Sottrazione rumore, Rilevamento contorni, Tracking oggetti},
    nodes near coords,
    grid = major,
    grid style={dashed,gray!30},
    xlabel = {Tempo di esecuzione [ms]},
    ]
    % ReliaGate 20-25
    \addplot 
        coordinates {(29.4414,Sottrazione background) (2.80683,Sottrazione rumore) (1.09507,Rilevamento contorni) 
                        (0.178409,Tracking oggetti)};
    % Sistema Linux
    \addplot 
        coordinates {(3.16497,Sottrazione background) (0.546563,Sottrazione rumore) (0.211879,Rilevamento contorni) 
                        (0.026478,Tracking oggetti)};
    \legend{ReliaGate 20-25, Sistema Linux}
    \end{axis}
    \end{tikzpicture}
    }
    \caption{Grafico tempi di esecuzione PCN con telecamera 30 fps.}
\end{figure}

\begin{table}[h!]
    \centering
	\begin{tabular}{|l|c|c|}
	\hline
    & ReliaGate 20-25 & Sistema Linux \\ \hline
    Tempo di esecuzione del loop principale & 41.254 ms & 31.1649 ms \\ \hline
    Algoritmo sottrazione del background & 29.4414 ms & 3.16497 ms \\ \hline
    Sottrazione del rumore & 2.80683 ms & 0.546563 ms \\ \hline
    Algoritmo rilevamento contorni & 1.09507 ms & 0.211879 ms \\ \hline
    Rilevamento e tracking degli oggetti & 0.178409 ms & 0.026478 ms \\ \hline
    Frame Rate & 20.1 fps & 30 fps \\ \hline
	\end{tabular}
    \caption{Risultati Passenger Counter a sottrazione del background con telecamera 30 fps.}
\end{table}

Si noti che il tempo di esecuzione del loop principale considera anche il tempo speso ad attendere i frame dalla telecamera.

%% Interruzione di pagina
\newpage

Nel seguito sono riportati i risultati relativi alle performance dell'applicazione utilizzando una webcam capace di fornire uno stream a 60 FPS ma con la stessa risoluzione della precedente:

\begin{figure}[h!]
    \centering
    \fbox{
    \begin{tikzpicture}
    \begin{axis}[
    xbar,
    y axis line style = { opacity = 0 },
    align = center,
    enlarge y limits  = 0.2,
    enlarge x limits  = 0.02,
    symbolic y coords = {Sottrazione background, Sottrazione rumore, Rilevamento contorni, Tracking oggetti},
    nodes near coords,
    grid = major,
    grid style={dashed,gray!30},
    xlabel = {Tempo di esecuzione [ms]},
    ]
    % ReliaGate 20-25
    \addplot[green!20!black,fill=green!80!white] 
    coordinates {(33.3335,Sottrazione background) (2.7739,Sottrazione rumore) (1.75694,Rilevamento contorni) 
                    (0.279599,Tracking oggetti)};
    % Sistema Linux
    \addplot[yellow!20!black,fill=yellow!80!white] 
    coordinates {(3.10198,Sottrazione background) (0.538276,Sottrazione rumore) (0.233947,Rilevamento contorni) 
                    (0.028581,Tracking oggetti)};
    \legend{ReliaGate 20-25, Sistema Linux}
    \end{axis}
    \end{tikzpicture}
    }
    \caption{Grafico tempi di esecuzione PCN con telecamera 60 fps.}
\end{figure}

\begin{table}[h!]
    \centering
	\begin{tabular}{|l|c|c|}
	\hline
    & ReliaGate 20-25 & Sistema Linux \\ \hline
    Tempo di esecuzione del loop principale & 46.7684 ms & 15.7044 ms \\ \hline
    Algoritmo sottrazione del background & 33.3335 ms & 3.10198 ms \\ \hline
    Sottrazione del rumore & 2.7739 ms & 0.538276 ms \\ \hline
    Algoritmo rilevamento contorni & 1.75694 ms & 0.233947 ms \\ \hline
    Rilevamento e tracking degli oggetti & 0.279599 ms & 0.028581 ms \\ \hline
    Frame Rate & 20.5 fps & 60 fps \\ \hline
	\end{tabular}
    \caption{Risultati Passenger Counter a sottrazione del background con telecamera 60 FPS.}
\end{table}

Come evidenziato dai dati riportati nelle tabelle la funzione pi\`u onerosa del punto di vista computazionale \`e quella che applica l'algoritmo di sottrazione del background. Inoltre si pu\`o notare come nel caso del 20-25, la computazione occupa talmente tanto tempo che la stabilit\`a del framerate risulta compromessa. Nominalmente il flusso video dovrebbe essere stabile sui 30 fps ma per il 20-25 si ferma a circa 20 frame al secondo. L'utilizzo di una telecamera con risoluzione pi\`u elevata comprometterebbe ulteriormente le performance a causa del maggior numero di pixel sul quale effettuare la computazione.

Infine \`e opportuno notare che la durata del loop principale nel caso della telecamera a 30 fps, \`e simile sui due sistemi poich\`e quello pi\`u performante impiega la maggior parte del tempo ad aspettare i frame. La funzione \verb|cap.read()| \`e infatti bloccante. Questo non si verifica invece nel caso della telecamera a 60 fps poich\`e fornisce i frame pi\`u velocemente.

%% Interruzione di pagina
\newpage

\section{Problemi}
I problemi principali rilevati in questa implementazione del Passenger Counter sono i seguenti:
\begin{enumerate}
\item \textbf{Imprecisione}: Le immagini risultati dalla sottrazione del background tendono ad essere molto rumorose, nonostante il post-processing applicato. Il conteggio ne risulta influenzato negativamente.
\item \textbf{Tracciamenti indesiderati}: Come visto nel paragrafo precedente, nonostante l'algoritmo riesca a rilevare la maggior parte delle ombre come tali, spesso le ombre vengono tracciate erroneamente. Ci\`o comporta errori nel conteggio.
\item \textbf{Performance}: L'algoritmo di sottrazione del background, come riportato nel paragrafo precedente, \`e piuttosto oneroso dal punto di vista computazionale. Ci\`o ne limita le possibili applicazioni: non \`e possibile utilizzare telecamere con risoluzione elevata e bisogna limitare il numero di telecamere utilizzate contemporaneamente.
\item \textbf{Tecnologia}: Il problema pi\`u grave \`e senz'altro determinato dalla tecnologia utilizzata. La tecnica della background subtraction si basa sulla costruzione di un modello di background costruito a partire dalle immagini ricavate dalla telecamera. Un ambiente come quello di un mezzo pubblico \`e troppo dinamico perch\'e l'algoritmo possa costruire un modello di background affidabile: le condizioni di luce cambiano con troppa frequenza, senza contare le vibrazioni del mezzo.
\end{enumerate}

Per questo motivo si \`e passati ad una seconda implementazione sfruttando una tecnologia esente da queste problematiche: il Passenger Counter con telecamere RealSense.

\chapter{Passenger Counter con telecamere RealSense}\label{CapitoloRSPCN}
In questo capitolo tratteremo la versione del contatore di passeggeri che utilizza le telecamere RealSense. Inizialmente descriveremo la struttura del codice, gli algoritmi utilizzati ed il suo funzionamento. Infine parleremo dei risultati ottenuti da questa implementazione e le problematiche principali.

\section{Struttura del codice}\label{StrutturaRSPCN}
In figura \ref{fig:RSPCNUML} \`e riportato il diagramma UML del Passenger Counter con telecamere RealSense. La struttura rimane molto simile a quella utilizzata per il contatore a sottrazione del background:

\begin{enumerate}
\item \textbf{Passenger}: \`e la stessa classe usata nella versione precedente del programma.
\item \textbf{RSPCN}: \`e la classe che implementa il contatore di passeggeri. Questa fa uso della libreria librealsense di cui si \`e discusso nel paragrafo \ref{librealsense} ed assolve gli stessi compiti della classe PCN visti nella versione precedente del Passenger Counter.
\item \textbf{main}: implementa l'interfaccia utente per gestire i settaggi run-time delle telecamere e del contatore. Si noti inoltre che rileva automaticamente il numero di telecamere collegate e istanzia un contatore per ognuna di esse.
\end{enumerate}

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{RSPCNUML.png}
  \caption{Diagramma UML del Passenger Counter con telecamere RealSense.}
  \label{fig:RSPCNUML}
\end{figure}

%% Interruzione di pagina
\newpage

\section{Classe RSPCN}
Questa classe ha gli stessi obiettivi della classe PCN nel caso del Passenger Counter a sottrazione del background, con qualche aggiunta. Essa deve infatti:

\begin{itemize}
\item Interfacciarsi con le telecamere RealSense per mezzo della libreria librealsense in modo da poterne controllare i settaggi in tempo reale e ricavare i dati di profondit\`a.
\item Convertire i dati ottenuti dalle telecamere RealSense in qualcosa di utilizzabile dall'algoritmo.
\end{itemize}

Qui di seguito \`e riportato l'header della classe RSPCN:

\begin{lstlisting}[language=C++, caption=RSPCN.h, style=customcpp]
class RSPCN {

public:
  // Constructor
  RSPCN(device *assignedDevice);
  ~RSPCN() { thread_.join(); }

  // Selectors
  string getThreadID(){return threadID;};
  float getDeviceScale(){return scale;};
  string getDeviceName(){return dev->get_name();};
  string getDeviceSerial(){return dev->get_serial();};
  string getDeviceFirmware(){return dev->get_firmware_version();};

  int getCountIn(){return cnt_in;};
  int getCountOut(){return cnt_out;};

  // Setters
  void setCalibration(bool value) {calibrationOn = value; return;};
  void setDisplayColor(bool value) {displayColor = value; return;};
  void setDisplayDepth(bool value) {displayDepth = value; return;};
  void setDisplayRawDepth(bool value) {displayRawDepth = value; return;};
  void setDisplayFrame(bool value) {displayFrame = value; return;};
  void setFramerateStabilization(bool value) {framerateStabilizationOn = value; return;};
  void setCameraPresets(int value);
  void setSaveVideo(bool value) {saveVideo = value; return;};

  // Methods
  void start();
  void count();
  void resetCounters(){cnt_in = 0; cnt_out = 0; return;};
  void stop(){halt = true;};

  Mat getColorMap(Mat depthImage);
  Mat getFrame(Mat depthImage, int thresholdCentimeters);
  
  void toggleCalibration();
  void toggleDisplayColor();
  void toggleDisplayDepth();
  void toggleDisplayRawDepth();
  void toggleDisplayFrame();
  void toggleFrameRateStabilization(){framerateStabilizationOn = !framerateStabilizationOn; return;};

private:
  device * dev;

  std::thread thread_;
  string threadID;

  bool halt = false;

  // Camera type
  bool cameraDevice;

  // Camera options
  int ImageWidth;
  int ImageHeight;
  int CameraFramerate;

  // Camera scale
  float scale;

  // Passenger counters
  int cnt_in  = 0;
  int cnt_out = 0;

  // Passengers tracker
  int pid = 0;
  vector<Passenger> passengers;

  // Options
  bool calibrationOn = false;
  bool displayColor = false;
  bool displayRawDepth = false;
  bool displayDepth = false;
  bool displayFrame = false;
  bool framerateStabilizationOn = true;
  bool saveVideo = false;
};
\end{lstlisting}

%% Interruzione di pagina
\newpage

\noindent Variabili:
\begin{itemize}
\item \verb|dev|: \`e la telecamera dalla quale il contatore raccoglie i dati. Questo tipo \`e definito nella libreria delle telecamere RealSense ed \`e usato per andare a raccogliere i frame in uscita dalla telecamera, nonch\`e altri dati utili per il funzionamento del programma.
\item \verb|cameraDevice|: si \`e resa necessaria per poter distinguere le due telecamere usate durante lo sviluppo del progetto poich\`e necessitano di settaggi diversi per il loro funzionamento.
\item \verb|ImageWidth|, \verb|ImageHeight|, \verb|CameraFramerate|: sono le impostazioni di risoluzione e framerate dei flussi video in ingresso dalle telecamere.
\item \verb|scale|: \`e la scala alla quale viene impostata la telecamera. 
\item \verb|cnt_in|, \verb|cnt_out|: sono i contatori veri e propri, resi accessibili dall'esterno per mezzo dei selettori \verb|getCountIn()|, \verb|getCountOut()| e \verb|resetCounters()|.
\item \verb|pid| e \verb|passengers| assolvono gli stessi compiti visti nella versione del Passenger Counter a sottrazione del background.
\item Opzioni: questi booleani permettono di modificare le impostazioni del contatore in real-time.

    \begin{itemize}
    \item \verb|displayColor|, \verb|displayRawDepth|, \verb|displayDepth|, \verb|displayFrame|: permettono di aprire e chiudere le finestre al cui interno vengono visualizzati gli stream della telecamere nei vari step di elaborazione. Nel seguito vedremo nel dettaglio quali sono questi stream.
    \item \verb|saveVideo|: viene usata dal metodo \verb|count()| per dichiarare le strutture necessarie al salvataggio degli stream video.
    \item \verb|setCalibration|: abilita la calibrazione real-time di alcuni parametri del kernel del programma. Questa calibrazione \`e resa possibile dal modulo HighGui di OpenCV il quale permette di aggiungere delle trackbar alle finestre video.
    \item \verb|framerateStabilizationOn|: va ad agire sulla funzione per mezzo della quale andiamo a ricavare i frame dalla telecamera. Se lasciato \verb|true| viene usato un metodo bloccante, che attende l'arrivo dei frame. Altrimenti viene usato un metodo non bloccante, ci\`o comporta un framerate molto pi\`u elevato ma altrettanto meno stabile.
    \end{itemize}
\end{itemize}

\noindent Metodi:
\begin{itemize}
\item Costruttore: qui di seguito \`e riportato il codice del costruttore della classe RSPCN.

\begin{lstlisting}[language=C++, caption=Costruttore della classe RSPCN, style=customcpp]
RSPCN::RSPCN(device *assignedDevice) {
    dev = assignedDevice;

    string devName = dev->get_name();

    // Camera settings
    if(devName.compare("Intel RealSense R200") == 0) {
        cameraDevice = R200;

        ImageWidth = IMAGE_WIDTH_R200;
        ImageHeight = IMAGE_HEIGHT_R200;
        CameraFramerate = FRAMERATE_R200;
    }
    else if(devName.compare("Intel RealSense SR300") == 0) {
        cameraDevice = SR300;

        ImageWidth = IMAGE_WIDTH_SR300;
        ImageHeight = IMAGE_HEIGHT_SR300;
        CameraFramerate = FRAMERATE_SR300;
    }

    // Configure stream
    dev->enable_stream(rs::stream::color, ImageWidth, ImageHeight, rs::format::bgr8, CameraFramerate);
    dev->enable_stream(rs::stream::depth, ImageWidth, ImageHeight, rs::format::z16, CameraFramerate);

    // Get device depth scale
    scale = dev->get_depth_scale();
}
\end{lstlisting}

Come \`e possibile notare al costruttore viene passato un oggetto della classe \verb|device| che identifica la telecamera che verr\`a usata come input dal contatore. Ci\`o permette di istanziare pi\`u contatori contemporaneamente.

Una volta inizializzata la variabile \verb|dev| si ricava il tipo di telecamera e, in base al nome, vengono configurati gli stream in ingresso. Ci\`o si \`e reso necessario perch\'e le telecamera operano a risoluzioni e framerate diversi. Si \`e deciso quindi di impostare i flussi come segue:

\begin{itemize}
\item Telecamera R200: 320x240 pixel @ 60 fps.
\item Telecamera SR300: 640x480 pixel @ 30 fps.
\end{itemize}

Infine si ricava la scala alla quale opera la telecamera e viene salvata all'interno della variabile \verb|scale|.

\item \verb|start()|: questo metodo crea il thread dove verr\`a eseguito il loop principale del programma. Dopo aver creato il thread lancia il metodo \verb|count()|.
\item \verb|count()|: anche in questo caso rappresenta il cuore del Passenger Counter e verr\`a analizzata separatamente. 
\item \verb|getColorMap()| e \verb|getFrame()| sono le funzioni che convertono i frame forniti dalle telecamere RealSense in qualcosa di utilizzabile dal nostro algoritmo. Nel seguito vedremo in dettaglio cosa fanno.
\end{itemize}

\subsubsection{Utilizzo della classe RSPCN}
La sequenza necessaria al funzionamento \`e la seguente:
\begin{itemize}
\item Dichiarazione della variabile della classe \verb|context|: questa classe definita nella libreria librealsense conserva tutti gli handle a tutte le telecamere connesse. \`E necessario per ricavare il numero di telecamere connesse e assegnare le device ai vari contatori. Si noti che all'interno del sistema di Passenger Counter deve esistere una sola istanza di tipo \verb|context|.
\item Costruttore: per ogni telecamera va creato un oggetto della classe RSPCN assegnandogli, per mezzo dell'oggetto \verb|device| passato al costruttore, una telecamera diversa dagli altri contatori.
\item \verb|start()|: ogni oggetto della classe RSPCN va lanciato utilizzando il metodo \verb|start()|, questo far\`a in modo che tutti i contatori vengano eseguiti in parallelo su thread diversi.
\item \verb|stop()|: per chiudere il programma bisogna effettuare la chiamata al metodo \verb|stop()| per ogni istanza di contatore attiva.
\end{itemize}

%% Interruzione di pagina
\newpage

\section{Metodo getColorMap()}
Questo metodo \`e stato utile in fase di sviluppo poich\`e fornisce una rappresentazione dei dati in uscita dalla telecamera molto intuitiva. Non ha un ruolo funzionale nel programma ma facilita la comprensione di quanto accade al suo interno, quindi \`e opportuno spiegarne il funzionamento.

\begin{lstlisting}[language=C++, caption=RSPCN.cpp metodo getColorMap(), style=customcpp]
Mat RSPCN::getColorMap(Mat depthImage) {
    Mat depthColorMap;

    double min;
    double max;
    Point tmpMinLoc;
    Point tmpMaxLoc;

    int nearestVal;
    Point nearestLoc;

    int farthestVal;
    Point farthestLoc;

    // Saving farthest point
    minMaxLoc(depthImage, &min, &max, &tmpMinLoc, &tmpMaxLoc);
    farthestVal = max;
    farthestLoc = tmpMaxLoc;

    // If pixelValue == 0 set it to 65535( = 2^16 - 1)
    depthImage.setTo(65535, depthImage == NODATA);

    // Saving nearest point
    minMaxLoc(depthImage, &min, &max, &tmpMinLoc, &tmpMaxLoc);
    nearestVal = min;
    nearestLoc = tmpMinLoc;

    // Converts CV_16U to CV_8U using a scale factor of 255.0/ 65535
    depthImage.convertTo(depthImage, CV_8UC1, 255.0 / 65535);

    // Current situation: Nearest object => Black, Farthest object => White
    // We want to have  : Nearest object => White, Farthest object => Black
    depthImage = cv::Scalar::all(255) - depthImage;

    // Color map: Nearest object => Red, Farthest object => Blue
    equalizeHist( depthImage, depthImage );
    applyColorMap(depthImage, depthColorMap, COLORMAP_JET);

    // Highlight nearest and farthest pixel
    circle( depthColorMap, nearestLoc, 5, WHITE, 2, 8, 0 );
    putText(depthColorMap, "Nearest: " + to_string(nearestVal*scale) + " m", Point(0, depthColorMap.rows - 30), FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 2);

    circle( depthColorMap, farthestLoc, 5, WHITE, 2, 8, 0 );
    putText(depthColorMap, "Farthest: " + to_string(farthestVal*scale) + " m", Point(0, depthColorMap.rows - 10), FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 2);

    return depthColorMap;
}
\end{lstlisting}

Come si \`e detto nel paragrafo \ref{librealsense} l'output delle telecamere RealSense \`e dato da delle immagini in scala di grigi i cui pixel hanno formato 16 bit privi di segno, i quali rappresentano la distanza del pixel dalla telecamera. Il valore 0 significa che non si \`e potuti ricavare dati sulla distanza. Questo \`e il formato dell'immagine che ci aspettiamo in ingresso alla funzione e in figura \ref{fig:rawDepthImage} \`e riportato il risultato della visualizzazione per mezzo di OpenCV di questo formato di immagini.

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=8cm]{rawDepthImageC.png}
  \caption{Output stream a colori Intel RealSense SR300.}
  \label{fig:colorSR300}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=8cm]{rawDepthImageD.png}
  \caption{Output stream di profondit\`a Intel RealSense SR300 grezzo.}
  \label{fig:rawDepthImage}
\end{minipage}
\end{figure}

Si noti che la leggera discrepanza tra le due immagini \`e dovuta al fatto che il sensore a infrarossi e quello per le immagini a colori si trovano in posizioni diverse. Inoltre il sensore a infrarossi ha un FOV pi\`u elevato.

\subsection{Funzionamento getColorMap()}
Il primo step \`e portare il valore dei pixel per i quali non \`e stato possibile ricavare il dato di distanza alla massima distanza rappresentabile. Quindi portiamo tutti i pixel nulli a 65535. Questo \`e implementato dalla funzione \verb|setTo()|.

Quindi convertiamo l'immagine da 16 bit a 8 bit usando un fattore di scala. Si noti che questa conversione \`e con perdita.

Otteniamo il risultato in figura \ref{fig:getColorMapStep1}.

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=8cm]{getColorMapStep1.png}
  \caption{getColorMap step 1.}
  \label{fig:getColorMapStep1}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=8cm]{getColorMapStep2.png}
  \caption{getColorMap step 2.}
  \label{fig:getColorMapStep2}
\end{minipage}
\end{figure}

A questo punto vogliamo rappresentare i pixel pi\`u vicini con colori pi\`u chiari e i pixel pi\`u lontani con colori pi\`u scuri. Per ottenere ci\`o \`e sufficiente sottrarre l'immagine ad un matrice delle stesse dimensioni in cui tutti i pixel sono bianchi. Otteniamo l'immagine in figura \ref{fig:getColorMapStep2}:

Quindi passiamo l'immagine cos\`\i\ ottenuta alla funzione \verb|equalizeHist()| la quale aumenta il contrasto dell'immagine che le viene passata come argomento. In questo modo i colori che rappresentano la distanza nell'immagine risultano pi\`u evidenti. Poich\'e questa funzione accetta in ingresso solamente immagini a 8 bit si \`e resa necessaria la conversione con perdita.

In figura \ref{fig:getColorMapStep3} il risultato dell'applicazione della funzione.

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=8cm]{getColorMapStep3.png}
  \caption{getColorMap step 3.}
  \label{fig:getColorMapStep3}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=8cm]{getColorMapStep4.png}
  \caption{getColorMap step 4.}
  \label{fig:getColorMapStep4}
\end{minipage}
\end{figure}

Infine viene applicata la funzione \verb|applyColorMap()| la quale va semplicemente a ricolorare l'immagine in scala di grigi per avere una migliore percezione delle informazioni fornite dalle telecamere RealSense. In questo modo i pixel pi\`u vicini alla telecamera assumeranno il colore rosso, mentre quelli pi\`u lontani tenderanno al blu.

Si noti infine che durante questa elaborazione sono stati conservati i dati del pixel pi\`u lontano e pi\`u vicino e convertiti in distanza. Ci\`o ha aiutato molto durante lo sviluppo del progetto per poter calibrare al meglio il contatore.

%% Interruzione di pagina
\newpage

\section{Metodo getFrame()}
Questo metodo \`e alla base del funzionamento di questa versione del Passenger Counter. Esso va a recuperare i dati forniti dalle telecamere RealSense e li converte in un formato che pu\`o essere dato in pasto allo stesso algoritmo usato a valle della sottrazione del background nella precedente versione del contatore.

Qui di seguito \`e riportato il codice della funzione:

\begin{lstlisting}[language=C++, caption=RSPCN.cpp metodo getFrame(), style=customcpp]
Mat RSPCN::getFrame(Mat depthImage, int thresholdCentimeters) {
    Mat frame;

    // If depthImage(x,y) == NODATA, set it to 65535
    depthImage.setTo(65535, depthImage == NODATA);

    // Threshold only accepts CV_8U or CV_32F types
    depthImage.convertTo(depthImage, CV_32FC1);

    // Converting threshold from cm to pixel value
    int threshPixel = thresholdCentimeters / (100*scale);

    // Applying threshold
    threshold(depthImage, depthImage, threshPixel, 65535, THRESH_BINARY);

    // Convert to CV_8U (lossy conversion)
    depthImage.convertTo(depthImage, CV_8UC1, 255.0 / 65535);

    // Invert b&w: white = foreground, black= background.
    frame = cv::Scalar::all(255) - depthImage;

    return frame;
}
\end{lstlisting}

In ingresso abbiamo sempre il formato immagine illustrato in figura \ref{fig:rawDepthImage} e la soglia espressa in centimetri da applicare all'immagine in ingresso.

Come nel caso precedente partiamo applicando la funzione \verb|setTo()| per imporre tutti i pixel nulli a 65535. Quindi convertiamo l'immagine da 16 bit privi di segno a 32 bit floating point. Questo passaggio  \`e necessario per poter dare in pasto l'immagine alla funzione \verb|threshold()|, la quale accetta solamente immagini nel formato 8 bit privi di segno o 32 bit floating point. Il risultato di questo passaggio \`e visibile in figura \ref{fig:getFrameStep1}.

Passiamo quindi ad applicare la soglia sull'immagine. Siccome i valori dei pixel rappresentano una distanza possiamo eliminare tutto ci\`o che si trova oltre una certa distanza dalla telecamera per mezzo della funzione \verb|threshold()|. Anzitutto \`e necessario convertire la soglia da cm al valore numerico applicabile sui pixel, quindi si passa all'applicazione della soglia vera e propria.

\verb|scale| \`e la scala alla quale \`e impostata la telecamera e viene ricavata interrogandola per mezzo della libreria librealsense. Va moltiplicata per un fattore cento in quanto l'unit\`a di misura con la quale viene fornita sono i metri e noi abbiamo una soglia espressa in centimetri.

%% Interruzione di pagina
\newpage

La soglia va quindi ad effettuare la seguente operazione:

\begin{equation}
dstPixel(x,y) = \begin{cases} 65535, & \mbox{se } srcPixel(x,y) > threshPixel \\ 0, & \mbox{altrimenti } \end{cases}
\end{equation}

In sostanza vengono eliminati dall'immagine tutti gli oggetti che si trovano ad una distanza dalla telecamera maggiore della soglia impostata. Si noti che a questo punto si sono perse tutte le informazioni di distanza date dai valori dei pixel. 

Ora \`e possibile convertire l'immagine al formato 8 bit privi di segno applicando un semplice fattore di scala per mezzo della funzione \verb|convertTo()|. In figura \ref{fig:getFrameStep2} \`e riportato il risultato dell'operazione di soglia e conversione nel formato 8 bit. Si pu\`o notare come lo sfondo sia stato completamente rimosso. 

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=8cm]{getFrameStep1.png}
  \caption{getFrame step 1.}
  \label{fig:getFrameStep1}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=8cm]{getFrameStep2.png}
  \caption{getFrame step 2.}
  \label{fig:getFrameStep2}
\end{minipage}
\end{figure}

Infine, poich\'e vogliamo avere che gli oggetti in primo piano siano bianchi e gli oggetti dello sfondo neri, non ci resta che sottrarre l'immagine ad un matrice delle stesse dimensioni in cui tutti i pixel sono bianchi. Il risultato \`e riportato in figura \ref{fig:getFrameStep3}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=9cm]{getFrameStep3.png}
  \caption{getFrame step 3.}
  \label{fig:getFrameStep3}
\end{figure}

L'obiettivo di queste operazioni \`e andare a ricavare una foreground mask contenente solo la testa dei passeggeri che transitano sotto la telecamera. Rilevando solamente la testa delle persone, infatti, otteniamo un tracciamento molto pi\`u preciso ed affidabile rispetto all'implementazione del Passenger Counter precedente.

\section{Analisi del kernel del programma: funzione count()}

La funzione \verb|count()|, anche per questa versione del contatore, \`e il cuore del programma. Il codice \`e suddivisibile ancora una volta in tre sezioni:

\begin{enumerate}
\item Estrazione degli oggetti in primo piano.
\item Rilevamento e tracking degli oggetti di nostro interesse.
\item Conteggio degli attraversamenti.
\end{enumerate}

\`E stato possibile riutilizzare il codice della versione del contatore a sottrazione del background per le ultime due sezioni. Vi sono per\`o alcune differenze nel codice che vedremo nel seguito.

\subsection{Estrazione degli oggetti in primo piano}

\begin{lstlisting}[language=C++, caption=RSPCN.cpp metodo count() parte 1, style=customcpp]
void RSPCN::count()
{
    // Streams
    Mat frame;
    Mat rawDepth;
    Mat morphTrans;
    Mat depthColorMap;

    // Contours variables
    vector<vector<Point> > contours;
    vector<Vec4i> hierarchy;

    // Calibration
    int thresholdCentimeters = MAX_RANGE_CM;
    int blur_ksize = BLUR_KSIZE;
    int areaMin = AREA_MIN;
    int xNear = X_NEAR;
    int yNear = Y_NEAR;
    int maxPassengerAge = MAX_PASSENGER_AGE;

    // Execution time
    duration<double> loopTime;
    bool firstLoop = true;

    // Start streaming
    dev->start();

    while(!halt)
    {
        // Synchronization
        if( dev->is_streaming( ) )
        {
            if(framerateStabilizationOn)
                dev->wait_for_frames( );
            else
                dev->poll_for_frames(); // Non blocking option
        }

        // Get frame data
        Mat color(Size(ImageWidth, ImageHeight), CV_8UC3, (void*)dev->get_frame_data(rs::stream::color), Mat::AUTO_STEP);
        Mat depth(Size(ImageWidth, ImageHeight), CV_16U , (void*)dev->get_frame_data(rs::stream::depth), Mat::AUTO_STEP);

        // -- CONVERTING DEPTH IMAGE AND THRESHOLDING
        frame = getFrame(depth, thresholdCentimeters);

        // -- DENOISING
        blur(frame, morphTrans, Size(blur_ksize,blur_ksize));

\end{lstlisting}

Come nella versione precedente del Passenger Counter, in questa fase abbiamo l'estrazione degli oggetti in primo piano.

\noindent Variabili:
\begin{itemize}
\item \verb|color|: questa \`e la matrice che conterr\`a i frame dello stream a colori in ingresso dalla telecamera. Verr\`a usata solamente per visualizzare le informazioni sul funzionamento del contatore.
\item \verb|frame|: rappresenta l'output della funzione \verb|getFrame()| di cui si \`e parlato in precedenza.
\item \verb|rawDepth|: questa \`e la matrice che conterr\`a i frame grezzi dello stream di profondit\`a in ingresso dalla telecamera ad infrarossi. \`E una copia della matrice \verb|depth| prima che sia passata alle funzioni di elaborazione delle immagini.
\item \verb|depthColorMap|: rappresenta l'output della funzione \verb|getColorMap()|.
\item \verb|morphTrans|: \`e la copia di \verb|frame| a seguito dell'applicazione degli algoritmi di riduzione del rumore.
\end{itemize}

\noindent Variabili di calibrazione del contatore:
\begin{itemize}
\item \verb|thresholdCentimeters|: soglia utilizzata nella funzione \verb|getFrame()| espressa in centimetri.
\item \verb|blur_ksize|: valore che regola l'intensit\`a della funzione di blur.
\item \verb|areaMin|: area minima per la quale l'oggetto viene tracciato dal programma.
\item \verb|xNear|: coordinata orizzontale entro la quale due entit\`a vengono identificate come lo stesso passeggero.
\item \verb|yNear|: coordinata verticale entro la quale due entit\`a vengono identificate come lo stesso passeggero.
\item \verb|maxPassengerAge|: valore massimo di \verb|age| permesso. Quando un passeggero supera questo valore viene rimosso dalla memoria.
\end{itemize}

%% Interruzione di pagina
\newpage

\noindent Il codice esegue le seguenti operazioni:
\begin{enumerate}
\item Prima di raccogliere i frame in ingresso si attende che questi vengano ricevuti per mezzo della funzione \verb|wait_for_frames()|. Esiste la versione non bloccante \verb|poll_for_frames()| il cui utilizzo per\`o comporta problemi di stabilit\`a del framerate.
\item A seguito di questa sincronizzazione vengono catturati i frame di profondit\`a grezzi dalla telecamera RealSense utilizzando la funzione \verb|get_frame_data()| resa disponibile dalla libreria RealSense e salvati nella variabile \verb|depth|.
\item Questi frame vengono convertiti in un formato utilizzabile dalla funzione \verb|findContours()| di OpenCV grazie alla funzione \verb|getFrame()| di cui si \`e parlato nel paragrafo precedente e salvati nella variabile \verb|frame|.
\item Quindi si effettua una veloce operazione di riduzione del rumore per mezzo della funzione \verb|blur|.
\end{enumerate}

\subsection{Rilevamento e tracking degli oggetti di nostro interesse}

\begin{lstlisting}[language=C++, caption=RSPCN.cpp metodo count() parte 2, style=customcpp, firstnumber=48]
        // --FINDING CONTOURS
        findContours(morphTrans, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_NONE);

        // For every detected object
        for(unsigned int idx = 0; idx < contours.size(); idx++) {
            // -- AREA
            // Calculating area
            double areaCurrentObject = contourArea(contours[idx]);

            // If calculated area is big enough begin tracking the object
            if(areaCurrentObject > areaMin) {
                // Get bounding rectangle
                Rect br = boundingRect(contours[idx]);
                Point2f objCenter = Point2f((int)(br.x + br.width/2) ,(int)(br.y + br.height/2) );

                // Draw mass center and bounding rectangle
                rectangle( color, br.tl(), br.br(), GREEN, 2, 8, 0 );
                circle( color, objCenter, 5, RED, 2, 8, 0 );

                // --PASSENGERS DB UPDATE
                bool newPassenger = true;

                for(unsigned int i = 0; i < passengers.size(); i++) {

                    // If the passenger is near a known passenger assume they are the same one
                    if( abs(objCenter.x - passengers[i].getCurrentPoint().x) <= xNear &&
                        abs(objCenter.y - passengers[i].getCurrentPoint().y) <= yNear ) {

                        // Update coordinates
                        newPassenger = false;
                        passengers[i].updateCoords(objCenter);
                        // If the object is being tracked it means its active we don't want it to disappear
                        passengers[i].resetAge();

                        break;
                    }
                }

                // If wasn't near any known object is a new passenger
                if(newPassenger) {
                    Passenger p(pid, objCenter);
                    passengers.push_back(p);
                    pid++;
                }
            }
        }
\end{lstlisting}

A questo punto passiamo a rilevare i contorni degli oggetti in primo piano tramite la funzione \verb|findCountours()|. Per ogni oggetto cos\`\i\ ricavato calcoliamo l'area. Se l'area supera un valore minimo deciso arbitrariamente in fase sperimentale viene considerato un passeggero.

\noindent Quindi si passa ad aggiornare la posizione dei passeggeri.

\begin{itemize}
\item Se il passeggero rilevato nei passaggi precedenti si trova entro una determinata distanza in pixel, data dalle variabili \verb|xNear| e \verb|yNear|, da un passeggero gi\`a presente nel database vengono considerati lo stesso passeggero e se ne aggiorna la posizione.
\item Se il passeggero non \`e vicino a nessun passeggero nel database viene considerato un nuovo passeggero e inserito nel database \verb|passengers|.
\end{itemize}

\noindent Infine si va ad effettuare il conteggio degli attraversamenti.

%% Interruzione di pagina
\newpage

\subsection{Conteggio degli attraversamenti}

\begin{lstlisting}[language=C++, caption=RSPCN.cpp metodo count() parte 3, style=customcpp, firstnumber=94]
        // For every passenger in passengers DB
        for(unsigned int i = 0; i < passengers.size(); i++) {
            
            if(passengers[i].getTracks().size() > 1) {

                // -- DRAWING PASSENGER TRAJECTORIES
                polylines(color, passengers[i].getTracks(), false, passengers[i].getTrackColor(),2);

                // -- COUNTING
                // Up to down
                if( (passengers[i].getLastPoint().y < frame.rows/2 && passengers[i].getCurrentPoint().y >= frame.rows/2) ||
                    (passengers[i].getLastPoint().y <= frame.rows/2 && passengers[i].getCurrentPoint().y > frame.rows/2) ) {

                    cnt_out++;

                }

                // Down to up
                if( (passengers[i].getLastPoint().y > frame.rows/2 && passengers[i].getCurrentPoint().y <= frame.rows/2) ||
                    (passengers[i].getLastPoint().y >= frame.rows/2 && passengers[i].getCurrentPoint().y < frame.rows/2) ) {

                    cnt_in++;

                }
            }

            // --UPDATE PASSENGER STATS
            // Updating age
            passengers[i].updateAge();

            // If passenger is outside field of view update coords with last known position
            if(passengers[i].getAge() > 1)
                passengers[i].updateCoords(passengers[i].getCurrentPoint());

            // Removing older passengers
            if(passengers[i].getAge() > (maxPassengerAge * fps) )
                passengers.erase(passengers.begin() + i);
        }
}
\end{lstlisting}

%% Interruzione di pagina
\newpage

Una volta aggiornate le posizioni viene letto l'intero database dei passaggeri attualmente rilevati all'interno del raggio visivo della telecamera. Per ognuno di essi:

\begin{enumerate}
\item Se ne disegna la traccia dello storico delle posizioni.
\item Si rilevano gli attraversamenti della soglia:
    \begin{itemize}
    \item Se l'ultima posizione si trovava al di sotto della soglia e la nuova posizione rilevata al di sopra, si aggiorna il contatore in ingresso. 
    \item Se l'ultima posizione si trovava al di sopra della soglia e la nuova posizione rilevata al di sotto, si aggiorna il contatore in uscita. 
    \end{itemize}
\item Si effettua l'update del parametro \verb|age| dei passeggeri.
\item Se il passeggero si trova fuori dal raggio d'azione della telecamera se ne aggiorna la posizione con l'ultima conosciuta.
\item I passeggeri la cui \verb|age| supera un limite massimo deciso in precedenza, vengono rimossi dalla memoria perch\'e troppo a lungo fuori dal raggio visivo della telecamera.
\end{enumerate}

Si noti che a differenza del caso del Passenger Counter a sottrazione del background, l'area non viene usata per decidere quante persone hanno effettuato l'attraversamento della soglia. Questo perch\'e nel caso delle telecamere RealSense riusciamo a rilevare con precisione la testa dei passeggeri e non l'intera sagoma.

Come nella versione precedente si \`e deciso di plottare a schermo tutte le informazioni utili a semplificare la comprensione del funzionamento del programma e al debugging. Un esempio della schermata tipica durante il funzionamento del contatore in fase sperimentale \`e riportato in figura \ref{fig:RSPCN}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{RSPCN.png}
  \caption{Esempio funzionamento Passenger Counter con telecamere RealSense.}
  \label{fig:RSPCN}
\end{figure}

Si noti che la lieve differenza nella posizione delle due telecamere fa in modo che il rettangolo che inscrive l'oggetto tracciato sia leggermente spostato rispetto all'oggetto, e cos\`\i\ pure il centro dell'oggetto, poich\`e il tracking viene effettuato sullo stream proveniente dalla telecamera a infrarossi, mentre il rettangolo viene disegnato sullo stream proveniente dalla telecamera a colori. 

Questo ovviamente non compromette la funzionalit\`a del programma ma pu\`o creare confusione durante la visualizzazione dei video.

%% Interruzione di pagina
\newpage

\section{Risultati ottenuti}
In questa sezione sono riportati i risultati dei test di funzionamento del contatore con telecamere RealSense. Si \`e cercato di replicare le stesse condizioni dei test effettuati con la versione a sottrazione del background. Ogni test \`e stato effettuato due volte per confrontare le prestazioni delle due telecamere.

\subsection{Dimostrazione funzionamento telecamera R200}
Nel seguito sono riportati alcuni fotogrammi tratti da una registrazione di un test del contatore. Per il test si \`e utilizzata la telecamera Intel RealSense R200, la quale \`e stata montata su uno stipite di una porta ad una altezza di circa 2,10m. Ogni frame riporta a sinistra l'immagine tratta dal video in ingresso dove sono state disegnate le forme per rendere pi\`u comprensibile il tracciamento, al centro l'immagine con le informazioni di profondit\`a (contenute nella variabile \verb|depthColorMap|), a destra l'immagine cui sono state applicate le operazioni di soglia e sottrazione del rumore (conentute nella variabile \verb|frame|).

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{R2001.png}
  \caption{Risultato test RSPCN con telecamera R200: frame 1.}
  \label{fig:R2001}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{R2002.png}
  \caption{Risultato test RSPCN con telecamera R200: frame 2.}
  \label{fig:R2002}
\end{figure}

Si noti ancora una volta come in figura \ref{fig:R2002} il rettangolo appare non centrato sulla testa del passeggero. Ci\`o \`e dovuto al fatto che la telecamera a colori e quelle a infrarossi hanno posizioni diverse.

Inoltre \`e opportuno notare come il centro della testa del passeggero, sempre in figura \ref{fig:R2002}, sia oscurato. Questo \`e dovuto al fatto che il modello R200, poich\`e basato sulla visione stereoscopica, ha un range di funzionamento minimo di circa 30 cm e, in questo caso, una parte della testa si trova fuori dal range operativo della telecamera.

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{R2003.png}
  \caption{Risultato test RSPCN con telecamera R200: frame 3.}
  \label{fig:R2003}
\end{figure}

In figura \ref{fig:R2003} il pallino verde in alto a destra nell'immagine indica la registrazione dell'evento di attraversamento della soglia.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{R2004.png}
  \caption{Risultato test RSPCN con telecamera R200: frame 4.}
  \label{fig:R2004}
\end{figure}

%% Interruzione di pagina
\newpage

\subsection{Dimostrazione funzionamento telecamera SR300}
Nel seguito sono riportati alcuni fotogrammi tratti da una registrazione di un test del contatore effettuata utilizzando la telecamera Intel RealSense SR300, la quale \`e stata montata su uno stipite di una porta ad una altezza di circa 2,10m. Il formato dei frame \`e lo stesso del caso precedente.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{SR3001.png}
  \caption{Risultato test RSPCN con telecamera SR300: frame 1.}
  \label{fig:SR3001}
\end{figure}

Si noti che in questo caso nell'immagine coi dati di profondit\`a non viene rilevato il pavimento. Questo \`e dovuto al fatto che il range operativo della telecamera SR300 \`e inferiore a quello della R200. Ci\`o per\`o non influisce sul corretto funzionamento del contatore.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{SR3002.png}
  \caption{Risultato test RSPCN con telecamera SR300: frame 2.}
  \label{fig:SR3002}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{SR3003.png}
  \caption{Risultato test RSPCN con telecamera SR300: frame 3.}
  \label{fig:SR3003}
\end{figure}

Si noti che, in questo caso, \`e stata rilevata correttamente l'intera testa del passeggero. Questo \`e dovuto al fatto che la telecamera SR300 ha un range di funzionamento molto migliore poich\`e sfrutta la luce strutturata per ricostruire i dati di profondit\`a dell'immagine.

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{SR3004.png}
  \caption{Risultato test RSPCN con telecamera SR300: frame 4.}
  \label{fig:SR3004}
\end{figure}

\subsection{Performance}
Per la misura dei tempi di esecuzione si \`e fatto uso della libreria standard \verb|chrono| di C++. I test sono stati effettuati usando le due telecamere RealSense su due diversi dispositivi: il ReliaGate 20-25 ed una macchina Linux ad alte prestazioni. I tempi sono stati mediati sul numero di cicli effettuati, cio\`e pari al numero di frame catturati dalla telecamera.

Nel seguito sono riportati i risultati relativi alle performance dell'applicazione utilizzando la telecamera RealSense SR300:

\begin{figure}[h!]
    \centering
    \fbox{
    \begin{tikzpicture}
    \begin{axis}[
    xbar,
    y axis line style = { opacity = 0 },
    align = center,
    enlarge y limits  = 0.2,
    enlarge x limits  = 0.02,
    symbolic y coords = {Metodo getFrame, Sottrazione rumore, Rilevamento contorni, Tracking oggetti},
    nodes near coords,
    grid = major,
    grid style={dashed,gray!30},
    xlabel = {Tempo di esecuzione [ms]},
    ]
    % ReliaGate 20-25
    \addplot 
        coordinates {(7.80747,Metodo getFrame) (8.69046,Sottrazione rumore) (2.0645,Rilevamento contorni)
                        (0.033218,Tracking oggetti)};
    % Sistema Linux
    \addplot 
        coordinates {(0.913448,Metodo getFrame) (0.295688,Sottrazione rumore) (0.228186,Rilevamento contorni)
                        (0.054902,Tracking oggetti)};
    \legend{ReliaGate 20-25, Sistema Linux}
    \end{axis}
    \end{tikzpicture}
    }
    \caption{Grafico tempi di esecuzione RSPCN con telecamera RealSense SR300.}
\end{figure}

\begin{table}[h!]
    \centering
	\begin{tabular}{|l|c|c|}
	\hline
    & ReliaGate 20-25 & Sistema Linux \\ \hline
    Tempo di esecuzione del loop principale & 37.1384 ms & 32.2761 ms \\ \hline
    Metodo getFrame & 7.80747 ms & 0.913448 ms \\ \hline
    Sottrazione del rumore & 8.69046 ms & 0.295688 ms \\ \hline
    Algoritmo rilevamento contorni & 2.0645 ms & 0.228186 ms \\ \hline
    Rilevamento e tracking degli oggetti & 0.033218 ms & 0.054902 ms \\ \hline
    Frame Rate & 30 fps & 30 fps \\ \hline
	\end{tabular}
    \caption{Performance Passenger Counter con telecamera RealSense SR300.}
\end{table}

Anche in questo caso all'interno del tempo di esecuzione del loop principale \`e incluso il tempo speso ad attendere i frame dalla telecamera.

%% Interruzione di pagina
\newpage

Nel seguito sono riportati i risultati relativi alle performance dell'applicazione utilizzando la telecamera RealSense R200:

\begin{figure}[h!]
    \centering
    \fbox{
    \begin{tikzpicture}
    \begin{axis}[
    xbar,
    y axis line style = { opacity = 0 },
    align = center,
    enlarge y limits  = 0.2,
    enlarge x limits  = 0.02,
    symbolic y coords = {Metodo getFrame, Sottrazione rumore, Rilevamento contorni, Tracking oggetti},
    nodes near coords,
    grid = major,
    grid style={dashed,gray!30},
    xlabel = {Tempo di esecuzione [ms]},
    ]
    % ReliaGate 20-25
    \addplot[green!20!black,fill=green!80!white] 
        coordinates {(2.23713,Metodo getFrame) (1.81564,Sottrazione rumore) (0.962472,Rilevamento contorni)
                        (4.48531e-4,Tracking oggetti)};
    % Sistema Linux
    \addplot[yellow!20!black,fill=yellow!80!white] 
        coordinates {(0.517965,Metodo getFrame) (0.162011,Sottrazione rumore) (0.095771,Rilevamento contorni)
                        (1.29373e-4,Tracking oggetti)};
    \legend{ReliaGate 20-25, Sistema Linux}
    \end{axis}
    \end{tikzpicture}
    }
    \caption{Grafico tempi di esecuzione RSPCN con telecamera RealSense R200.}
\end{figure}

\begin{table}[h!]
    \centering
	\begin{tabular}{|l|c|c|}
	\hline
    & ReliaGate 20-25 & Sistema Linux \\ \hline
    Tempo di esecuzione del loop principale & 16.4351 ms & 15.8996 ms \\ \hline
    Metodo getFrame & 2.23713 ms & 0.517965 ms \\ \hline
    Sottrazione del rumore & 1.81564 ms & 0.162011 ms \\ \hline
    Algoritmo rilevamento contorni & 0.962472 ms & 0.095771 ms \\ \hline
    Rilevamento e tracking degli oggetti & 4.48531e-4 ms & 1.29373e-4 ms \\ \hline
    Frame Rate & 60 fps & 60 fps \\ \hline
	\end{tabular}
    \caption{Performance Passenger Counter con telecamera RealSense R200.}
\end{table}

Come risulta evidente da queste tabelle i tempi di esecuzione sono molto migliorati rispetto al contatore a sottrazione del background. Ora il ReliaGate \`e in grado di mantenere il frame-rate stabile al valore impostato, ci\`o \`e dovuto al fatto che parte della computazione \`e stata spostata in hardware sull'ASIC delle telecamere.

Sempre grazie a questo fatto possiamo notare come i tempi di esecuzione del loop principale siano pressoch\'e identici per entrambi i sistemi, ora entrambi impiegano la maggior parte del tempo di CPU ad attendere il frame successivo in ingresso dalla telecamera (anche in questo caso il metodo \verb|wait_for_frames()| \`e bloccante).

%% Interruzione di pagina
\newpage

\section{Problemi}
Nonostante i miglioramenti anche questa versione del software non \`e esente da problemi, in questo caso sono pi\`u legati alle tecnologie impiegate.

Anzitutto \`e necessario considerare che il programma \`e molto dipendente dall'altezza dei passeggeri. La soglia applicata alle informazioni di profondit\`a implica che i passeggeri aventi un'altezza inferiore a detta soglia non verranno rilevati, mentre i passeggeri troppo alti potrebbero risultare fuori dal range della telecamera. Siccome si suppone che la telecamera sia posta a circa 2 metri da terra, avendo una soglia massima posta a 45 cm, le persone pi\`u basse di 150 cm risultano invisibili al contatore. Questo problema potrebbe essere risolta applicando una soglia pi\`u ``intelligente'', che tenga conto dei massimi locali nell'immagine invece dei massimi assoluti.

Inoltre \`e necessario considerare le caratteristiche tecniche delle telecamere utilizzate.

Come riportato nella tabella \ref{table:SR300Spec} a pagina \pageref{table:SR300Spec} la telecamera SR300 ha un buon range di funzionamento, poich\`e basata sulla luce strutturata per il rilevamento dei dati di profondit\`a, ma non funziona in ambienti esterni. Ci\`o \`e dovuto al fatto che la luce infrarossa emessa dai proiettori della telecamera risulterebbero quasi invisibile sugli oggetti sui quali viene proiettata se \`e presente la luce solare, la quale ha una forte componente infrarossa. Siccome l'ambito di applicazione del contatore \`e dato dai mezzi pubblici, ci si aspetta una illuminazione naturale abbastanza importante e ci\`o pu\`o compromettere il funzionamento della telecamera.

La telecamera R200, i cui dati sono riportati nella tabella \ref{table:R200Spec} a pagina \pageref{table:R200Spec}, garantisce un buon funzionamento in ambiente esterno poich\`e basata sulla visione stereoscopica invece che la luce strutturata. La luce solare in questo caso ne migliora il funzionamento invece che interferire. Purtroppo essendo basata sulla visione stereoscopica il range di funzionamento \`e molto peggiore rispetto alla SR300 per il nostro caso applicativo. Dovendo installare la telecamera ad una altezza di 2,10 metri, il range minimo di 40 cm genera problemi di rilevamento per tutti i passeggeri con una statura superiore al metro e ottanta. Infatti in figura \ref{fig:R2002} \`e possibile notare come parte della testa del passeggero risulti non rilevata correttamente. Nel caso questa area non rilevata fosse troppo ampia si perderebbero le capacit\`a di tracciare i passeggeri e quindi il funzionamento della telecamera risulterebbe compromesso.

Inoltre la telecamera R200 ha un FOV e una risoluzione leggermente inferiori alla telecamera SR300, ci\`o aumenta ulteriormente le difficolt\`a di tracciamento.

In conclusione possiamo affermare che entrambe le telecamere hanno dei difetti per l'applicazione considerata in questa tesi, comunque il funzionamento \`e molto migliore rispetto al caso del Passenger Counter a sottrazione del background e i punti di forza di questa tecnologia sono di gran lunga maggiori dei punti deboli.

\chapter{Confronto implementazioni del Passenger Counter}\label{CapitoloConfronto}
Nel corso della tesi sono state implementate diverse versioni del contatore, in diversi linguaggi. In questa sezione andremo ad analizzare quali sono queste implementazioni, come sono state realizzate  e quali sono le loro prestazioni.

Per brevit\`a nel seguito verranno adottate le seguenti abbreviazioni:
\begin{itemize}
\item PCN: Passenger Counter a sottrazione del background.
\item RSPCN: Passenger Counter con telecamere RealSense.
\end{itemize}

\section{Prestazioni}
Finora abbiamo trattato nel dettaglio le due diverse implementazioni del Passenger Counter. Ora confronteremo quali differenze comportano le diverse tecnologie ed algoritmi utilizzati del punto di vista delle prestazioni del programma.

\subsection{Costo computazionale}
L'algoritmo di sottrazione del background oltre ad essere impreciso \`e molto pesante dal punto di vista computazionale. L'aver spostato gran parte del lavoro sugli ASIC presenti nelle telecamere RealSense permette di avere uno stream in ingresso con un framerate molto pi\`u stabile ed elevato, inoltre permette di utilizzare molte pi\`u telecamere contemporaneamente.

In figura \ref{fig:GraficoTempiPCNRSPCN} a pagina \pageref{fig:GraficoTempiPCNRSPCN} \`e riportato un confronto sui tempi di esecuzione delle funzioni principali delle due implementazioni del contatore. Per il confronto si \`e usata la telecamera SR300 e la webcam poich\`e posseggono la stessa risoluzione e lo stesso framerate.

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
    \centering
    \fbox{
    \begin{tikzpicture}
    \begin{axis}[
    xbar,
    y axis line style = { opacity = 0 },
    align = center,
    enlarge y limits  = 0.2,
    enlarge x limits  = 0.02,
    symbolic y coords = {Rilevamento passeggeri, Sottrazione rumore, Rilevamento contorni, Tracking oggetti},
    nodes near coords,
    grid = major,
    grid style={dashed,gray!30},
    xlabel = {Tempo di esecuzione [ms]},
    ]
    % ReliaGate 20-25
    \addplot 
        coordinates {(7.80747,Rilevamento passeggeri) (8.69046,Sottrazione rumore) (2.0645,Rilevamento contorni)
                        (0.033218,Tracking oggetti)};
    % ReliaGate 20-25 BSPCN
    \addplot 
        coordinates {(29.4414,Rilevamento passeggeri) (2.80683,Sottrazione rumore) (1.09507,Rilevamento contorni) 
                        (0.178409,Tracking oggetti)};
    \legend{RSPCN, PCN}
    \end{axis}
    \end{tikzpicture}
    }
    \caption{Grafico tempi di esecuzione RSPCN e PCN.}
    \label{fig:GraficoTempiPCNRSPCN}
\end{figure}

\subsection{Precisione nel tracciamento}

\subsubsection{Setup esperimento}

Per confrontare i due sistemi di Passenger Counter si \`e utilizzata la seguente configurazione:

\begin{itemize}
\item Telecamera: Intel RealSense SR300. Nel caso del PCN \`e stata usata come semplice telecamera a colori, ci\`o \`e reso possibile dal fatto che, se non viene utilizzata la libreria librealsense, per OpenCV la telecamera SR300 \`e una webcam qualsiasi. Usare la stessa telecamera per entrambe le versioni ha permesso un confronto pi\`u alla pari.
\item Ambiente: la telecamera \`e stata montata sullo stipite di una porta ad una altezza di circa 2,10 m. Entrambe le versioni del programma sono state eseguite sul ReliaGate 20-25.
\item Attraversamenti: per entrambe le versioni sono stati eseguiti 30 attraversamenti della soglia e raccolti i dati del conteggio. Inoltre si \`e testato il funzionamento con l'attraversamento di pi\`u passeggeri contemporaneamente.
\end{itemize}

\subsubsection{Risultati attraversamenti singolo passeggero}

\begin{table}[h!]
\parbox{.45\linewidth}{
\centering
    \begin{tabular}{| l | c |}
    \hline
    \multicolumn{2}{| c |}{Risultati con sottrazione background} \\
    \hline
    \hline
    Passaggi registrati IN & 22 \\ \hline
    Passaggi effettivi IN & 15 \\ \hline
    Passaggi registrati OUT & 19 \\ \hline
    Passaggi effettivi OUT & 15 \\ \hline
    \bf Errore & \bf 36\% \\ \hline
    \end{tabular}
\caption{Risultati tracciamento PCN.}
}
\hfill
\parbox{.45\linewidth}{
\centering
    \begin{tabular}{| l | c |}
    \hline
    \multicolumn{2}{| c |}{Risultati con telecamere RealSense} \\
    \hline
    \hline
    Passaggi registrati IN & 15 \\ \hline
    Passaggi effettivi IN & 15 \\ \hline
    Passaggi registrati OUT & 15 \\ \hline
    Passaggi effettivi OUT & 15 \\ \hline
    \bf Errore & \bf 0\% \\ \hline
    \end{tabular}
\caption{Risultati tracciamento RSPCN.}
}
\end{table}

Le prestazioni del Passenger Counter a sottrazione del background sono di gran lunga peggiori dell'implementazione successiva. Il numero di falsi positivi nel tracciamento \`e molto maggiore del numero di oggetti tracciati correttamente.

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=8cm]{TrackPCN.png}
  \caption{Dettaglio tracking PCN.}
  \label{fig:trackPCN}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=8cm]{TrackRSPCN.png}
  \caption{Dettaglio tracking RSPCN.}
  \label{fig:trackRSPCN}
\end{minipage}
\end{figure}

Il fatto che nel RSPCN venga tracciata solo la testa dei passeggeri permette un tracking molto pi\`u preciso. Infatti nel caso del PCN la forma del soggetto tracciato varia continuamente e, poich\`e il tracciamento \`e riferito al centro dell'area, ci\`o comporta spostamenti repentini. In figura \ref{fig:trackPCN} possiamo notare come la traccia lasciata dal passeggero sia ricca di discontinuit\`a e linee spezzate, d'altra parte in figura \ref{fig:trackRSPCN} la linea di tracciamento risulta molto pi\`u continua.

Inoltre bisogna tenere conto dei falsi positivi generati dalle ombre. Nel caso della sottrazione del background questo \`e un problema molto grave. Qui di seguito \`e riportato un confronto tra le due versioni.

\begin{figure}[h!]
\begin{minipage}[b]{8.5cm}
  \centering
  \includegraphics[width=8cm]{OmbraPCN.png}
  \caption{Dettaglio tracking ombra PCN.}
  \label{fig:ombraPCN}
\end{minipage}
\ \hspace{2mm} \hspace{3mm} \
\begin{minipage}[b]{8.5cm}
 \centering
  \includegraphics[width=8cm]{OmbraRSPCN.png}
  \caption{Dettaglio tracking ombra RSPCN.}
  \label{fig:ombraRSPCN}
\end{minipage}
\end{figure}

Appare evidente che l'RSPCN \`e completamente esente da falsi positivi nel tracciamento dovuti alle ombre, mentre nel caso del PCN sono presenti ben due falsi positivi dovuti ad esse. Questo accade nonostante l'algoritmo per la sottrazione del background sia stato pensato per rilevarle ed eliminarle.

%% Interruzione di pagina
\newpage

\subsubsection{Risultati attraversamenti di pi\`u passeggeri}

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{PassaggioPCN1.png}
  \caption{Attraversamento 2 Passeggeri PCN frame 1.}\label{fig:AttrPCN1}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{PassaggioPCN2.png}
  \caption{Attraversamento 2 Passeggeri PCN frame 2.}\label{fig:AttrPCN2}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{PassaggioPCN3.png}
  \caption{Attraversamento 2 Passeggeri PCN frame 3.}\label{fig:AttrPCN3}
\endminipage
\end{figure}

Anche in questa situazione la versione basata su sottrazione del background non \`e precisa. Fare affidamento sull'area per rilevare il passaggio di pi\`u persone contemporaneamente \`e alla base del problema e comporta un'altissima imprecisione in quanto, soprattutto nel caso di pi\`u passeggeri, la forma dei blob varia molto nel tempo e quindi anche l'area ne \`e influenzata.

Come visibile in figura \ref{fig:AttrPCN3}, il contatore rileva due attraversamenti di due passeggeri invece che uno solo. Questo \`e dovuto al fatto che, essendo la forma del blob cambiata proprio durante l'attraversamento, il centro del blob \`e transitato due volte sulla soglia.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{PassaggioRSPCN1.png}
  \caption{Attraversamento 2 Passeggeri RSPCN frame 1.}\label{fig:AttrRSPCN1}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{PassaggioRSPCN2.png}
  \caption{Attraversamento 2 Passeggeri RSPCN frame 2.}\label{fig:AttrRSPCN2}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{PassaggioRSPCN3.png}
  \caption{Attraversamento 2 Passeggeri RSPCN frame 3.}\label{fig:AttrRSPCN3}
\endminipage
\end{figure}

Per quanto riguarda la versione con telecamere RealSense non ci sono invece problemi di precisione da rilevare. Il comportamento \`e altamente affidabile poich\'e basato sul rilevamento delle teste dei passeggeri le quali non cambiano forma in modo cos\`\i\ drammatico.

%% Interruzione di pagina
\newpage

\subsubsection{Risultati incrocio di pi\`u passeggeri}

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{IncrocioPCN1.png}
  \caption{Incrocio 2 Passeggeri PCN frame 1.}\label{fig:IncrocioPCN1}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{IncrocioPCN2.png}
  \caption{Incrocio 2 Passeggeri PCN frame 2.}\label{fig:IncrocioPCN2}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{IncrocioPCN3.png}
  \caption{Incrocio 2 Passeggeri PCN frame 3.}\label{fig:IncrocioPCN3}
\endminipage
\end{figure}

In questo caso il comportamento del Passenger Counter a sottrazione del background peggiora notevolmente, fino al completo fallimento del conteggio. Come visibile in figura \ref{fig:IncrocioPCN1}, \ref{fig:IncrocioPCN2} e \ref{fig:IncrocioPCN3} il contatore rileva attraversamenti multipli senza che siano avvenuti a causa dell'interferenza delle ombre rilevate erroneamente come passeggeri e al fatto che, come spiegato in precedenza, la funzione \verb|findContours| ingloba nello stesso blob due passeggeri che transitano sotto la telecamera se troppo vicini tra loro. Quindi le due persone nel raggio visivo della telecamera, raggiunto il centro, verranno inglobate nello stesso blob, a discapito dei blob ai quali erano associati precedentemente. Ci\`o implica che verr\`a tracciata una nuova entit\`a composta dai due passeggeri che resta ferma al centro del frame finch\`e le due persone non sono sufficientemente lontane da tornare ad essere rilevate come due blob separati. Il risultato \`e un comportamento impredicibile del programma.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{IncrocioRSPCN1.png}
  \caption{Incrocio 2 Passeggeri RSPCN frame 1.}\label{fig:IncrocioRSPCN1}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{IncrocioRSPCN2.png}
  \caption{Incrocio 2 Passeggeri RSPCN frame 2.}\label{fig:IncrocioRSPCN2}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{IncrocioRSPCN3.png}
  \caption{Incrocio 2 Passeggeri RSPCN frame 3.}\label{fig:IncrocioRSPCN3}
\endminipage
\end{figure}

\noindent La versione con telecamere ad infrarossi invece, ancora una volta, si conferma molto precisa e rileva correttamente gli attraversamenti anche in questa situazione.

\subsection{Dipendenza dall'ambiente}
La versione a sottrazione del background ha una forte dipendenza dalle condizioni di luce. Variazioni troppo brusche nelle condizioni di illuminazione ne compromettono il funzionamento, essendo basato su un modello dello sfondo costante. Per lo stesso motivo necessita che la telecamera sia montata su una struttura che non sia soggetta a vibrazioni. Nell'applicazione del sistema in un mezzo di trasporto entrambe queste situazioni si verificano spesso e ci\`o degraderebbe le prestazioni del PCN, mentre la versione RealSense del contatore \`e esente da entrambi questi problemi.

Infine, in condizioni di bassa illuminazione, il PCN si trova ad avere a che fare con un rumore molto elevato e quindi un degrado delle prestazioni. D'altro canto, essendo le telecamere RealSense dotate di proiettori di luce infrarossa, il RSPCN non risente di problemi di scarsa illuminazione e anzi, pi\`u l'ambiente \`e privo di luce migliore \`e il suo funzionamento.

\subsection{Raggio di azione}
L'implementazione con sottrazione del background non ha problemi legati al range di funzionamento, mentre le telecamere RealSense hanno un range piuttosto limitato e, nel caso della telecamra R200, ci\`o pu\`o compromettere il corretto funzionamento del programma.

Abbiamo visto infatti in figura \ref{fig:R2002} a pagina \pageref{fig:R2002} come l'eccessiva vicinanza dell'oggetto da tracciare non permetta alla telecamera di rilevare correttamente i dati di profondit\`a. Questo comporta un'area priva di dati al centro dell'oggetto da tracciare e, nel caso quest'area diventi troppo grande, un fallimento nel rilevamento dell'oggetto nell'immagine e/o un oggetto tracciato in modo non corretto.

\subsection{Costi}
Il contatore con sottrazione dello sfondo non \`e basato su nessun tipo di tecnologia particolare e pu\`o essere implementato con qualsiasi tipo di telecamera. L'RSPCN invece dipende dalle telecamere Intel RealSense, le quali hanno un costo pi\`u elevato di una semplice telecamera. Dal punto di vista dei costi di produzione questo \`e un grosso vantaggio.

Bisogna altres\`\i\ considerare il fatto che il PCN necessita di un hardware pi\`u potente, e quindi pi\`u costoso, a causa del suo costo computazionale pi\`u elevato.

\subsection{Conclusioni}
Poich\`e i vantaggi della versione con telecamere RealSense superano di gran lunga gli svantaggi, si \`e ritenuto opportuno utilizzare questa come versione principale del progetto. Nel seguito verranno trattati i vari porting e versioni del codice, tutte queste sono riferite alla versione con telecamere ad infrarossi. La versione a sottrazione del background, a causa della sua imprecisione ed eccessivo costo computazionale, non \`e stata considerata meritevole di ulteriore attenzione e quindi accantonata in favore della versione RealSense.

%% Interruzione di pagina
\newpage

\section{Versioni realizzate e porting}
Nel corso della tesi sono state realizzate molteplici versioni dello stesso codice. Il motivo di questi porting \`e quello di adattarsi al linguaggio di programmazione utilizzato maggiormente in Eurotech. Nel seguito analizzeremo le versioni del Passenger Counter con telecamere RealSense che sono state realizzate nel corso della tesi.

\subsection{Versione C++}
Il codice analizzato finora \`e riferito a questa versione del codice. Siccome OpenCV e la libreria RealSense sono scritte in questo linguaggio si \`e pensato di usare questo come punto di partenza. In seguito alla prima implementazione si \`e reso necessario il porting negli altri linguaggi per le motivazioni descritte in precedenza.

\subsection{Wrapping in Java}
Come primo passo si \`e deciso di effettuare un wrapping in Java del codice nativo sfruttando il tool open source \textbf{SWIG}\footnote{Pagina web del progetto: http://www.swig.org/}.

% TODO: Il seguente paragrafo \`e stato copiato spudoratamente. Cambiare.
SWIG (Simplified Wrapper and Interface Generator - Wrapper semplificato e generatore di interfacce) \`e un wrapper open source utilizzato per collegare i programmi per elaboratore o librerie scritte in C o C++ con altri linguaggi come C Sharp, Java, JavaScript, Go, Modula-3, OCaml, Octave, e Scheme.

L'obiettivo \`e quello di consentire la chiamata di funzioni native (C o C++) da altri linguaggi di programmazione, passando tipi di dati complessi, mantenendo la memoria libera, ereditariet\`a delle classi dei vari linguaggi, ecc. Il programmatore scrive un file di interfaccia che contiene un elenco di funzioni C/C++ che devono essere rese visibili ad un interprete. SWIG compila il file di interfaccia e genera normale C/C++ ed il codice nel linguaggio di programmazione di destinazione. SWIG generer\`a codice di conversione o serializzazione per le funzioni con argomenti semplici; il codice di conversione per i tipi complessi di argomenti deve essere scritto dal programmatore. Lo strumento SWIG crea codice sorgente che fornisce il collante tra C/C++ e il linguaggio di destinazione. A seconda del linguaggio software, questo collante \`e disponibile in due forme:

\begin{itemize}
\item una libreria condivisa che un interprete esistente pu\`o collegare come una qualche forma di modulo di estensione, o
\item una libreria condivisa che pu\`o essere collegata ad altri programmi compilati nel linguaggio di destinazione (ad esempio, utilizzando JNI in Java).
\end{itemize}

SWIG non viene utilizzato per chiamare le funzioni interpretate da codice nativo, questo deve essere fatto dal programmatore manualmente.

La versione realizzata con questo tool conserva le prestazioni del codice nativo pur permettendo l'interfacciamento con Java. Si \`e comunque ritenuto necessario compiere un ulteriore passo ed effettuare il porting vero e proprio dell'applicazione in Java.

\subsection{Versione Java}
Il porting dell'applicazione ha comportato subito grossi problemi di compatibilit\`a poich\'e, bench\'e OpenCV avesse introdotto recentemente il supporto a Java, a causa della libreria librealsense \`e stato necessario appoggiarsi al progetto \textbf{JavaCPP}\footnote{Repository del progetto all'indirizzo: https://github.com/bytedeco/javacpp}.

JavaCPP \`e un progetto open source il cui obiettivo \`e fornire un accesso efficiente al codice nativo C++ dentro Java. Esso sfrutta le similitudini sintattiche e semantiche tra Java e C++ per raggiungere questo obiettivo. Sfrutta la Java Nativi Interface (JNI) quindi funziona per tutte le implementazioni di Java SE.

Praticamente, parsando il codice nativo degli header file delle librerie, realizza delle interfacce Java che permettono al codice Java di effettuare le chiamate delle librerie C++ installate sulla macchina host. Tramite questa pratica, i manutentori del progetto hanno prodotto delle interfacce complete per OpenCV, FFmpeg, libdc1394, PGR FlyCapture, OpenKinect, videoInput, ARToolKitPlus, e altri come parti del progetto \textbf{JavaCPP Presets}\footnote{Repository del progetto all'indirizzo: https://github.com/bytedeco/javacpp-presets}.

JavaCPP Presets \`e un sotto progetto di JavaCPP che rende disponibile le configurazioni e le classi di interfacciamento verso le librerie C++ pi\`u usate realizzate sfruttando JavaCPP. I file di configurazione nel pacchetto org.bytedeco.javacpp sono usati dal Parser per creare dai file header C/C++ le interfacce Java che puntano al pacchetto di JavaCPP, il quale a sua volta \`e usato dal Generator e il compilatore nativo C++ per produrre le librerie JNI richieste.

Nella repository del progetto sono inoltre disponibili i file \verb|.jar| precompilati per l'interfacciamento alle librerie librealsense, i quali sono stati usati per facilitare il porting dell'applicazione in Java.

Purtroppo questa interfaccia fa uso del progetto \textbf{JavaCV}\footnote{Repository del progetto all'indirizzo: https://github.com/bytedeco/javacv}, il quale \`e un wrapper Java di OpenCV realizzato sempre dal progetto JavaCPP sfruttando i preset resi disponibili da JavaCPP Presets. \`E stato quindi necessario utilizzare JavaCV al posto di OpenCV per realizzare questa versione del codice nonostante quest'ultimo disponesse di un suo interfacciamento a Java. \`E necessario notare per\`o che il progetto JavaCV \`e pi\`u vecchio dell'attuale interfacciamento ufficiale a Java di OpenCV e molto pi\`u completo della controparte. Nonostante ci\`o le differenze tra i due erano cos\`\i\ spiccate che \`e stata necessaria una revisione abbastanza corposa del codice e in alcuni casi non \`e stato possibile portare tutte le feature presenti nella versione C++.

Caratteristiche di questa implementazione:
\begin{itemize}
\item Utilizzo del framework Java per il display delle immagini al posto del modulo HighGui di OpenCV assente in questa versione della libreria.
\item Parallelizzazione della computazione affidata a Java
\item Aumento della portabili\`a del codice
\end{itemize}
Nonostante l'overhead computazionale dovuto all'interfacciamento Java, l'ottima parallelizzazione compiuta dalla Java Virtual Machine ha permesso una distribuzione del calcolo su tutti i thread disponibili sul ReliaGate che mitiga questo problema. Il codice cos\`\i\ realizzato quindi ha delle ottime performance anche su macchine poco prestanti.

%% Interruzione di pagina
\newpage

Qui di seguito sono riportati i dati dei tempi di esecuzione delle funzioni principali del Passenger Counter con telecamere RealSense in Java. I dati sono riferiti all'uso della telecamera R200 sul ReliaGate 20-25.

\begin{figure}[h!]
    \centering
    \fbox{
    \begin{tikzpicture}
    \begin{axis}[
    xbar,
    y axis line style = { opacity = 0 },
    align = center,
    enlarge y limits  = 0.2,
    enlarge x limits  = 0.02,
    symbolic y coords = {Metodo getFrame, Sottrazione rumore, Rilevamento contorni, Tracking oggetti},
    nodes near coords,
    grid = major,
    grid style={dashed,gray!30},
    xlabel = {Tempo di esecuzione [ms]},
    ]
    % ReliaGate 20-25 C++
    \addplot[green!20!black,fill=green!80!white] 
        coordinates {(2.23713,Metodo getFrame) (1.81564,Sottrazione rumore) (0.962472,Rilevamento contorni)
                        (4.48531e-4,Tracking oggetti)};
    % ReliaGate 20-25 Java
    \addplot[magenta!20!black,fill=magenta!80!white] 
        coordinates {(4.008504,Metodo getFrame) (1.230020,Sottrazione rumore) (0.163716,Rilevamento contorni)
                        (0.059985,Tracking oggetti)};
    \legend{Versione C++, Versione Java}
    \end{axis}
    \end{tikzpicture}
    }
    \caption{Grafico tempi di esecuzione versioni Passenger Counter con telecamera R200.}
\end{figure}

\begin{table}[h!]
    \centering
	\begin{tabular}{|l|c|c|}
	\hline
    & Versione C++ & Versione Java \\ \hline
    Tempo di esecuzione del loop principale & 16.4351 ms & 16.932457 ms \\ \hline
    Metodo getFrame & 2.23713 ms & 4.008504 ms \\ \hline
    Sottrazione del rumore & 1.81564 ms & 1.230020 ms \\ \hline
    Algoritmo rilevamento contorni & 0.962472 ms & 0.163716 ms \\ \hline
    Rilevamento e tracking degli oggetti & 4.48531e-4 ms & 0.059985 ms \\ \hline
    Frame Rate & 60 FPS & 60 FPS \\ \hline
	\end{tabular}
    \caption{Confronto performance versioni Passenger Counter con telecamera R200.}
\end{table}

Per ricavare questi dati si \`e fatto uso del metodo \verb|currentTimeMillis()| reso disponibile dalla classe \verb|System| di Java. Questo metodo ritorna il tempo corrente in millisecondi e permette una granularit\`a sufficiente a fare una stima dei tempi di esecuzione delle funzioni implementate in Java. Similmente al caso C++ si \`e mediato i tempi di esecuzione sul numero di cicli del loop principale.

% \subsection{Versione OSGi}
% \textbf{In pausa finch\`e non siamo sicuri di implementarla.}

\chapter{Piattaforma Yocto: realizzazione e features}\label{CapitoloYocto}
In questo capitolo tratteremo la distribuzione Linux, realizzata per mezzo del progetto Yocto, a supporto degli applicativi trattati nelle sezioni precedenti. Vedremo quali sono le sue caratteristiche, come \`e stata realizzata e i problemi riscontrati nella sua realizzazione.

\section{Caratteristiche della distribuzione}
Il risultato finale di quanto si \`e realizzato \`e riportato in figura \ref{fig:SWStack}.

\begin{figure}[h!]
  \centering
\includegraphics[width=0.8\textwidth]{SoftwareStackDiag.png}
  \caption{Diagramma dello stack software della distribuzione realizzata.}
  \label{fig:SWStack}
\end{figure}

La distribuzione \`e formata da quattro componenti fondamentali:

\begin{enumerate}
\item \textbf{Poky}: la reference distribution del progetto Yocto. Fa da template per la costruzione della propria distro Linux customizzata. Nel progetto si \`e usata la versione 2.2 (nome in codice Morty) a causa delle restrizioni dovute alle librerie RealSense.
\item \textbf{OpenCV}: come visto in precedenza, fondamentale per l'image processing.
\item \textbf{librealsense}: necessaria per l'interfacciamento con le telecamere RealSense.
\item \textbf{OpenJDK 8}: necessario per l'esecuzione delle versioni basate su Java del Passenger Counter.
\end{enumerate}

%% Interruzione di pagina
\newpage

\section{Build System di Yocto}
Come spiegato nella sezione \ref{Yocto} a pagina \pageref{Yocto}, il progetto Yocto \`e basato su ``ricette''. Queste ricette contengono dei metadati con le istruzioni su come installare il software nella distribuzione da realizzare. Ogni ricetta esegue le seguenti operazioni in ordine:

\begin{enumerate}
\item Scarica i sorgenti del software.
\item Effettua la cross-compilazione dei sorgenti.
\item Realizza un pacchetto (.rpm, .deb, a seconda delle impostazioni).
\item Installa il pacchetto software nella distribuzione.
\end{enumerate}

Una volta scaricate ed installate nella distribuzione tutte le ricette, viene creata l'immagine del sistema operativo che pu\`o essere scritta su un supporto di memoria e caricata sull'hardware.

Il build system di Yocto \`e organizzato in cartelle: la directory principale, o \textbf{Source Directory}, viene creata in seguito alla clonazione della repository Git di Poky. La Source Directory contiene BitBake, metadati, documentazione e altri file che supportando il progetto Yocto. Al suo interno sono presenti i layer fondamentali per la costruzione della distribuzione template e qui andranno scaricati i layer per le ricette aggiuntive. Qui verr\`a creata la \textbf{Build Directory}. Questo termine si riferisce all'area usata dall'OpenEmbedded build system per effettuare la build della distribuzione.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.5]{YoctoDirectory.png}
  \caption{Struttura della Source Directory del progetto Yocto.}
  \label{fig:YoctoDirectory}
\end{figure}

%% Interruzione di pagina
\newpage

Qui di seguito verr\`a analizzata la struttura delle cartelle principali della Source Directory e le loro funzioni.

\subsubsection{bitbake}
Questa directory include una copia di BitBake per facilitare l'uso del build system. Solitamente questa copia \`e riferita alla versione pi\`u recente di BitBake. BitBake \`e un interprete di metadati, legge i metadati del progetto Yocto ed esegue le task definite da quei dati. Quando viene eseguito il comando \verb|bitbake| che lancia il processo di build, viene chiamato l'eseguibile principale di BitBake, il quale risiede nella cartella bitbake/bin/. 

Lanciare l'environment setup script (esempio: \verb|oe-init-build-end| o \verb|oe-init-build-env-memred|) inserisce gli script e la directory bitbake/bin nel PATH della shell.

\subsubsection{build}
Questa cartella rappresenta la Build Directory.

Questa cartella contiene i file di configurazione dell'utente e l'output generato dal build system OpenEmbedded nella sua configurazione standard. La Build Directory viene creata inizialmente quando viene lanciato l'environment setup script di OpenEmbedded.

Le preferenze circa il software da installare nella distribuzione e il processo di build vanno specificate in due file di configurazione della Build Directory:

\begin{itemize}
\item \textbf{build/conf/local.conf}: Il file local.conf contiene le varie opzioni di configurazione. Ogni variabile impostata qui fa l'override di una qualsiasi altra variabile impostata da un'altra parte nell'environment.

Questo file permette l'impostazione della macchina per la quale si vuole realizzare la build, quali tipi di pacchetti usare, e la locazione di memoria alla quale accedere ai file scaricati.

Se il file local.conf non \`e presente quando viene iniziato il processo di build l'OpenEmbedded build system lo crea a partire dal file local.conf.sample quando viene eseguito l'environment setup script.

La sorgente del file local.conf.sample dipende dalla variabile \verb|$TEMPLATECONF|, la quale \`e impostata di default a meta-poky/conf quando il processo di build proviene dall'ambiente di sviluppo del progetto Yocto, e a meta/conf quanto il processo di build proviene dall'ambiente di sviluppo di OpenEmbedded Core.

\item \textbf{build/conf/bblayers.conf}: Questo file di configurazione definisce i layer, i quali sono degli alberi di directory, attraversati da BitBake. Il file bblayers.conf usa la variabile \verb|BBLAYERS| per elencare i layer che BitBake prova a trovare.

Se il file bblayers.conf non \`e presente quando viene iniziato il processo di build l'OpenEmbedded build system lo crea a partire dal file bblayers.conf.sample quando viene eseguito l'environment setup script.

La sorgente del file bblayers.conf.sample dipende dalla variabile \verb|$TEMPLATECONF|, la quale \`e impostata di default a meta-poky/conf quando il processo di build proviene dall'ambiente di sviluppo del progetto Yocto, e a meta/conf quanto il processo di build proviene dall'ambiente di sviluppo di OpenEmbedded Core.
\end{itemize}

\subsubsection{documentation}
Questa directory contiene i sorgenti della documentazione del progetto Yocto e template e tool necessari a generare le versioni PDF e HTML dei manuali. Ogni manuale \`e contenuto in una sotto cartella.

\subsubsection{meta}
Questa cartella contiene i metadati di OpenEmbedded Core. La directory contiene ricette, classi comuni e file di configurazione per le architetture da emulare (qemux86, qemuarm ecc.). Ad esempio all'interno di questa cartella \`e presente la folder meta/recipes-sato, la quale contiene Sato demo/reference UI/UX ed i dati associati alle sue applicazioni e configurazioni.

\subsubsection{meta-yocto}
Questa directory contiene le configurazioni per la distribuzione reference di Poky.

\subsubsection{meta-yocto-bsp}
Qui sono contenuti i metadati del reference hardware Board Supperto Packages del progetto Yocto.

\subsubsection{meta-selftest}
Questa directory aggiunge ricette e file addizionali usati da OpenEmbedded per verificare il corretto comportamento del build system. Perch\'e questi test vengano eseguiti \`e necessario aggiungere questo layer al file bblayers.conf.

\subsubsection{meta-skeleton}
Questa directory contiene ricette template per lo sviluppo di BSP e kernel custom.

\subsubsection{scripts}
Qui sono contenuti vari script di integrazione che implementano funzionalit\`a extra nell'ambiente di sviluppo del progetto Yocto (ad esempio: script QEMU). Gli environment setup script aggiungono questa cartella al PATH della shell.

\subsubsection{oe-init-build-env}
Questo script \`e uno dei due environment setup script dell'ambiente OpenEmbedded. L'altro script non \`e stato usato nel corso dello sviluppo e non verr\`a trattato nel seguito. Per approfondimenti si faccia riferimento al manuale del progetto Yocto.

L'esecuzione di questo script tramite il comando \verb|source| in una shell applica cambiamenti al \verb|PATH| e imposta altre variabili fondamentali di BitBake basandosi sulla directory di lavoro corrente. \`E necessario eseguire lo script prima di poter eseguire qualsiasi comando BitBake. Lo script sfrutta altri script presenti nella cartella scripts/ per fare gran parte del lavoro.

Una volta eseguito lo script, l'ambiente del progetto Yocto \`e impostato, la Build Directory viene creata, la working directory diventa la Build Directory e compare un elenco di possibili target per BitBake all'interno della shell.

\begin{lstlisting}[language=bash, caption=Output dell'environment setup script.]
$ source oe-init-build-env

### Shell environment set up for builds. ###

You can now run 'bitbake <target>'

Common targets are:
    core-image-minimal
    core-image-sato
    meta-toolchain
    adt-installer
    meta-ide-support

You can also run generated qemu images with a command like 'runqemu qemux86'
\end{lstlisting}
             
Lo script ricava la lista dei defalut target dal file conf-notes.txt, il quale si trova nella directory meta-yocto/.

Eseguire questo script senza una Build Directory crea una build directory all'interno dell'attuale working directory. Se viene fornita una Build Directory come argomento quando viene eseguito lo script, l'OpenEmbedded build system crea la Build Directory scelta. Ad esempio, il seguente comando crea una Build Directory chiamata mybuild che si trova fuori dalla Source Directory.

\begin{lstlisting}[language=bash]
$ source oe-init-build-env ~/mybuilds
\end{lstlisting}
            
%% Interruzione di pagina
\newpage

\section{Sviluppo della distribuzione}
Lo sviluppo della distribuzione \`e stato parallelo allo sviluppo dell'applicazione. Mano a mano che si aggiungevano componenti aumentavano i requisiti dell'applicativo e di conseguenza venivano aggiunti nuovi tool software alla distribuzione. Ci\`o significava aggiungere gradualmente nuovi layer e nuove ricette al build system.

Per lo sviluppo del progetto sono state realizzate principalmente tre versioni della distribuzione, verranno analizzate nel seguito in ordine cronologico di realizzazione.

\subsection{Prima versione}
La prima versione \`e stata realizzata per supportare l'applicazione del Passenger Counter a sottrazione del background, quindi prevedeva solamente l'installazione di OpenCV sul ReliaGate 20-25. Per questa versione sono stati aggiunti solamente due layer alla configurazione standard di Poky:

\begin{itemize}
\item \textbf{meta-intel}: layer contenente metadati per il supporto di hardware intel. Questo layer \`e necessario per poter realizzare un'immagine dell'OS finalizzata all'esecuzione sul processore Intel, sul quale \`e basato il ReliaGate 20-25.
\item \textbf{meta-openembedded}: \`e un layer contenente una collezione di layer dove sono riportate le ricette per i software pi\`u comuni. Questo layer era necessario in quanto contiene le ricette per l'installazione di OpenCV.
\end{itemize}

Poich\`e non vi erano problemi di compatibilit\`a fra i layer \`e stato possibile installare l'ultima versione disponibile di tutto il software. Quindi in questa versione della distribuzione era disponibile OpenCV 3.2.

Inoltre, per visualizzare le finestre di OpenCV contenenti gli stream video dalle telecamere, era necessario disporre del Window System X11. Per questo motivo si \`e deciso di installare la Sato reference User Interface, resa disponibile dal layer \textbf{openembedded-core}, fornito assieme a Poky. Questa interfaccia utente \`e basata su GTK+ e rende disponibile X11.

\subsection{Seconda versione}
Questa versione \`e stata realizzata per supportare il Passenger Counter con telecamere RealSense. Per poter installare correttamente le librerie librealsense e i driver delle telecamere \`e stato necessario aggiungere un nuovo layer: \textbf{meta-intel-realsense}.

Purtroppo questo layer risultava essere compatibile solamente con la versione 4.8 del kernel Linux, contenuta nella versione Morty di Poky. \`E stato quindi necessario effettuare un downgrade della versione di Poky con conseguente downgrade della versione di OpenCV. In questa versione della distribuzione erano quindi presenti OpenCV 3.1 e la libreria librealsense 1.12.1.

Il downgrade di OpenCV ha richiesto qualche accorgimento in fase di cross-compilazione del Passenger Counter in quanto la versione 3.2 di OpenCV aveva ridotto il numero di moduli, integrando pi\`u funzionalit\`a in uno stesso modulo.

\subsection{Terza versione}
Questa versione \`e stata realizzata per supportare il Passenger Counter con telecamere RealSense e le varie versioni Java. Ci\`o ha significato installare una Java Run-time Environment ed un JDK (Java Developer Kit). Si \`e scelto di installare OpenJDK-8 perch\'e il processo di installazione era pi\`u semplice. Sono infatti noti dei problemi di installazione per la versione Oracle del JDK.

\`E stato quindi necessario installare un nuovo layer: \textbf{meta-java}. Purtroppo il versioning di questo layer non seguiva lo standard dei restanti layer e mostrava problemi di compatibilit\`a con gli altri layer. \`E stato necessario percorrere all'indietro lo storico dei commit della repository contenente il layer, fino a trovare una versione compatibile con l'immagine realizzata finora.

Poich\`e si voleva rendere disponibile la API Java ufficiale di OpenCV \`e stato necessario modificare a mano la ricetta di OpenCV in quanto solo l'ultima versione di detta ricetta si occupava di generare tale API. Usando una versione pi\`u vecchia, per i problemi di compatibilit\`a delle librerie RealSense, la API non veniva generata.

Lo stack software completo di questa versione della distribuzione \`e riportato in figura \ref{fig:SWStack} ed \`e la versione finale della distribuzione.

%% Interruzione di pagina
\newpage

\section{File di configurazione: local.conf}\label{ModLocalConf}
Qui di seguito \`e riportato il file di configurazione della versione finale della distribuzione.

\begin{lstlisting}[caption=local.conf]
MACHINE ??= "intel-corei7-64"
DISTRO ?= "poky"
PACKAGE_CLASSES ?= "package_rpm"
SDKMACHINE ?= "x86_64"
EXTRA_IMAGE_FEATURES ?= "debug-tweaks"

USER_CLASSES ?= "buildstats image-mklibs image-prelink"
PATCHRESOLVE = "noop"
BB_DISKMON_DIRS = "\
    STOPTASKS,${TMPDIR},1G,100K \
    STOPTASKS,${DL_DIR},1G,100K \
    STOPTASKS,${SSTATE_DIR},1G,100K \
    STOPTASKS,/tmp,100M,100K \
    ABORT,${TMPDIR},100M,1K \
    ABORT,${DL_DIR},100M,1K \
    ABORT,${SSTATE_DIR},100M,1K \
    ABORT,/tmp,10M,1K"

PACKAGECONFIG_append_pn-qemu-native = " sdl"
PACKAGECONFIG_append_pn-nativesdk-qemu = " sdl"

CONF_VERSION = "1"

BB_NUMBER_THREADS = '12'
PARALLEL_MAKE = '-j 12'

# Java installation
IMAGE_INSTALL_append = " openjdk-8 "
PREFERRED_PROVIDER_virtual/java-initial-native = "cacao-initial-native"
PREFERRED_PROVIDER_virtual/java-native = "jamvm-native"
PREFERRED_PROVIDER_virtual/javac-native = "ecj-bootstrap-native"

# OpenCV installation
CORE_IMAGE_EXTRA_INSTALL += "opencv opencv-samples libopencv-core-dev libopencv-highgui-dev libopencv-imgproc-dev libopencv-objdetect-dev libopencv-ml-dev"
\end{lstlisting}

Questo \`e il file principale per la configurazione della build. Come detto in precedenza contiene le istruzioni su quali software installare e altri metadati sul processo di build. 

%% Interruzione di pagina
\newpage

Vediamo ora nel dettaglio il significato di questi parametri:

\begin{itemize}
\item \verb|MACHINE|: Specifica la target device per la quale l'immagine viene costruita. La variabile corrisponde ad un file di configurazione della macchina con lo stesso nome, attraverso il quale le configurazioni specifiche per quella macchina vengono impostate. Di conseguenza, quando \verb|MACHINE| \`e impostato a \verb|quemux86| esister\`a un file qemux86.conf corrispondente nella directory meta/conf/machine. Nel nostro caso stiamo costruendo una distribuzione per una architettura Intel 64-bit e il file di configurazione della macchina si trova nel layer \textbf{meta-intel}.
\item \verb|DISTRO|: Il nome della distribuzione. Questa variabile corrisponde ad un file di configurazione di distribuzione la cui radice del nome \`e la stessa dell'argomento della variabile e la cui esensione \`e .conf. Per esempio, il file di configurazione della distribuzione Poky \`e poky.conf e risiede in meta-poky/conf/distro.
\item \verb|PACKAGE_CLASSES|: specifica il package manager che viene usato dal build system di OpenEmbedded quando vengono creati i pacchetti del software in fase di build.
\item \verb|SDKMACHINE|: La macchina per la quale \`e costruita la toolchain di cross-sviluppo. In altre parole, il SDK \`e costruito in modo tale che possa essere eseguito su un target specifico con il valore \verb|SDKMACHINE|. Il valore punta ad un file .conf corrispondente sotto la cartella conf/machine-sdk/. Possono essere usati ``\verb|i686|'' e ``\verb|x86_64|'' come possibili valori per questa variabile.
\item \verb|EXTRA_IMAGE_FEATURES|: Una lista di feature addizionali da includere nell'immagine. Ad esempio ``\verb|debug-tweaks|'' rende l'immagine adatta al debugging, permettendo il login come root senza l'uso di password e abilitando il logging post-installazione.
\item \verb|PATCHRESOLVE|: Determina che azione intraprendere quando l'operazione di applicazione di patch fallisce. In questo caso \`e stato lasciato al valore di default.
\item \verb|BB_DISKMON_DIRS|: Monitora lo spazio su disco e gli inode disponibili durante la build permettendo di controllare il processo di build per mezzo di questi parametri.
\item \verb|BB_NUMBER_THREADS|: Il massimo numero di task che BitBake pu\`o eseguire in parallelo contemporaneamente. Il build system di OpenEmbedded imposta automaticamente questo valore al numero di core disponibili sulla macchina host del build system. Per esempio un sistema con un processore dual-core che fa anche uso dell'hyper-threading implica che la variabile \verb|BB_NUMBER_THREADS| venga impostata a "4".
\item \verb|PARALLEL_MAKE|: Opzioni extra passate al comando \verb|make| durante la task \verb|do_compile| task in modo tale da specificare la compilazione parallela sulla macchina del build system. Questa variabile solitamente \`e nella forma "-j x", dove x rappresenta il numero massimo di thread paralleli che il comando make pu\`o istanziare.
\item \verb|IMAGE_INSTALL|: Specifica i pacchetti da installare in una immagine.
\item \verb|CORE_IMAGE_EXTRA_INSTALL|: Specifica la lista di pacchetti che devono essere aggiunti all'immagine. Questa operazione \`e ristretta alle immagini di tipo OE Core.
\end{itemize}

%% Interruzione di pagina
\newpage

Si noti la mancanza della libreria RealSense tra i pacchetti installati nella distribuzione. Questo \`e dovuto al fatto che la sua installazione avviene usando un secondo file di configurazione ``auto.conf'', come specificato nelle istruzioni di installazione della libreria.

\begin{lstlisting}[caption=auto.conf]
require include/intel-librealsense.inc

# Intel Realsense
CORE_IMAGE_EXTRA_INSTALL += "librealsense-graphical-examples"
\end{lstlisting}

La keyword \verb|require| va a prendere la ricetta specificata, necessaria all'installazione della libreria.

\section{Modifica alla ricetta di OpenCV}\label{ModRicettaOpenCV}
Per aggiungere le API Java di OpenCV \`e stato necessario modificare la ricetta di OpenCV come segue:

\begin{lstlisting}[language=diff]
@@ -48,1 +48,1 @@ EXTRA_OECMAKE_append_x86 = " -DX86=ON"
EXTRA_OECMAKE_append_x86 = " -DX86=ON"

-PACKAGECONFIG ??= "eigen jpeg png tiff v4l libv4l gstreamer samples tbb gphoto2 \
+PACKAGECONFIG ??= "eigen jpeg png tiff v4l libv4l gstreamer samples tbb java gphoto2 \
    ${@bb.utils.contains("DISTRO_FEATURES", "x11", "gtk", "", d)} \
    ${@bb.utils.contains("LICENSE_FLAGS_WHITELIST", "commercial", "libav", "", d)}"
\end{lstlisting}

\begin{lstlisting}[language=diff]
@@ -58,11 +58,12 @@ PACKAGECONFIG[gphoto2] = "-DWITH_GPHOTO2=ON,-DWITH_GPHOTO2=OFF,libgphoto2,"
PACKAGECONFIG[gstreamer] = "-DWITH_GSTREAMER=ON,-DWITH_GSTREAMER=OFF,gstreamer1.0 gstreamer1.0-plugins-base,"
PACKAGECONFIG[gtk] = "-DWITH_GTK=ON,-DWITH_GTK=OFF,gtk+3,"
PACKAGECONFIG[jasper] = "-DWITH_JASPER=ON,-DWITH_JASPER=OFF,jasper,"
+PACKAGECONFIG[java] = "-DJAVA_INCLUDE_PATH=${JAVA_HOME}/include -DJAVA_INCLUDE_PATH2=${JAVA_HOME}/include/linux -DJAVA_AWT_INCLUDE_PATH=${JAVA_HOME}/include -DJAVA_AWT_LIBRARY=${JAVA_HOME}/lib/amd64/libjawt.so -DJAVA_JVM_LIBRARY=${JAVA_HOME}/lib/amd64/server/libjvm.so,,ant-native fastjar-native openjdk-8-native,"
PACKAGECONFIG[jpeg] = "-DWITH_JPEG=ON,-DWITH_JPEG=OFF,jpeg,"
PACKAGECONFIG[libav] = "-DWITH_FFMPEG=ON,-DWITH_FFMPEG=OFF,libav,"
PACKAGECONFIG[libv4l] = "-DWITH_LIBV4L=ON,-DWITH_LIBV4L=OFF,v4l-utils,"
PACKAGECONFIG[opencl] = "-DWITH_OPENCL=ON,-DWITH_OPENCL=OFF,opencl-headers,"
-PACKAGECONFIG[oracle-java] = "-DJAVA_INCLUDE_PATH=${JAVA_HOME}/include -DJAVA_INCLUDE_PATH2=${JAVA_HOME}/include/linux -DJAVA_AWT_INCLUDE_PATH=${JAVA_HOME}/include -DJAVA_AWT_LIBRARY=${JAVA_HOME}/lib/amd64/libjawt.so -DJAVA_JVM_LIBRARY=${JAVA_HOME}/lib/amd64/server/libjvm.so,,ant-native oracle-jse-jdk oracle-jse-jdk-native,"
+PACKAGECONFIG[oracle-java] = "-DJAVA_INCLUDE_PATH=${ORACLE_JAVA_HOME}/include -DJAVA_INCLUDE_PATH2=${ORACLE_JAVA_HOME}/include/linux -DJAVA_AWT_INCLUDE_PATH=${ORACLE_JAVA_HOME}/include -DJAVA_AWT_LIBRARY=${ORACLE_JAVA_HOME}/lib/amd64/libjawt.so -DJAVA_JVM_LIBRARY=${ORACLE_JAVA_HOME}/lib/amd64/server/libjvm.so,,ant-native oracle-jse-jdk oracle-jse-jdk-native,"
PACKAGECONFIG[png] = "-DWITH_PNG=ON,-DWITH_PNG=OFF,libpng,"
PACKAGECONFIG[samples] = "-DBUILD_EXAMPLES=ON -DINSTALL_PYTHON_EXAMPLES=ON,-DBUILD_EXAMPLES=OFF,,"
PACKAGECONFIG[tbb] = "-DWITH_TBB=ON,-DWITH_TBB=OFF,tbb,"
\end{lstlisting}

\begin{lstlisting}[language=diff]
@@ -73,12 +74,14 @@ inherit distutils-base pkgconfig cmake
export PYTHON_CSPEC="-I${STAGING_INCDIR}/${PYTHON_DIR}"
export PYTHON="${STAGING_BINDIR_NATIVE}/python"
-export JAVA_HOME="${STAGING_DIR_NATIVE}/usr/bin/java"
+export ORACLE_JAVA_HOME="${STAGING_DIR_NATIVE}/usr/bin/java"
+export JAVA_HOME="${STAGING_DIR_NATIVE}/usr/lib/jvm/openjdk-8-native"
export ANT_DIR="${STAGING_DIR_NATIVE}/usr/share/ant/"

TARGET_CC_ARCH += "-I${S}/include "

PACKAGES += "${@bb.utils.contains('PACKAGECONFIG', 'oracle-java', '${PN}-java-dbg ${PN}-java', '', d)} \
+    ${@bb.utils.contains('PACKAGECONFIG', 'java', '${PN}-java-dbg ${PN}-java', '', d)} \
    ${PN}-samples-dbg ${PN}-samples ${PN}-apps python-opencv"
\end{lstlisting}

Praticamente grazie a queste modifiche, se durante il processo di build, BitBake si accorge che l'immagine possiede un JDK, effettua le operazioni necessarie a generare le API Java di OpenCV.

La patch si \`e resa necessaria in quanto questa operazione avveniva solamente nel caso in cui BitBake rilevasse il JDK di Oracle, ma per problemi di compatibilit\`a \`e stato possibile installare solamente il JDK OpenJDK-8. Quindi \`e stato necessario effettuare le modifiche di cui sopra le quali non fanno altro che rilevare la presenza del nuovo JDK e lo usano per implementare la API.

%% Interruzione di pagina
\newpage

\section{Problemi}
La distribuzione cos\`\i\ realizzata purtroppo non \`e esente da problemi.

\begin{itemize}
\item \textbf{Mancanza dei driver per l'hardware Eurotech}: non \`e stato possibile aggiungere alla distribuzione i driver necessari all'interfacciamento con l'hardware proprietario Eurotech installato sul ReliaGate 20-25. Questo \`e dovuto alla scarsa conoscenza del tool Yocto e alla mancanza di tempo per la realizzazione di questo ultimo passaggio.
\item \textbf{Mancanza di tool}: la distribuzione cos\`\i\ realizzata \`e barebone: mancano moltissimi tool software adatti a farne una distribuzione completa.
\item \textbf{Incompatibilit\`a con le versioni pi\`u recenti del progetto Yocto}: purtroppo il layer librealsense non viene aggiornato spesso e quindi si \`e stati costretti ad utilizzare una versione di Poky che non \`e la pi\`u recente. Di conseguenza tutti pacchetti installati sono bloccati a versioni non aggiornate (es: OpenCV si trova alla versione 3.1 invece che alla 3.2).
\item \textbf{Mancanza del supporto ad OpenCL}: inizialmente si prevedeva di accelerare parte della computazione sulla GPU. Purtroppo, per mancanza di supporto da parte di Intel, non \`e stato possibile installare OpenCL e quindi accelerare gli algoritmi di OpenCV sulla GPU integrata dell'Intel E3827.
\end{itemize}

Nonostante questi problemi la distribuzione possiede tutti i tool necessari a supportare l'applicativo sviluppato nel corso della tesi, ed \`e stata ritenuta un ottimo proof of concept da parte di Eurotech.

\chapter{Manuale d'uso del progetto}\label{CapitoloManuale}
In questo capitolo verr\`a riportata la struttura del progetto e come replicare i risultati ottenuti nel corso dello sviluppo.

\section{Repository Github}
Il progetto \`e disponibile pubblicamente all'indirizzo:

\begin{verbatim}
https://github.com/mattdibi/RSPassengerCounter
\end{verbatim}

\noindent La struttura della repository \`e la seguente:

\begin{itemize}
\item \verb|build_config|: qui sono raccolti i file di configurazione del progetto Yocto e le istruzioni per generare la distribuzione per il ReliaGate 20-25.
\item \verb|cpp_src|: in questa cartella sono contenuti i sorgenti per la versione C++ del codice.
\item \verb|java_wrap|: qui sono contenuti i file di configurazione di SWIG per generare il wrapper Java del progetto.
\item \verb|java_src|: qui sono contenuti i sorgenti e i file .jar necessari per la versione Java dell'applicativo.
\end{itemize}

\section{Prerequisiti}
Per poter eseguire l'applicativo sulla piattaforma target \`e necessario solamente disporre dell'immagine della distribuzione Yocto e il cross-compilatore generato con il progetto Yocto.

Per poter eseguire l'applicativo su una macchina host \`e necessario che su questa siano installati:

\begin{itemize}
\item OpenCV
\item librealsense
\item openjdk-8
\end{itemize}

Inoltre \`e necessario assicurarsi di avere delle porte USB 3.0 disponibili per il collegamento con le telecamere RealSense ed un hardware sufficientemente prestante.

\section{Generazione dell'immagine Yocto}
Qui sono riportati i passaggi per poter generare correttamente l'immagine dell'OS per il ReliaGate 20-25 a supporto del progetto.

\subsubsection{Step 1: Clonare le repository dei layer}

\begin{lstlisting}[language=bash]
$ git clone git://git.yoctoproject.org/poky -b morty
$ cd $INSTALL_DIR/poky
$ git clone git://git.yoctoproject.org/meta-intel -b morty
$ git clone git://git.openembedded.org/meta-openembedded -b morty
$ git clone https://github.com/IntelRealSense/meta-intel-realsense.git -b morty
$ git clone git://git.yoctoproject.org/meta-java
\end{lstlisting}

\noindent Importante: \`e necessario impostare il branch del layer \textbf{meta-java} ad uno specifico commit come segue:

\begin{lstlisting}[language=bash]
$ git reset --hard 67e48693501bddb80745b9735b7b3d4d28dce9a1
\end{lstlisting}

\subsubsection{Step 2: Impostare il build environment}

\begin{lstlisting}[language=bash]
$ source oe-init-build-env
\end{lstlisting}

\subsubsection{Step 3: Aggiungere i layer}

\begin{lstlisting}[language=bash]
$ cd $INSTALL_DIR/poky/build
$ bitbake-layers add-layer "$INSTALL_DIR/poky/meta-intel"
$ bitbake-layers add-layer "$INSTALL_DIR/poky/meta-openebemdded/meta-oe"
$ bitbake-layers add-layer "$INSTALL_DIR/poky/meta-intel-realsense"
$ bitbake-layers add-layer "$INSTALL_DIR/poky/meta-java"
\end{lstlisting}

\noindent Il file bblayers.conf risultante deve corrispondere al seguente:

\begin{lstlisting}[caption=File bblayers.conf risultante]
# POKY_BBLAYERS_CONF_VERSION is increased each time build/conf/bblayers.conf
# changes incompatibly
POKY_BBLAYERS_CONF_VERSION = "2"

BBPATH = "${TOPDIR}"
BBFILES ?= ""

BBLAYERS ?= " \
  $INSTALL_DIR/poky/meta \
  $INSTALL_DIR/poky/meta-poky \
  $INSTALL_DIR/poky/meta-yocto-bsp \
  $INSTALL_DIR/poky/meta-intel \
  $INSTALL_DIR/poky/meta-openembedded/meta-oe \
  $INSTALL_DIR/poky/meta-intel-realsense \
  $INSTALL_DIR/poky/meta-java \
  "
\end{lstlisting}

\subsubsection{Step 4: Modificare i file di configurazione}

Modificare i file di configurazione \textbf{local.conf} e \textbf{auto.conf} come descritto nel paragrafo \ref{ModLocalConf}. Nella repository sono disponibili i file corretti e possono essere copiati semplicemente nelle cartelle di destinazione.

\subsubsection{Step 5: Modificare la ricetta OpenCV}

Come descritto nel paragrafo \ref{ModRicettaOpenCV} \`e necessario modificare la ricetta di OpenCV disponibile nella cartella:

\begin{verbatim}
$INSTALL_DIR/poky/meta-openembedded/meta-oe/recipes-support/opencv/opencv_3.1.bb
\end{verbatim}

\noindent Anche in questo caso nella repository \`e contenuta una versione del file gi\`a modificato.

\subsubsection{Step 6: Lanciare la build}

\noindent Lanciare il processo di build:

\begin{lstlisting}[language=bash]
$ bitbake core-image-sato
\end{lstlisting}

\noindent Una volta completato il processo i file dell'immagine del sistema operativo si trovano nella cartella:

\begin{verbatim}
$INSTALL_DIR/poky/build/tmp/deploy/images/intel-corei7-64/
\end{verbatim}

\noindent Sar\`a quindi necessario lanciare il processo di build dell'installatore della toolchain di cross-compilazione:

\begin{lstlisting}[language=bash]
$ bitbake core-image-sato -c populate_sdk
\end{lstlisting}

\noindent Una volta completata la task l'installatore della toolchain si trova nella cartella:

\begin{verbatim}
$INSTALL_DIR/poky/build/tmp/deploy/sdk/
\end{verbatim}

\noindent A questo punto \`e necessario scrivere l'immagine del sistema operativo su un supporto di memoria dal quale il ReliaGate pu\`o effettuare il boot. A tal fine \`e possibile usare il comando riportato qui di seguito.

\begin{lstlisting}[language=bash]
$ sudo dd if=$INSTALL_DIR/poky/build/tmp/deploy/images/intel-corei7-64/core-image-base-intel-corei7-64.wic of=TARGET_DEVICE status=progress
\end{lstlisting}

\subsection{Script di installazione}
Per semplificare il processo di installazione del build system Yocto per la generazione dell'immagine per il 20-25 \`e stato realizzato uno script di installazione disponibile all'interno della repository del progetto.

Per effettuare l'installazione del build system \`e necessario eseguire i seguenti comandi:

\begin{lstlisting}[language=bash]
$ chmod +x build_install
$ ./build_install
\end{lstlisting}

\noindent Inoltre prima di lanciare il processo di build \`e necessario impostare il nome della Build Directory come segue. Questo step non \`e necessario se si effettua l'installazione manuale del build system.

\begin{lstlisting}[language=bash]
$ cd poky/
$ set build
$ source oe-init-build-env
$ bitbake core-image-sato
\end{lstlisting}

%% Interruzione di pagina
\newpage

\subsection{Problemi noti}
Esiste un problema noto con alcune vecchie versioni del compilatore C++ che possono interuompere il processo di build dell'immagine. Nel caso si presenti l'errore ``\textbf{unrecognized command line option -fno-lifetime-dse}'' \`e necessario modificare la ricetta \textbf{\$INSTALL\_DIR/poky/meta-java/recipes-core/openjdk/openjdk-8-common.inc} alla linea 224 come segue:

\begin{lstlisting}[language=diff]
# GCC 6 sets the default C++ standard to C++14 and introduces dead store
# elimination by default. OpenJDK 8 is not ready for either of these
# changes.
- FLAGS_GCC6 = "-fno-lifetime-dse -fno-delete-null-pointer-checks"
+ FLAGS_GCC6 = "-fno-delete-null-pointer-checks"
\end{lstlisting}

\subsection{Configurazione risultante}

\begin{lstlisting}
BB_VERSION        = "1.32.0"
BUILD_SYS         = "x86_64-linux"
NATIVELSBSTRING   = "universal-4.8"
TARGET_SYS        = "x86_64-poky-linux"
MACHINE           = "intel-corei7-64"
DISTRO            = "poky"
DISTRO_VERSION    = "2.2.1"
TUNE_FEATURES     = "m64 corei7"
TARGET_FPU        = ""
meta              
meta-poky         
meta-yocto-bsp    = "morty:924e576b8930fd2268d85f0b151e5f68a3c2afce"
meta-intel        = "morty:6add41510412ca196efb3e4f949d403a8b6f35d7"
meta-oe           = "morty:fe5c83312de11e80b85680ef237f8acb04b4b26e"
meta-intel-realsense = "morty:2c0dfe9690d2871214fba9c1c32980a5eb89a421"
meta-java         = "master:67e48693501bddb80745b9735b7b3d4d28dce9a1"
\end{lstlisting}

%% Interruzione di pagina
\newpage

\section{Compilazione ed esecuzione dell'applicativo}
Nel seguito sono riportate le istruzioni per compilare ed eseguire le varie versioni dell'applicativo Passenger Counter con telecamere RealSense.

\subsection{Versione C++}

\subsubsection{Compilazione ed esecuzione della versione per la macchina host}
Dopo essersi assicurati di aver installato tutto il software necessario basta eseguire i comandi:

\begin{lstlisting}[language=bash]
$ cd cpp_src/
$ cmake .
$ make
$ ./RSPCN
\end{lstlisting}

\`E necessario assicurarsi di aver collegato correttamente le telecamere RealSense alla macchina host prima di eseguire il programma.

\subsubsection{Compilazione ed esecuzione della versione per il ReliaGate 20-25}
Dopo aver installato la toolchain di cross-compilazione resa disponibile dal progetto Yocto \`e sufficiente eseguire i seguenti comandi:

\begin{lstlisting}[language=bash]
$ source /opt/poky/2.2.1/environment-setup-corei7-64-poky-linux
$ $CXX -std=c++11 -lX11 -pthread -o RSPCN20-25 main.cpp  \
-lopencv_core \
-lopencv_highgui \
-lopencv_imgproc \
-lopencv_ml \
-lopencv_objdetect \
-lopencv_videoio \
-lopencv_video \
-lrealsense
\end{lstlisting}

\noindent Dove si presume che la cartella di installazione della toolchain sia quella di default. \`E necessario quindi trasferire il binario compilato sulla macchina target ed eseguirlo normalmente.

\subsubsection{Esecuzione remota}
Poich\'e nell'immagine della build Yocto \`e presente \verb|ssh|, \`e possibile eseguire in remoto il programma del Passenger Counter effettuando il tunneling di X11 per mezzo del comando:

\begin{lstlisting}[language=bash]
$ ssh -X root@<IP macchina target>
\end{lstlisting}

In questo modo sar\`a possibile visualizzare sulla macchina host gli stream catturati dalle telecamere collegate alla macchina target ed il risultato della computazione.

Si noti che questa modalit\`a degrada leggermente le prestazioni del contatore rendendo il framerate instabile.

\subsubsection{Modalit\`a di esecuzione}
L'applicativo supporta due modalit\`a di esecuzione:

\begin{itemize}
\item Senza argomenti: apre tutte le telecamere collegate alla macchina di default e cattura gli input stream allocando un contatore per ogni telecamera.
\item \verb|-s|: in aggiunta alle operazioni di cui sopra, salva in formato video gli stream in ingresso dalle telecamere nei vari passaggi intermedi dell'elaborazione.
\end{itemize}

\subsubsection{Comandi run-time}
Una volta lanciato l'applicativo, il programma resta in attesa di input dall'utente perch\'e vengano immessi i comandi run-time. I comandi disponibili sono i seguenti e vengono applicati a tutti i contatori attivi:

\begin{itemize}
\item \verb|r|: Resetta i contatori.
\item \verb|p|: Visualizza il conteggio attuale sulla console.
\item \verb|c|: Abilita/Disabilita la finestra nella quale viene visualizzato lo stream a colori con le informazioni di conteggio e tracking.
\item \verb|C|: Abilita/Disabilita le trackbar per effettuare la calibrazione dei settaggi del contatore, tra questi settaggi sono disponibili:
    \begin{itemize}
    \item Threshold: soglia in centimetri fino alla quale si effettua il rilevamento degli oggetti.
    \item Blur: quantit\`a di offuscamento dell'immagine per ridurre il rumore.
    \item xNear: numero di pixel dell'asse orizzontale per il tracking.
    \item yNear: numero di pixel dell'asse verticale per il tracking.
    \item Area min: area minima per la quale l'oggetto rilevato viene tracciato.
    \item Passenger age: tempo per il quale i passeggeri tracciati vengono lasciati in memoria prima di essere cancellati quando non vengono pi\`u rilevati nel raggio visivo della telecamera.
    \end{itemize}
\item \verb|d|: Abilita/Disabilita la finestra nella quale viene visualizzato lo stream della colorMap (contenuto nella variabile \verb|depthColorMap|).
\item \verb|D|: Abilita/Disabilita la finestra nella quale viene visualizzato lo stream dei dati di profondit\`a grezzi provenienti dalla telecamera RealSense (contenuto nella variabile \verb|rawDepth|).
\item \verb|f|: Abilita/Disabilita la finestra nella quale viene visualizzato lo stream dei dati di profondit\`a post processati e che vengono dati in pasto all'algoritmo di rilevamento dei contorni (contenuto nella variabile \verb|frame|).
\item \verb|s|: Abilita/Disabilita la stabilizzazione del framerate.
\item \verb|q|: Termina il programma.
\item \verb|h|: Visualizza il messaggio di aiuto.
\item \verb|0-5|: Selezione i preset per la telecamera da utilizzare. I preset sono delle impostazioni della telecamera ad infrarossi che ne modificano il comportamento permettendo di impostare l'esposizione, la precisione e altri parametri.
\end{itemize}

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{preset0.png}
  \caption{Telecamera SR300 preset 0.}\label{fig:preset0}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{preset1.png}
  \caption{Telecamera SR300 preset 1.}\label{fig:preset1}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{preset2.png}
  \caption{Telecamera SR300 preset 2.}\label{fig:preset2}
\endminipage
\end{figure}

Come \`e possibile notare dalle figure \ref{fig:preset0}, \ref{fig:preset1} e \ref{fig:preset2} i preset sono molto utili per gestire varie situazioni di illuminazione e permettono di variare leggermente il range di funzionamento delle telecamere.

Qui di seguito \`e riportato uno screenshot della schermata tipica di funzionamento del programma su una macchina host. Il programma si trova in modalit\`a di esecuzione normale e sono stati visualizzati lo stream a colori e abilitata la calibrazione dei settaggi del contatore.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{EsempioRSPCNcpp.png}
  \caption{Schermata tipica RSPCN versione C++.}
  \label{fig:EsempioRSPCNcpp}
\end{figure}

%% Interruzione di pagina
\newpage

\subsection{Versione Java Wrap}

\subsubsection{Compilazione ed esecuzione della versione per la macchina host}
Per semplificare la compilazione sono stati resi disponibili degli script.

\begin{lstlisting}[language=bash]
$ cd java_wrap/
$ sh swigwrapcmd.sh
$ cd swig_output/
$ javac *.java
$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/swig_output
$ java Main
\end{lstlisting}

\subsubsection{Compilazione ed esecuzione della versione per il ReliaGate 20-25}
Dopo aver installato la toolchain di cross-compilazione generata dal progetto Yocto.

\begin{lstlisting}[language=bash]
$ source /opt/poky/2.2.1/environment-setup-corei7-64-poky-linux
$ sh swigwrapcmd20-25.sh
$ scp -r swig_output_20-25/ root@<ReliGATE 20-25 IP address>:
\end{lstlisting}

\noindent Anche qui si presume che la cartella di installazione della toolchain sia quella di default.

Una volta connessi al ReliaGate 20-25:

\begin{lstlisting}[language=bash]
$ cd swig_output_2025/
$ javac *.java
$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/swig_output_2025
$ java Main
\end{lstlisting}

%% Interruzione di pagina
\newpage

\subsection{Versione Java}
I seguenti comandi sono gli stessi per la macchina host e per il ReliaGate 20-25.

\noindent Compilazione degli eseguibili:

\begin{lstlisting}[language=bash]
$ cd java_src/
$ javac -cp jar_files/ffmpeg-platform.jar: \
    jar_files/javacpp.jar:jar_files/librealsense-platform.jar: \
    jar_files/javacv.jar:jar_files/opencv-platform.jar:. Main.java
\end{lstlisting}

\noindent Esecuzione dell'applicativo:

\begin{lstlisting}[language=bash]
$ java -cp jar_files/ffmpeg-platform.jar: \
    jar_files/javacpp.jar:jar_files/librealsense-platform.jar: \
    jar_files/javacv.jar:jar_files/opencv-platform.jar:. Main
\end{lstlisting}

\subsubsection{Modalit\`a di esecuzione}
La versione Java del Passenger Counter supporta quattro modalit\`a di esecuzione. La scelta su quale avviare viene effettuata una volta lanciata l'applicazione.

Anche in questo caso, all'avvio, viene istanziato un contatore per telecamera collegata.

\begin{enumerate}
\item \verb|M|: \textbf{Bare metal mode}. Non vengono visualizzate le finestre contenenti gli stream video. 
\item \verb|N|: \textbf{Normal mode}. Vengono visualizzate le finestre contenenti gli stream video.
\item \verb|V|: \textbf{Video recording mode}. Vengono visualizzate le finestre contenenti gli stream video e salvati gli stream su file.
\item \verb|B|: \textbf{Video recording in bare metal mode}. Vengono salvati su file gli stream video ma non vengono visualizzati in run-time.
\end{enumerate}

L'ultima modalit\`a di esecuzione si \`e resa necessaria poich\`e, essendo le finestre di questa versione basate sul framework Java e non su X11, impediscono il corretto funzionamento del programma sul ReliaGate 20-25.

\subsubsection{Comandi Run-time}
Anche in questo caso le opzioni vengono applicate a tutti i contatori attualmente attivi sulla macchina.

\begin{itemize}
\item \verb|q|: Termina il programma.
\item \verb|r|: Resetta i contatori
\item \verb|c|: Visualizza il conteggio attuale
\item \verb|p|: Selezione i preset per la telecamera da utilizzare. I preset sono delle impostazioni della telecamera ad infrarossi che ne modificano il comportamento permettendo di impostare l'esposizione, la precisione ecc ecc..
\item \verb|t|: Imposta la soglia in centimetri fino alla quale si effettua il rilevamento degli oggetti.
\item \verb|x|: Imposta il numero di pixel dell'asse orizzontale per il tracking.
\item \verb|y|: Imposta il numero di pixel dell'asse verticale per il tracking.
\item \verb|b|: Imposta la quantit\`a di offuscamento dell'immagine per ridurre il rumore.
\item \verb|a|: Imposta il tempo per il quale i passeggeri tracciati vengono lasciati in memoria prima di essere cancellati quando non vengono pi\`u rilevati nel raggio visivo della telecamera.
\item \verb|h|: Visualizza il messaggio di aiuto.
\end{itemize}

Qui di seguito \`e riportato uno screenshot della schermata tipica di funzionamento del programma su una macchina host. Il programma si trova in modalit\`a di esecuzione Normal Mode.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{EsempioRSPCNjava.png}
  \caption{Schermata tipica RSPCN versione Java.}
  \label{fig:EsempioRSPCNjava}
\end{figure}

% Capitolo 5: Conclusioni
\chapter{Conclusioni}\label{CapitoloConclusioni}

\section{Riassunto}
L'obiettivo di questa tesi era quello di sviluppare un sistema di Passenger Counter, un applicativo dedicato ad effettuare il conteggio delle persone in entrata/uscita da un mezzo pubblico o un ambiente, e la piattaforma software a supporto dello stesso, partendo dalla soluzione tecnologia adottata dalla Eurotech. Inoltre si \`e ritenuto necessario tenere in considerazione i punti deboli della soluzione adottata in azienda e cercare di risolverli opportunamente. 

Il lavoro si \`e svolto come segue.

Inizialmente ci si \`e dedicati all'indagine sullo stato attuale delle tecnologie disponibili per l'elaborazione delle immagini in real-time, considerando al contempo l'hardware disponibile per lo sviluppo, e di studio degli strumenti disponibili. In questa fase \`e stato necessario prendere confidenza con la libreria OpenCV, l'ambiente di sviluppo Xilinx ed il progetto Yocto, per poter capire quale fosse lo strumento pi\`u adatto all'obiettivo che ci si prefiggeva. 

Una volta deciso quale tecnologia/ambiente di sviluppo  adottare, \`e cominciato lo sviluppo dell'applicazione.

Il primo passo \`e stato il Passenger Counter a sottrazione del background. Questo contatore, per rilevare il transito dei passeggeri attraverso la soglia virtuale, adotta algoritmi di sottrazione dello sfondo e rilevamento dei contorni. L'algoritmo di sottrazione dello sfondo ``impara'' qual'\`e lo sfondo dell'immagine e ne crea un modello. Quando viene rilevato un cambiamento nell'immagine, questo modello viene sottratto all'immagine e ci\`o che rimane sono gli oggetti in movimento nel raggio visivo della telecamera. A questi oggetti viene applicato un algoritmo di rilevamento dei contorno e quindi calcolato il centro dell'oggetto. Questo centro viene usato per tracciare il percorso effettuato dagli oggetti all'interno del raggio visivo della telecamera. Quando il centro dell'oggetto attraversa la soglia vengono aggiornati i contatori. Purtroppo la soluzione cos\`\i\ ottenuta \`e molto imprecisa e caratterizzata da un elevato costo computazionale.

A corredo di questa soluzione \`e stata realizzata una distribuzione customizzata destinata all'hardware in dotazione. La Eurotech ha fornito il ReliaGate 20-25, un gateway IoT Multi-Service dotato di CPU Intel con architettura x86-64. Per mezzo del Progetto Yocto \`e stato possibile creare una distribuzione ad hoc per il gateway, che avesse installato al suo interno i tool necessari all'applicazione. L'immagine del sistema operativo permette di utilizzare una semplice webcam e il ReliaGate per implementare il contatore di passeggeri.

Non soddisfatti del risultato ottenuto con la sottrazione del background si \`e visto nelle telecamere Intel RealSense il tassello mancante all'applicazione.

Prima di poter procedere nello sviluppo dell'applicazione si \`e ritenuto necessario integrarle nell'immagine del sistema operativo. Solo una volta installate le librerie e i driver con successo si \`e potuti proseguire nello sviluppo. Questa fase si \`e rivelata non priva di problematiche viste le limitazioni nella compatibilit\`a delle librerie di interfacciamento verso le telecamere.

\`E stato quindi realizzato il Passenger Counter con telecamere RealSense. Questa soluzione rileva il transito dei passeggeri facendo affidamento sulle informazioni di profondit\`a fornite dalle telecamere. In sostanza, l'applicazione elimina tutto ci\`o che si trova ad una distanza preimpostata dalla telecamera e traccia i massimi locali nell'immagine i quali corrispondono alla testa dei passeggeri. Di questi massimi viene tracciato il centro e, quando questo attraversa la soglia, vengono aggiornati i contatori. Questa soluzione si \`e rivelata essere molto precisa e dal costo computazionale basso, poich\`e gran parte dell'elaborazione avviene all'interno di un ASIC presente all'interno delle telecamere.

Una volta constatato che la soluzione con le telecamera RealSense soddisfava i requisiti di precisione e qualit\`a richiesti si \`e proceduto ad effettuare il porting del codice nel linguaggio adottato dall'azienda. Infatti l'applicazione \`e stata sviluppata in C++, linguaggio nativo delle libreria OpenCV e librealsense, ma in Eurotech il linguaggio pi\`u usato e adottato come standard \`e il Java.

Inizialmente si \`e effettuato un wrapping Java sfruttando il tool SWIG. Successivamente si \`e deciso di effettuare un porting completo dell'applicazione appoggiandosi al progetto open-source JavaCPP. Anche questo passaggio ha comportato notevoli problematiche in quanto \`e stato necessario appoggiarsi ad una API Java di OpenCV non ufficiale, la quale non era completamente compatibile con quanto realizzato fino a questo punto.

A supporto di questa versione dell'applicativo si \`e aggiunto all'immagine del sistema operativo del ReliaGate 20-25 un Java Real-time Environment ed un JDK. Inoltre si \`e deciso di rendere disponibile la API Java ufficiale di OpenCV. Quest'ultimo passaggio \`e stato complicato dai problemi di compatiblit\`a delle telecamere RealSense e si \`e reso necessario modificare a mano i metadati del progetto Yocto necessari all'installazione della API Java di OpenCV.

A questo punto dello sviluppo si \`e ritenuto il progetto completato e ci si \`e ritenuti soddisfatti dei risultati ottenuti.

%% Interruzione di pagina
\newpage

\section{Risultati Passenger Counter con telecamere RealSense}
I risultati ottenuti da questa versione del contatore sono stati ritenuti molto buoni. \`E stato possibile infatti migliorare tutti i punti deboli della versione del contatore attualmente implementata da Eurotech.

Come visto nel paragrafo \ref{ProblemiPCNEurotech} a pagina \pageref{ProblemiPCNEurotech} il contatore realizzato da Eurotech presentava delle criticit\`a che mi \`e stato chiesto di risolvere durante lo sviluppo di questa versione del Passenger Counter. Queste criticit\`a sono state risolte come segue:

\begin{enumerate}
\item \textbf{Difficolt\`a nel reperire l'hardware}: le telecamere RealSense sono un dispositivo di nuova concezione e da poco sul mercato. La Intel \`e intenzionata a fornire supporto a questa tecnologia a lungo e quindi non sembra possano insorgere problemi di reperibilit\`a a breve termine.
\item \textbf{Utilizzo di FPGA}: \`e stato risolto per mezzo dell'utilizzo delle telecamere RealSense. Non \`e pi\`u necessario fare affidamento alla FPGA per la ricostruzione delle informazioni di profondit\`a in quanto questo compito \`e assolto dagli ASIC presenti all'interno delle telecamere. Questo permette un abbassamento dei costi di produzione del sistema.
\item \textbf{Numero di unit\`a computazionali}: nella versione attuale del Passenger Counter di Eurotech \`e necessario che vi sia un Gateway per ogni DynaPCN 10-20. Con la versione realizzata in questa tesi \`e sufficiente un RealiaGate per pi\`u telecamere. Questo \`e possibile grazie al ridotto costo computazionale dell'applicativo e alla parallelizzazione delle istanze del contatore. Si noti che per poter sfruttare appieno questo vantaggio \`e necessario che l'applicativo sia eseguito su una macchina sufficientemente potente.
\item \textbf{Dimensioni}: l'ingombro di queste telecamere \`e molto pi\`u basso rispetto al dispositivo usato da Eurotech, questo aumenta la facilit\`a di installazione e l'appetibilit\`a sul mercato.
\end{enumerate}

In conclusione si pu\`o affermare che la soluzione adottata da questo Passenger Counter sembra essere promettente e raggiunge tutti gli obiettivi che ci si era prefissati all'inizio della tesi.

\subsection{Possibili miglioramenti e sviluppi futuri}
Nonostante si ritenga questa soluzione soddisfacente sotto molti punti di vista, sono sicuramente possibili dei miglioramenti:

\begin{itemize}
\item \textbf{Implementazione di algoritmi pi\`u sofisticati per il rilevamento dei passeggeri}: l'implementazione realizzata in questa tesi \`e sicuramente un solido proof of concept ma per una applicazione nel mondo reale \`e necessario implementare un rilevamento dei passeggeri pi\`u solido. Una soluzione interessante sarebbe quella di adottare algoritmi di machine learning per il riconoscimento delle sagome dei passeggeri che transitano sotto la telecamera. Questo permetterebbe una maggior precisione diminuendo il numero di falsi positivi rilevati dalla telecamera.
\item \textbf{Carico computazionale}: al momento il carico computazionale \`e interamente assorbito dalla CPU del ReliaGate. Scaricare parte di questo carico sulla GPU potrebbe permettere di aumentare il numero massimo di istanze del contatore contemporaneamente presenti sull'unit\`a computazionale abbassando ulteriormente i costi di produzione del sistema.
\item \textbf{Test e calibrazioni}: i mezzi a mia disposizione per effettuare test e calibrazioni dei parametri del contatore non sono adeguati ad ottenere risultati adatti all'applicazione in ambito industriale del sistema e questo \`e un punto in cui investire risorse per effettuare dei miglioramenti.
\item \textbf{Porting in OSGi}: OSGi (Open Service Gateway Initiative) \`e un framework Java per lo sviluppo e il deployment di programmi software e librerie modulari. L'infrastruttura Eurotech \`e realizzata interamente seguento le specifiche OSGi e sarebbe opportuno completare il porting del codice del progetto adattandolo a queste specifiche.
\end{itemize}

Purtroppo a causa della mancanza di tempo per lo sviluppo, non \`e stato possibile applicare questi miglioramenti in quanto \`e stata data la precedenza ad altri aspetti del progetto che hanno sottratto tempo e risorse, quindi sono lasciati ad implementazioni future del progetto.

%% Interruzione di pagina
\newpage

\section{Risultati distribuzione Yocto}
Anche in questo caso i risultati ottenuti sono stati pi\`u che soddisfacenti. Uno degli obiettivi della distribuzione era dimostrare che il RealiaGate potesse supportare librerie e tool software di largo uso come OpenCV e hardware specializzato come le telecamere RealSense. Questo permette di aumentare l'appetibilit\`a del gateway sul mercato e ne amplia gli ambiti di utilizzo in un'ottica non limitata solamente all'applicazione nel sistema del Passenger Counter.

La distribuzione realizzata supporta:

\begin{itemize}
\item OpenCV 3.1: di cui sono rese disponibili le API Java, Python e C++. Inoltre \`e possibile sfruttare l'accelerazione hardware per mezzo delle librerie Intel IPP ed Eigen.
\item Librealsense 1.12.1
\item OpenJDK-8
\item Eurotech ESF 4.1
\end{itemize}

\subsection{Possibili miglioramenti e sviluppi futuri}
Anche in questo caso sono possibili dei miglioramenti:

\begin{itemize}
\item \textbf{Installazione del supporto al calcolo parallelo su GPU}: da specifiche sappiamo che il processore installato sul ReliaGate possiede una GPU integrata e, per poter scaricare parte del carico computazionale su di essa, sarebbe necessario installare ulteriori componenti sulla distribuzione. Uno dei componenti fondamentali \`e OpenCL, il quale viene usato pesantemente da OpenCV per accelerare i suoi algoritmi. Purtroppo per inesperienza con il tool e per mancanza di supporto ufficiale da parte di Intel non \`e stato possibile effettuare questo passaggio. \`E sicuramente un miglioramento da tenere in considerazione per gli sviluppi futuri del progetto.
\item \textbf{Integrazione delle feature nella distribuzione ufficiale di Eurotech}: i prossimi passi nello sviluppo del progetto prevedono l'integrazione delle feature presenti sull'immagine realizzata nel corso di questa tesi nell'immagine del sistema operativo ufficiale di Eurotech. Questo comporterebbe l'integrazione delle interfacce verso l'hardware proprietario di Eurotech per mezzo dei driver dell'azienda a cui non si \`e avuto accesso durante lo sviluppo della tesi. Questa integrazione non \`e banale visti i vincoli di compatiblit\`a imposti dalle telecamere RealSense.
\end{itemize}

%% Fine dei capitoli normali, inizio dei capitoli-appendice (opzionali)
\appendix

\part{Appendici}

\chapter{Computer Stereo Vision}\label{ComputerStereoVision}

La computer stereo vision \`e una tecnica per l'estrazione di informazioni di profondit\`a dalla immagini digitali. Comparando le informazioni della scena acquisita da due punti di vista (telecamere), le informazioni 3D possono essere estratte esaminando le posizioni relative degli oggetti nei due piani. Il principio di funzionamento di questa tecnica \`e simile al processo biologico noto come \textit{Stereopsi}.

\section{Principio di funzionamento}
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.6]{Overview.png}
  \caption{Overview dei passaggi della computer stereo vision.}
  \label{fig:overviewStereoVision}
\end{figure}

In figura \ref{fig:overviewStereoVision} sono riportati i passaggi fondamentali per ricavare le informazioni 3D.

\begin{enumerate}
\item \textbf{Rettificazione}: le due immagini ottenute dalle telecamere vanno proiettate su un piano comune per permettere la comparazione tra le due. Questo passaggio \`e noto come image rectification. Ed \`e necessaria a semplificare il passaggio di stereo correpondance.
\item \textbf{Stereo correspondance}: questa operazione permette la generazione della disparity map. La disparity map si riferisce alla differenza tra i pixel ricavata da due immagini stereo dovuta al diverso posizionamento delle telecamere.
\item \textbf{Triangolazione}: \`e il processo che determina un punto nello spazio 3D data la sua proiezione su due o pi\`u immagini. In questo caso \`e il passaggio necessario a ricostruire le informazioni di distanza a partire dalla disparity map.
\end{enumerate}

\section{Stereo correspondance: simple block matching}
Si abbiano due immagini ricavate da altrettante telecamere disposte sullo stesso piano ad una distanza nota tra di loro. Essenzialmente viene presa una piccola regione di pixel nell'immagine proveniente dalla telecamera sinistra (ad esempio) e si cerca la regione pi\`u simile a quella considerata nell'immagine proveniente dalla telecamera destra.

Per esempio, si consideri la regione di pixel contenuta nel riquadro nero nell'immagine proveniente dalla telecamera sinistra.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{left_wtemplate_crop.png}
  \caption{Dettaglio immagine ricavata dalla telecamera di sinistra.}
  \label{fig:}
\end{figure}

Cerchiamo il riquadro pi\`u simile nell'immagine di destra.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{right_wsearch_crop.png}
  \caption{Dettaglio immagine ricavata dalla telecamera di destra.}
  \label{fig:rightSearchCrop}
\end{figure}

Quando si cerca la regione di pixel simile si parte dalle stesse coordinate dell'immagine template (indicate in figura \ref{fig:rightSearchCrop} con il riquadro bianco) e si cerca a destra e a sinistra di queste cooridinate fino ad una massima distanza decisa precedentemente. La regione pi\`u simile \`e evidenziata dal riquadro verde in figura \ref{fig:rightSearchCrop}. La disparit\`a \`e data dalla distanza orizzontale tra i centri del riguardo verde e il riquadro bianco. La metrica di similitudine tra le regioni nei riquadri \`e data dall'operazione nota come SAD (Sum of Absolute Differences, somma delle differenze finite).

\begin{equation}
\sum \left | L(r,c) - R(r,c - d)  \right |
\end{equation}

Dove L e R si riferiscono alle immagini di sinistra e di destra mentre r e c si riferiscono alla riga e colonna corrente delle immagini analizzate. ``d'' si riferisce alla dispari\`a dell'immagine di destra.

Prima di calcolare la disparity map convertiamo le due immagini in scala di grigi cosicch\`e si abbano solo valori compresi tra 0 e 255 per ogni pixel.

Per calcolare la somma delle differenze assolute tra la il riquadro template e la regione di interesse, sottraiamo ogni pixel nel riquadro template dal pixel corrispondente nella regione e prendiamo il valore assoluto di questa differenza. Quindi sommiamo tutte le differenze e questo ci da un singolo valore che approssima la misura di similitudine tra le due regioni delle immagini. Un valore basso significa che le immagini sono molto simili.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{sadillustration.png}
  \caption{Illustrazione SAD.}
  \label{fig:}
\end{figure}

Per trovare la regione pi\`u simile al riquadro template, si calcola il SAD tra il template ed ogni blocco nella regione di ricerca, quindi si sceglie il riquadro con il valore di SAD pi\`u basso.

Una volta ricavate le regioni simili \`e possibile calcolare la disparity map, la quale \`e ottenuta ricavando la disparity di tutti i pixel dell'immagine nei quali si \`e ricavata il valore di SAD pi\`u basso.

\section{Rettificazione}

Si noti come nel simple block matching si \`e cercata la corrispondenza tra blocchi solamente in orizzontale e non in verticale. Ci\`o \`e reso possibile dall'operazione di rettificazione delle immagini. Questa operazione, la quale coinvolge il rilevamento di un insieme di keypoint (usando algoritmi come SIFT o SURF) delle due immagini, e quindi l'applicazione di trasformazioni sulle immagini in modo tale che i keypoint siano allineati verticalmente. Ci\`o permette di eliminare eventuali rotazioni o deformazioni dovute alle lenti nelle immagini e si pu\`o ignorare la componente verticale nella ricerca dei riquadri simili durante l'operazione di simple block matching.

\section{Triangolazione}
Si supponga di avere due telecamere disposte sullo stesso piano, poste ad una distanza nota B di cui si conosce la distanza focale f. Dalle immagini ricavate da queste due telecamere si vuole ricavare la distanza tra le due telecamere ed un oggetto P.

%% Interruzione di pagina
\newpage

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.6]{SchemaStereoVision.png}
  \caption{Schema triangolazione.}
  \label{fig:SchemaStereoVision}
\end{figure}

In figura \ref{fig:SchemaStereoVision} abbiamo che:

\begin{itemize}
\item P: \`e il punto di cui vogliamo ricavare la distanza dalle telecamere (Z).
\item B: \`e la distanza fra gli obiettivi delle due telecamere.
\item p e p': sono le proiezioni del punto P sui piani delle due telecamere.
\item f: \`e la distanza focale delle due telecamere.
\end{itemize}

Quindi sfruttando il fatto che i triangoli PO\ped{R}O\ped{T} e Ppp' sono triangoli simili possiamo ricavare:

\begin{equation}
\frac{B}{Z} = \frac{(B + x_T) - x_R}{Z - f}
\end{equation}

e quindi:

\begin{equation}
Z = \frac{B \times f}{x_R - x_T} = \frac{B \times f}{d}
\end{equation}

Dove si definisce \textit{Disparit\`a}:

\begin{equation}
d = x_R - x_T
\end{equation}

ovvero la differenza tra le coordinate x di due punti corrispondenti. La disparit\`a \`e pi\`u alta per i punti pi\`u vicini alla telecamera.

Nota quindi la disparity map \`e possibile ricavare le informazioni di distanza per ogni oggetto presente nelle immagini.

\chapter{Luce strutturata}\label{LuceStrutturata}

La luce strutturata \`e un processo attraverso il quale, proiettando dei pattern di luce (tipicamente griglie o barre orizzontali) su una scena, si ricavano informazioni circa la profondit\`a e la superficie degli oggetti all'interno della scena grazie alle deformazioni dei pattern ottenute quando questi colpiscono la superficie degli oggetti.

\section{Principio di funzionamento}

Proiettare una stretta banda di luce su una superficie tridimensionale produce una linea di illuminazione che appare distorta da una prospettiva diversa da quella del proiettore, e pu\`o essere usata per estrarre informazioni geometriche circa la superficie.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.6]{StructuredLight.png}
  \caption{Schematizzazione funzionamento luce strutturata.}
  \label{}
\end{figure}

Per effettuare la scansione di un intera superficie velocemente vengono proiettati dei pattern che consistono di molte bande di luce, disposte ad una distanza arbitraria, poich\`e cos\`\i\ facendo si raccolgono molti campioni contemporaneamente. Visti da punti di vista diversi, i pattern vengono distorti geometricamente a causa della forma degli oggetti.

\section{Analisi dei pattern}

Vi sono molte informazioni relative alla profondit\`a contenute all'interno dei pattern di luce osservati. Il discostamento di ogni singola banda pu\`o essere direttamente convertito in coordinate 3D. A questo scopo, le bande vegnono identificate individualmente tramite diversi metodi.

Un'altra informazione importante sulla profondit\`a \`e data dalla variazione di spessore delle bande proiettate sulla superficie degli oggetti. Lo spessore delle bande \`e una funzione della ripidezza della superficie, la derivata prima della elevazione.

La frequenza e la fase delle bande portano informazioni simili che possono essere analizzate per mezzo di una trasformata di Fourier.

In molte implementazioni pratiche di questa tecnica viene effettuate una serie di misure che combinano la pattern recognition, identificazione delle bande e traformate di Fourier per ottenere una ricostruzione non ambigua delle forme in 3D.

%% Parte conclusiva del documento; tipicamente per riassunto, bibliografia e/o indice analitico.
\backmatter

\nocite{*}

%% Bibliografia (opzionale)
\bibliographystyle{plain_\languagename}%% Carica l'omonimo file .bst, dove \languagename è la lingua attiva.
%% Nel caso in cui si usi un file .bib (consigliato)
\bibliography{thud}
%% Nel caso di bibliografia manuale, usare l'environment thebibliography.

%% Per l'indice analitico, usare il pacchetto makeidx (o analogo).

\end{document}

--- Istruzioni per l'aggiunta di nuove lingue ---
Per ogni nuova lingua utilizzata aggiungere nel preambolo il seguente spezzone:
    \addto\captionsitalian{%
        \def\abstractname{Sommario}%
        \def\acknowledgementsname{Ringraziamenti}%
        \def\authorcontactsname{Contatti dell'autore}%
        \def\candidatename{Candidato}%
        \def\chairname{Direttore}%
        \def\conclusionsname{Conclusioni}%
        \def\cosupervisorname{Co-relatore}%
        \def\cosupervisorsname{Co-relatori}%
        \def\cyclename{Ciclo}%
        \def\datename{Anno accademico}%
        \def\indexname{Indice analitico}%
        \def\institutecontactsname{Contatti dell'Istituto}%
        \def\introductionname{Introduzione}%
        \def\prefacename{Prefazione}%
        \def\reviewername{Controrelatore}%
        \def\reviewersname{Controrelatori}%
        %% Anno accademico
        \def\shortdatename{A.A.}%
        \def\summaryname{Riassunto}%
        \def\supervisorname{Relatore}%
        \def\supervisorsname{Relatori}%
        \def\thesisname{Tesi di \expandafter\ifcase\csname thud@target\endcsname Laurea\or Laurea Magistrale\or Dottorato\fi}%
        \def\tutorname{Tutor aziendale%
        \def\tutorsname{Tutor aziendali}%
    }
sostituendo a "italian" (nella 1a riga) il nome della lingua e traducendo le varie voci.
